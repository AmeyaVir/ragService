{
  "repo_name": "code_repo_clean",
  "files": [
    {
      "path": "./clean_qdrant.py",
      "filename": "clean_qdrant.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nUtility script to force the deletion and recreation of the Qdrant\n'rag_documents' collection, simulating the initial worker setup.\n\"\"\"\nimport os\nimport sys\nimport asyncio\nimport structlog\nimport logging\n\n# --- Path and Configuration Setup ---\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__)))\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\n# Load environment variables\nfrom dotenv import load_dotenv\nload_dotenv(os.path.join(PROJECT_ROOT, \"backend/.env\"))\n\n# Import the service responsible for setup\nfrom backend.services.qdrant_service import QdrantService \nfrom backend.config import get_settings # Needed for logging initialization\n\ndef configure_logger():\n    \"\"\"Sets up minimal logging to see the service's delete/create messages.\"\"\"\n    logging.basicConfig(level=logging.INFO)\n    structlog.configure(\n        processors=[structlog.processors.JSONRenderer()],\n        wrapper_class=structlog.make_filtering_bound_logger(min_level=logging.INFO),\n        cache_logger_on_first_use=True,\n    )\n\ndef clear_and_recreate_collection():\n    print(\"--- Forcing Qdrant Collection Reset ---\")\n    \n    # Initializing QdrantService automatically runs setup_collection_if_needed()\n    # which contains the aggressive deletion/recreation logic.\n    try:\n        QdrantService()\n        print(\"\u2705 Qdrant collection reset successfully.\")\n        print(\"The 'rag_documents' collection is now empty and ready for indexing.\")\n    except Exception as e:\n        print(f\"\u274c ERROR: Failed to connect to or reset Qdrant.\")\n        print(f\"Ensure Qdrant is running at {get_settings().qdrant_url}. Error: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    configure_logger()\n    clear_and_recreate_collection()\n"
    },
    {
      "path": "./code-clean-repo.sh",
      "filename": "code-clean-repo.sh",
      "content": "#!/bin/bash\n\n# Define the name of the output zip file\nOUTPUT_ZIP=\"code_repo_clean.zip\"\n\n# Define the root directory of your code repository\n# Change this to the actual path of your repository\nREPO_ROOT=\".\" \n\n# Exclude patterns:\n# - .env: Environment variable files\n# - *.bak: Common backup file extension\n# - *~: Editor backup files (e.g., Emacs)\n# - *.tmp: Temporary files\n# - *.log: Log files\n# - .DS_Store: macOS specific metadata file\n# - Thumbs.db: Windows specific thumbnail cache file\n# - *.zone.identifier: Windows security zone information\nEXCLUDE_PATTERNS=(\n    \"*.env\"\n    \"*.bak\"\n    \"*_bk*\"\n    \"*~\"\n    \"*.tmp\"\n    \"*.log\"\n    \".DS_Store\"\n    \"Thumbs.db\"\n    \"*.zone.identifier\"\n    \"*:Zone.Identifier\"\n    \"code_repo_clean_v1/*\"\n    \"*/uploads/*\"\n    \"*node_modules/*\" # Example: exclude node_modules for JavaScript projects\n    \"*/__pycache__/*\" # Exclude all __pycache__ directories\n    \"*/venv/*\" # Exclude the entire venv directory\n    \"*.mypy_cache/*\" # Exclude the top-level mypy cache directory\n    \"*/.mypy_cache/*\" # Exclude mypy's cache directory in subfolders\n    \"*.json*\"\n    \"*/temp_files/*\"\n)\n\n# Construct the exclude arguments for the zip command\nEXCLUDE_ARGS=\"\"\nfor pattern in \"${EXCLUDE_PATTERNS[@]}\"; do\n    EXCLUDE_ARGS+=\" -x \\\"$pattern\\\"\"\ndone\n\n# Create the zip archive, excluding specified patterns\n# The -r flag is for recursive zipping of directories\n# The eval command is used to correctly interpret the EXCLUDE_ARGS string\neval \"zip -r \\\"$OUTPUT_ZIP\\\" \\\"$REPO_ROOT\\\" $EXCLUDE_ARGS\"\n\necho \"Repository successfully zipped to '$OUTPUT_ZIP' excluding specified files.\"\n"
    },
    {
      "path": "./debug_celery.py",
      "filename": "debug_celery.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nStandalone script to debug the Celery worker's synchronous database connection failure.\nThis version uses the fully explicit URL, assuming the .env file is now correct.\n\"\"\"\nimport os\nimport sys\nimport structlog\nimport logging\nfrom sqlalchemy import create_engine, select\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy.engine.base import Engine as SyncEngine\nfrom dotenv import load_dotenv\n\n# --- Path and Configuration Setup ---\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__)))\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\n# Load environment variables\nload_dotenv(os.path.join(PROJECT_ROOT, \".env\"))\n\n# Import the necessary backend components (Config and User model)\nfrom backend.config import get_settings\nfrom backend.database import User, Base \n\n# --- Configuration ---\nsettings = get_settings()\nlogger = structlog.get_logger()\n# Logger setup remains the same\n\nstructlog.configure(\n    processors=[structlog.processors.TimeStamper(fmt=\"%Y-%m-%d %H:%M:%S\"), structlog.processors.JSONRenderer()],\n    wrapper_class=structlog.make_filtering_bound_logger(\n        min_level=logging.INFO\n    ),\n    cache_logger_on_first_use=True,\n)\n\n\ndef init_sync_db_engine() -> SyncEngine:\n    \"\"\"Initializes a synchronous DB engine using the fixed database URL.\"\"\"\n    \n    # CRITICAL FIX: Use the DATABASE_URL directly, assuming it now contains +psycopg2\n    db_url_sync = settings.database_url \n\n    print(f\"DEBUG: Initializing SYNCHRONOUS Engine for DB: {db_url_sync.split('@')[-1]}\")\n    \n    try:\n        # Create a standard synchronous engine\n        engine = create_engine(\n            db_url_sync,\n            echo=False, \n            # Note: connect_args is generally not needed if sslmode is in the URL, \n            # but we keep it minimal here.\n        )\n        # Verify connection immediately\n        with engine.connect():\n            print(\"\u2705 SYNCHRONOUS DB connection successful.\")\n        \n        return engine\n        \n    except Exception as e:\n        print(f\"\u274c FATAL: Synchronous DB Engine initialization failed. Error: {type(e).__name__}: {e}\")\n        raise\n\n# --- Token Fetch Simulation (remains the same) ---\n\ndef simulate_token_fetch(engine: SyncEngine, user_id: int):\n    \"\"\"Simulates the core synchronous token retrieval query.\"\"\"\n    print(f\"\\nDEBUG: Attempting token fetch for user ID {user_id}\")\n\n    try:\n        with Session(engine) as session:\n            # We must use the ORM query since the user model is defined\n            stmt = select(User.gdrive_refresh_token).where(User.id == user_id)\n            token = session.scalar(stmt)\n            \n            if token:\n                print(f\"\u2705 SUCCESS: Token retrieved. Length: {len(token)} chars.\")\n            else:\n                print(\"\u26a0\ufe0f SUCCESS: User found, but token is NULL (not yet synced).\")\n            \n            # The session automatically rolls back here since we didn't commit anything\n\n    except Exception as e:\n        print(\"\\n--- CRITICAL EXCEPTION DURING SYNCHRONOUS QUERY ---\")\n        print(f\"\u274c The query failed! Error: {type(e).__name__}: {e}\")\n        print(\"This indicates a fundamental SQLAlchemy/ORM conflict in the worker context.\")\n        raise\n\n\nif __name__ == \"__main__\":\n    \n    try:\n        # Step 1: Initialize the Synchronous Engine\n        sync_engine = init_sync_db_engine()\n        \n        # Step 2: Run the Fetch Simulation\n        simulate_token_fetch(sync_engine, user_id=1)\n        \n    except Exception as e:\n        print(f\"\\nDEBUGGER ABORTED: {e}\")\n"
    },
    {
      "path": "./debug_db_commit.py",
      "filename": "debug_db_commit.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nStandalone script to debug a specific database commit failure.\nIt simulates the token update logic to expose a hidden SQLAlchemy/asyncpg exception.\n\"\"\"\nimport os\nimport sys\nimport asyncio\nimport structlog\nfrom datetime import datetime\n\n# --- Path and Configuration Setup ---\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__)))\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\n# Import the necessary backend components\nfrom backend.database import init_db, get_db_session, User\nfrom sqlalchemy import update, select # CRITICAL FIX: Added 'select' import\nfrom sqlalchemy.sql import func\nfrom backend.config import get_settings\n\n# --- Setup ---\nlogger = structlog.get_logger()\nsettings = get_settings()\n\nasync def simulate_token_save():\n    print(\"--- Starting Database Commit Debugger ---\")\n    print(f\"DB URL: {settings.database_url.split('@')[-1]}\")\n    \n    # 1. Initialize DB Engine (where the fatal connection error usually happens)\n    try:\n        await init_db()\n        print(\"\u2705 DB Engine Initialized and connection validated.\")\n    except Exception as e:\n        print(f\"\u274c FATAL: DB Engine initialization failed. Error: {e}\")\n        # Print full trace for fatal connection error\n        import traceback\n        traceback.print_exc(file=sys.stdout)\n        return\n\n    # 2. Simulate the Token Data\n    user_id = 1\n    # Use a dummy token string that is the correct data type (Text/str)\n    DUMMY_TOKEN = f\"DEBUG_TOKEN_{datetime.now().isoformat()}\"\n    \n    print(f\"\\nAttempting to save dummy token to user ID: {user_id}\")\n    \n    # 3. Simulate the Transaction and Commit (The exact code that is failing live)\n    try:\n        async with get_db_session() as db:\n            # We must first ensure the user exists for the UPDATE to work\n            # FIX: 'select' is now defined and the query will execute\n            if await db.scalar(select(User.id).where(User.id == user_id)) is None:\n                print(f\"\u274c FATAL: User ID {user_id} not found. Rerun init_db.py!\")\n                return\n            \n            # Execute the update statement from auth.py\n            stmt = update(User).where(User.id == user_id).values(\n                gdrive_refresh_token=DUMMY_TOKEN,\n                gdrive_linked_at=func.now()\n            )\n            await db.execute(stmt)\n            await db.commit() # THIS IS THE LINE THAT FAILS LIVE\n\n        print(f\"\\n\u2705 SUCCESS! Database commit completed for User ID {user_id}.\")\n        print(f\"Token saved: {DUMMY_TOKEN}. Please verify in psql.\")\n\n    except Exception as e:\n        print(\"\\n--- CRITICAL EXCEPTION DURING DB COMMIT ---\")\n        print(f\"\u274c The transaction failed! Error: {e}\")\n        import traceback\n        traceback.print_exc(file=sys.stdout)\n        print(\"------------------------------------------\")\n\n\nif __name__ == \"__main__\":\n    # Ensure this script is run from the project root.\n    try:\n        asyncio.run(simulate_token_save())\n    except RuntimeError as e:\n        print(f\"Runtime Error: {e}. Ensure no other asyncio loop is running.\")\n"
    },
    {
      "path": "./debug_socket.py",
      "filename": "debug_socket.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nStandalone script to debug the WebSocket connection by acting as a client.\nIt connects to the FastAPI /ws endpoint, sends a complete RAG query payload,\nand prints the server's raw response.\n\"\"\"\nimport asyncio\nimport json\nimport websockets\nimport os\nimport sys\n\n# --- Configuration ---\n# Your FastAPI server should be running on localhost:8000\nWS_URL = \"ws://localhost:8000\"\nSESSION_ID = f\"debug_session_{asyncio.get_event_loop().time()}\"\nENDPOINT = f\"{WS_URL}/ws/{SESSION_ID}\"\n\n# Based on your logs, the successful project/tenant IDs are \"6\" and \"1\"\nPAYLOAD = {\n    \"message\": \"What is the primary analytical goal regarding reservoir characteristics and performance?\",\n    \"project_context\": {\n        \"tenant_id\": \"demo\", # Use the slug for tenant consistency\n        \"project_ids\": [\"6\"], # Use the confirmed numeric ID as string for Qdrant filter\n        \"selected_project\": \"Diatomite\"\n    },\n    \"type\": \"chat\"\n}\n\nasync def run_websocket_test():\n    print(f\"--- WebSocket RAG Client Debugger ---\")\n    print(f\"Connecting to: {ENDPOINT}\")\n    print(f\"Sending Payload: {PAYLOAD['message'][:50]}...\")\n    \n    try:\n        # Use websockets.connect to establish the connection\n        async with websockets.connect(ENDPOINT) as websocket:\n            print(\"\u2705 Connection established.\")\n\n            # 1. Send the message payload\n            await websocket.send(json.dumps(PAYLOAD))\n            print(\"INFO: Message sent. Waiting for server response...\")\n\n            # 2. Wait for the server's response\n            response_json = await websocket.recv()\n            response_data = json.loads(response_json)\n\n            print(\"\\n--- SERVER RESPONSE (JSON) ---\")\n            print(json.dumps(response_data, indent=4))\n            print(\"------------------------------\")\n            \n            # Check for success/failure\n            if response_data.get(\"sources\"):\n                print(\"\u2705 RESULT: Context retrieval SUCCEEDED (Sources are present).\")\n            else:\n                print(\"\u274c RESULT: Context retrieval FAILED (No sources provided).\")\n\n    except ConnectionRefusedError:\n        print(\"\\n\u274c ERROR: Connection refused. Ensure FastAPI is running at http://localhost:8000.\")\n    except websockets.exceptions.ConnectionClosedOK:\n        print(\"\\n\u26a0\ufe0f WARNING: Connection closed by server before receiving a full response.\")\n        print(\"This indicates a crash in the backend's WebSocket handler.\")\n    except Exception as e:\n        print(f\"\\n\u274c UNEXPECTED ERROR: {e}\")\n\n\nif __name__ == \"__main__\":\n    try:\n        asyncio.run(run_websocket_test())\n    except Exception as e:\n        print(f\"FATAL SCRIPT ERROR: {e}\")\n"
    },
    {
      "path": "./docker-compose.yml",
      "filename": "docker-compose.yml",
      "content": "version: '3.8'\n\nservices:\n  # Backend API\n  backend:\n    build: \n      context: ./backend\n      dockerfile: Dockerfile\n    ports:\n      - \"8000:8000\"\n    environment:\n      - DATABASE_URL=postgresql://postgres:postgres@postgres:5432/analytics_rag\n      - REDIS_URL=redis://redis:6379/0\n      - QDRANT_URL=http://qdrant:6333\n      - NEO4J_URL=bolt://neo4j:7687\n    depends_on:\n      - postgres\n      - redis\n      - qdrant\n      - neo4j\n    volumes:\n      - ./backend:/app\n    env_file:\n      - .env\n\n  # Frontend Chat Interface\n  frontend:\n    build:\n      context: ./frontend\n      dockerfile: Dockerfile\n    ports:\n      - \"3000:3000\"\n    environment:\n      - VITE_API_URL=http://localhost:8000\n      - VITE_WS_URL=ws://localhost:8000\n    depends_on:\n      - backend\n    volumes:\n      - ./frontend:/app\n\n  # Microsite Server\n  microsite:\n    build:\n      context: ./microsite\n      dockerfile: Dockerfile\n    ports:\n      - \"5173:5173\"\n    environment:\n      - VITE_API_URL=http://localhost:8000\n    depends_on:\n      - backend\n    volumes:\n      - ./microsite:/app\n\n  # PostgreSQL Database\n  postgres:\n    image: postgres:15-alpine\n    environment:\n      - POSTGRES_DB=analytics_rag\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\n  # Redis Cache\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n\n  # Qdrant Vector Database\n  qdrant:\n    image: qdrant/qdrant:latest\n    ports:\n      - \"6333:6333\"\n    volumes:\n      - qdrant_data:/qdrant/storage\n\n  # Neo4j Knowledge Graph\n  neo4j:\n    image: neo4j:5-community\n    ports:\n      - \"7474:7474\"\n      - \"7687:7687\"\n    environment:\n      - NEO4J_AUTH=neo4j/password\n    volumes:\n      - neo4j_data:/data\n\nvolumes:\n  postgres_data:\n  qdrant_data:\n  neo4j_data:\n"
    },
    {
      "path": "./fix_import.py",
      "filename": "fix_import.py",
      "content": "import os\nimport re\nimport sys\n\n# Define the directory to search and the absolute imports to fix\nBACKEND_DIR = os.path.join(os.path.dirname(__file__), \"backend\")\nMODULES_TO_FIX = [\n    \"config\",\n    \"database\",\n    \"celery_app\",\n    \"services.gdrive_service\", \n    \"services.rag_service\",\n    \"services.neo4j_service\",\n    \"parsers.document_parser\",\n]\n\ndef fix_backend_imports(backend_dir: str, modules_to_fix: list):\n    \"\"\"\n    Scans Python files in the backend directory and converts absolute \n    internal imports to relative imports, with robust encoding handling.\n    \"\"\"\n    if not os.path.isdir(backend_dir):\n        print(f\"ERROR: Backend directory not found at {backend_dir}\")\n        sys.exit(1)\n\n    print(f\"Scanning and fixing internal imports in: {backend_dir}\")\n    print(\"-\" * 50)\n    \n    fixed_files_count = 0\n    \n    target_modules_pattern = '|'.join([re.escape(m.split('.')[-1]) for m in modules_to_fix])\n    \n    # Regex to find absolute imports that should be relative. \n    # This remains the same logic.\n    import_regex = re.compile(\n        r'^(from\\s+|import\\s+)(' + target_modules_pattern + r')(\\s+.*)$', \n        re.MULTILINE\n    )\n\n    for root, _, files in os.walk(backend_dir):\n        for filename in files:\n            if filename.endswith('.py'):\n                filepath = os.path.join(root, filename)\n                relative_filepath = os.path.relpath(filepath, BACKEND_DIR)\n                \n                try:\n                    # FIX: Read the file content using a more permissive encoding \n                    # and the 'ignore' handler for safety, as the file is text.\n                    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n                        content = f.read()\n                except Exception as e:\n                    print(f\"  \\u274c FAILED to read {relative_filepath} due to encoding error: {e}\")\n                    continue\n\n                # Replace absolute imports with relative imports (e.g., 'from config' -> 'from .config')\n                new_content = import_regex.sub(r'\\1.\\2\\3', content)\n\n                if new_content != content:\n                    # Write the modified content back to the file\n                    try:\n                        with open(filepath, 'w', encoding='utf-8') as f:\n                            f.write(new_content)\n                        print(f\"  \\u2705 Fixed imports in: {relative_filepath}\")\n                        fixed_files_count += 1\n                    except Exception as e:\n                        print(f\"  \\u274c FAILED to write fixed content to {relative_filepath}: {e}\")\n\n    print(\"-\" * 50)\n    print(f\"Import fixing complete. Total files modified: {fixed_files_count}\")\n\n\nif __name__ == \"__main__\":\n    # Ensure this script is run from the project root: ~/rag-folder/code_repo_clean_exploded/\n    fix_backend_imports(BACKEND_DIR, MODULES_TO_FIX)\n"
    },
    {
      "path": "./README.md",
      "filename": "README.md",
      "content": "# Analytics RAG Platform\n\nEnterprise-grade RAG system for analytics companies with multi-tenant project management and executive chat interface.\n\n## Features\n\n- Multi-tenant Google Drive integration (via **User-Specific OAuth**)\n- Intelligent document parsing (PDF, PPTX, Excel, Word)  \n- Project-aware RAG with knowledge graphs\n- Executive chat interface with microsite generation\n- LLM agents using Gemini API\n- Docker-based deployment\n\n## Quick Setup\n\n1. Copy `.env.example` to `.env` and configure:\n   ```bash\n   cp .env.example .env\n   # Edit .env with your API keys and Google OAuth Client IDs\n   ```\n\n2. Start services:\n   ```bash\n   docker-compose up -d\n   ```\n\n3. Initialize database:\n   ```bash\n   docker-compose exec backend python scripts/init_db.py\n   ```\n\n4. Access the application:\n   - Chat Interface: http://localhost:3000\n   - API Documentation: http://localhost:8000/docs\n   - Microsite Preview: http://localhost:5173\n\n## Development\n\nSee docs/DEPLOYMENT.md for detailed setup instructions.\n"
    },
    {
      "path": "./requirements.txt",
      "filename": "requirements.txt",
      "content": "# Core Framework\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\npydantic==2.5.0\npydantic-settings==2.1.0\n\n# Database & ORM\nsqlalchemy==2.0.23\nalembic==1.13.1\nasyncpg==0.29.0\npsycopg2-binary==2.9.9\n\n# Redis & Caching\nredis==5.0.1\ncelery==5.3.4\n\n# Vector Database\nqdrant-client==1.7.0\nsentence-transformers==2.2.2\n\n# Knowledge Graph\nneo4j==5.15.0\n\n# Google Services\ngoogle-api-python-client==2.108.0\ngoogle-auth==2.25.2\n# NEW: Crucial for user-based OAuth flow\ngoogle-auth-oauthlib==1.1.0\ngoogle-auth-httplib2==0.2.0\n\n# LLM & AI\ngoogle-generativeai==0.3.2\nlangchain==0.0.350\nlangchain-google-genai==0.0.6\n\n# Document Processing\npython-multipart==0.0.6\npython-docx==1.1.0\npython-pptx==0.6.23\nPyPDF2==3.0.1\npymupdf==1.23.8\nopenpyxl==3.1.2\npandas==2.1.4\nunstructured==0.11.8\n\n# Web & HTTP\nhttpx==0.25.2\nwebsockets==12.0\naiofiles==23.2.1\n\n# Security & Auth\npython-jose[cryptography]==3.3.0\npasslib[bcrypt]==1.7.4\n\n# Utilities\npython-dotenv==1.0.0\nstructlog==23.2.0\nrich==13.7.0\ntyper==0.9.0\n\n# Testing\npytest==7.4.3\npytest-asyncio==0.21.1\n"
    },
    {
      "path": "./token.txt",
      "filename": "token.txt",
      "content": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWUsImlhdCI6MTUxNjIzOTAyMn0.KMUFsIDTnFmyG3nMiGM6H9FNFUROf3wh7SmqJp-QV30"
    },
    {
      "path": "./update_file.py",
      "filename": "update_file.py",
      "content": "import json\nimport os\nfrom typing import Dict, Any\n\n# --- Simulated Reading of the JSON File ---\n# In a real scenario, this block would be: \n# with open('rag_service_update.json', 'r') as f: data = json.load(f)\n\n# The content provided by the user is structured as a Python dictionary.\n# We simulate loading it from the JSON file by assigning it directly.\nRAG_SERVICE_UPDATE_JSON_CONTENT = {\n  \"files\": [\n    {\n      \"path\": \"backend/services/history_service.py\",\n      \"content\": \"#!/usr/bin/env python3\\nimport redis.asyncio as redis\\nimport structlog\\nimport json\\nfrom typing import List, Dict, Any, Optional\\n\\n# Use multi-level parent relative import for config\\nfrom ..config import get_settings\\n\\nlogger = structlog.get_logger()\\nsettings = get_settings()\\n\\nclass HistoryService:\\n    \\\"\\\"\\\"\\n    Manages chat history persistence in Redis for conversational context.\\n    The history is stored as a list of JSON objects.\\n    \\\"\\\"\\\"\\n    def __init__(self, redis_client: redis.Redis):\\n        self.redis = redis_client\\n        # Key format: 'chat_history:{session_id}'\\n        self.key_prefix = \\\"chat_history:\\\"\\n        self.max_history_length = 10  # Store last 10 messages (5 user, 5 assistant)\\n        self.history_ttl = 3600  # History expires after 1 hour (3600 seconds)\\n\\n    def _get_key(self, session_id: str) -> str:\\n        \\\"\\\"\\\"Generates the Redis key for a session.\\\"\\\"\\\"\\n        return f\\\"{self.key_prefix}{session_id}\\\"\\n\\n    async def get_history(self, session_id: str) -> List[Dict[str, str]]:\\n        \\\"\\\"\\\"Retrieves the full conversation history for a session.\\\"\\\"\\\"\\n        key = self._get_key(session_id)\\n        \\n        # Redis returns a list of JSON strings\\n        history_json_list = await self.redis.lrange(key, 0, -1)\\n        \\n        history = []\\n        for json_str in history_json_list:\\n            try:\\n                # Decode the JSON string back into a Python dictionary\\n                history.append(json.loads(json_str))\\n            except json.JSONDecodeError:\\n                logger.error(\\\"Failed to decode history item.\\\", json_str=json_str)\\n                continue\\n                \\n        # Return history in chronological order (oldest first)\\n        return history\\n\\n    async def add_message(self, session_id: str, role: str, content: str):\\n        \\\"\\\"\\\"Adds a new message to the session history.\\\"\\\"\\\"\\n        key = self._get_key(session_id)\\n        \\n        message = {\\n            \\\"role\\\": role,\\n            \\\"content\\\": content\\n        }\\n        message_json = json.dumps(message)\\n        \\n        # 1. Add the new message to the right (end of list)\\n        await self.redis.rpush(key, message_json)\\n        \\n        # 2. Trim the list to the maximum length (removes oldest messages from the left)\\n        await self.redis.ltrim(key, -self.max_history_length, -1)\\n        \\n        # 3. Reset the expiry time\\n        await self.redis.expire(key, self.history_ttl)\\n        \\n        logger.debug(\\\"Message added to history.\\\", session_id=session_id, role=role, content=content[:50])\\n\\n    async def clear_history(self, session_id: str):\\n        \\\"\\\"\\\"Clears the history for a session.\\\"\\\"\\\"\\n        key = self._get_key(session_id)\\n        await self.redis.delete(key)\\n        logger.info(\\\"Chat history cleared.\\\", session_id=session_id)\\n\\n    async def format_history_for_prompt(self, session_id: str) -> List[Dict[str, str]]:\\n        \\\"\\\"\\\"Retrieves history and formats it for LLM chat messages (Gemini format).\\\"\\\"\\\"\\n        raw_history = await self.get_history(session_id)\\n        \\n        # Gemini API chat format expects role: 'user' or 'model'\\n        formatted_history = []\\n        for msg in raw_history:\\n            # Map internal 'assistant' role to Gemini's 'model' role\\n            role = 'model' if msg.get('role') == 'assistant' else msg.get('role', 'user')\\n            \\n            # Ensure only 'user' and 'model' roles are used\\n            if role in ['user', 'model']:\\n                formatted_history.append({\\n                    \\\"role\\\": role,\\n                    \\\"parts\\\": [{\\\"text\\\": msg.get(\\\"content\\\", \\\"\\\")}]\\n                })\\n        \\n        return formatted_history\\n\"\n    },\n    {\n      \"path\": \"backend/services/artifact_service.py\",\n      \"content\": \"#!/usr/bin/env python3\\nimport io\\nimport structlog\\nfrom typing import Dict, Any, List, Optional\\nimport uuid\\n# Imported libraries are assumed to be in requirements.txt (docx, openpyxl, pptx)\\nfrom docx import Document\\nfrom openpyxl import Workbook\\nfrom pptx import Presentation\\n# Importing utilities to allow for robust PPTX creation (even with mock data)\\nfrom pptx.util import Inches\\n\\nfrom ..config import get_settings\\n\\nlogger = structlog.get_logger()\\nsettings = get_settings()\\n\\nclass ArtifactService:\\n    \\\"\\\"\\\"\\n    Handles the generation of Project Management artifacts (Excel, Word, PPTX).\\n    Methods return file bytes.\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        pass\\n\\n    def _generate_excel_risk_register(self, project_name: str, risks: List[Dict[str, Any]]) -> bytes:\\n        \\\"\\\"\\\"Generates a simple Excel file (Risk Register) and returns its bytes.\\\"\\\"\\\"\\n        wb = Workbook()\\n        ws = wb.active\\n        ws.title = \\\"Risk Register\\\"\\n        \\n        # Headers\\n        headers = [\\\"Risk ID\\\", \\\"Description\\\", \\\"Impact\\\", \\\"Probability\\\", \\\"Status\\\", \\\"Mitigation Plan\\\", \\\"Owner\\\"]\\n        ws.append(headers)\\n\\n        # Simple data based on mock or LLM summary\\n        for i, risk in enumerate(risks, 2):\\n            ws.append([\\n                i - 1,\\n                risk.get(\\\"description\\\", \\\"Geological uncertainty.\\\"),\\n                risk.get(\\\"impact\\\", \\\"High\\\"),\\n                risk.get(\\\"probability\\\", \\\"Medium\\\"),\\n                \\\"Open\\\",\\n                risk.get(\\\"mitigation\\\", \\\"Increased seismic survey.\\\"),\\n                risk.get(\\\"owner\\\", \\\"Geology Team\\\")\\n            ])\\n\\n        file_stream = io.BytesIO()\\n        wb.save(file_stream)\\n        file_stream.seek(0)\\n        return file_stream.read()\\n\\n    def _generate_word_status_report(self, project_name: str, summary_content: str) -> bytes:\\n        \\\"\\\"\\\"Generates a simple Word file (Status Report) and returns its bytes.\\\"\\\"\\\"\\n        doc = Document()\\n        \\n        doc.add_heading(f\\\"Project Status Report: {project_name}\\\", 0)\\n        doc.add_paragraph(f\\\"Report Generated: {structlog.get_logger().info('Current time').get('timestamp')}\\\")\\n        \\n        doc.add_heading(\\\"Executive Summary\\\", 1)\\n        doc.add_paragraph(summary_content)\\n\\n        doc.add_heading(\\\"Spud Prediction Key Insights\\\", 1)\\n        p = doc.add_paragraph(\\\"Based on the latest data for the target Q4 window, the spud predictive model remains highly accurate, maintaining an 85% confidence level for identified sites.\\\")\\n        \\n        file_stream = io.BytesIO()\\n        doc.save(file_stream)\\n        file_stream.seek(0)\\n        return file_stream.read()\\n\\n    def _generate_pptx_executive_pitch(self, project_name: str, key_metrics: Dict[str, Any]) -> bytes:\\n        \\\"\\\"\\\"Generates a simple PPTX file (Executive Pitch) and returns its bytes.\\\"\\\"\\\"\\n        prs = Presentation()\\n        \\n        # Slide 1: Title Slide\\n        title_slide_layout = prs.slide_layouts[0]\\n        slide = prs.slides.add_slide(title_slide_layout)\\n        slide.shapes.title.text = f\\\"Executive Pitch: {project_name}\\\"\\n        slide.placeholders[1].text = \\\"Predicting Spud Activity Success | PMO Analytics\\\"\\n\\n        # Slide 2: Key Metrics Slide\\n        bullet_slide_layout = prs.slide_layouts[1]\\n        slide = prs.slides.add_slide(bullet_slide_layout)\\n        slide.shapes.title.text = \\\"Key Success Factors\\\"\\n\\n        body = slide.shapes.placeholders[1]\\n        tf = body.text_frame\\n        tf.clear()  \\n        \\n        p = tf.add_paragraph()\\n        p.text = f\\\"Predicted Spud Sites: {key_metrics.get('sites', 'Sites A & C')}\\\"\\n        p.level = 0\\n        \\n        p = tf.add_paragraph()\\n        p.text = f\\\"Confidence Level: {key_metrics.get('confidence', '85%')}\\\"\\n        p.level = 1\\n        \\n        file_stream = io.BytesIO()\\n        prs.save(file_stream)\\n        file_stream.seek(0)\\n        return file_stream.read()\\n\\n\\n    async def generate_artifact(self, artifact_type: str, project_name: str, data: Dict[str, Any] = None) -> Optional[bytes]:\\n        \\\"\\\"\\\"Routes the generation request to the correct internal method.\\\"\\\"\\\"\\n        if data is None:\\n            data = {}\\n        \\n        try:\\n            if artifact_type == \\\"excel_risk_register\\\":\\n                risks = data.get(\\\"risks\\\", [{\\\"description\\\": data.get(\\\"summary\\\", \\\"New risk identified.\\\"), \\\"impact\\\": \\\"Medium\\\", \\\"owner\\\": \\\"AI\\\"}])\\n                return self._generate_excel_risk_register(project_name, risks)\\n            \\n            elif artifact_type == \\\"word_status_report\\\":\\n                summary = data.get(\\\"summary\\\", \\\"Project status is Green.\\\")\\n                return self._generate_word_status_report(project_name, summary)\\n            \\n            elif artifact_type == \\\"pptx_executive_pitch\\\":\\n                key_metrics = data.get(\\\"key_metrics\\\", {\\\"confidence\\\": \\\"85%\\\", \\\"sites\\\": \\\"Sites A & C\\\"})\\n                return self._generate_pptx_executive_pitch(project_name, key_metrics)\\n            \\n            else:\\n                logger.error(\\\"Unknown artifact type requested.\\\", type=artifact_type)\\n                return None\\n        \\n        except Exception as e:\\n            logger.error(\\\"Failed to generate artifact.\\\", type=artifact_type, error=str(e))\\n            raise\"\n    },\n    {\n      \"path\": \"backend/api/routes/artifacts.py\",\n      \"content\": \"#!/usr/bin/env python3\\nfrom fastapi import APIRouter, HTTPException, status, Depends\\nfrom fastapi.responses import Response, JSONResponse\\nfrom typing import Dict, Any, Union\\nimport structlog\\nimport uuid\\n\\n# Use multi-level parent relative imports\\nfrom ...services.artifact_service import ArtifactService\\nfrom ...main import get_artifact_service, manager # Import manager for WebSocket access\\n\\nrouter = APIRouter()\\nlogger = structlog.get_logger()\\n\\n# --- Mock Data Store (In a real system, this would be S3 or GCS) ---\\n# Stores temporary artifact bytes with a UUID key and metadata\\nARTIFACT_CACHE: Dict[str, Dict[str, Union[bytes, str]]] = {} \\n# ------------------------------------------------------------------\\n\\ndef add_artifact_to_cache(file_bytes: bytes, filename: str, mime_type: str) -> Dict[str, str]:\\n    \\\"\\\"\\\"Stores the generated file in a temporary cache and returns metadata.\\\"\\\"\\\"\\n    file_id = str(uuid.uuid4())\\n    ARTIFACT_CACHE[file_id] = {\\n        \\\"bytes\\\": file_bytes,\\n        \\\"filename\\\": filename,\\n        \\\"mime_type\\\": mime_type\\n    }\\n    \\n    # Return metadata for the chat response\\n    return {\\n        \\\"id\\\": file_id,\\n        \\\"filename\\\": filename,\\n        \\\"mime_type\\\": mime_type,\\n        \\\"size_bytes\\\": str(len(file_bytes))\\n    }\\n\\n@router.post(\\\"/generate\\\")\\nasync def generate_artifact_endpoint(\\n    request_data: Dict[str, Union[str, Dict[str, Any]]],\\n    artifact_service: ArtifactService = Depends(get_artifact_service)\\n):\\n    \\\"\\\"\\\"\\n    Called internally by the AgentOrchestrator to create a file \\n    and send a temporary download link via WebSocket.\\n    \\\"\\\"\\\"\\n    artifact_type = request_data.get(\\\"artifact_type\\\")\\n    project_name = request_data.get(\\\"project_name\\\")\\n    session_id = request_data.get(\\\"session_id\\\")\\n    data = request_data.get(\\\"data\\\", {})\\n\\n    if not all([artifact_type, project_name, session_id]):\\n        raise HTTPException(status_code=400, detail=\\\"Missing artifact_type, project_name, or session_id.\\\")\\n\\n    try:\\n        file_bytes = await artifact_service.generate_artifact(artifact_type, project_name, data)\\n        \\n        if file_bytes is None:\\n            raise HTTPException(status_code=500, detail=\\\"Artifact generation failed internally.\\\")\\n\\n        # Determine file metadata based on type\\n        mime_type = \\\"application/octet-stream\\\"\\n        if artifact_type == \\\"excel_risk_register\\\":\\n            filename = f\\\"{project_name}_Risk_Register.xlsx\\\"\\n            mime_type = \\\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\\\"\\n        elif artifact_type == \\\"word_status_report\\\":\\n            filename = f\\\"{project_name}_Status_Report.docx\\\"\\n            mime_type = \\\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\\\"\\n        elif artifact_type == \\\"pptx_executive_pitch\\\":\\n            filename = f\\\"{project_name}_Executive_Pitch.pptx\\\"\\n            mime_type = \\\"application/vnd.openxmlformats-officedocument.presentationml.presentation\\\"\\n        else:\\n            filename = f\\\"{project_name}_Artifact.bin\\\"\\n\\n        metadata = add_artifact_to_cache(file_bytes, filename, mime_type)\\n        \\n        # Send a direct WebSocket message to the user about the generated artifact\\n        await manager.send_message(session_id, {\\n            \\\"type\\\": \\\"artifact_generated\\\",\\n            \\\"session_id\\\": session_id,\\n            \\\"artifact\\\": {\\n                \\\"id\\\": metadata[\\\"id\\\"],\\n                \\\"filename\\\": metadata[\\\"filename\\\"],\\n                \\\"mime_type\\\": metadata[\\\"mime_type\\\"],\\n                \\\"download_url\\\": f\\\"/api/v1/artifacts/download/{metadata['id']}\\\" # Relative path\\n            }\\n        })\\n\\n        # Return a simple confirmation to the orchestrator\\n        return {\\\"status\\\": \\\"success\\\", \\\"artifact_id\\\": metadata[\\\"id\\\"]}\\n\\n    except Exception as e:\\n        logger.error(\\\"Artifact generation endpoint failed.\\\", error=str(e))\\n        raise HTTPException(status_code=500, detail=f\\\"Failed to generate artifact: {e}\\\")\\n\\n@router.get(\\\"/download/{file_id}\\\")\\nasync def download_artifact_endpoint(file_id: str):\\n    \\\"\\\"\\\"Retrieves and serves the temporary artifact file.\\\"\\\"\\\"\\n    if file_id not in ARTIFACT_CACHE:\\n        raise HTTPException(status_code=404, detail=\\\"File not found or expired.\\\")\\n\\n    # Retrieve data from cache\\n    artifact_data = ARTIFACT_CACHE[file_id]\\n    file_bytes = artifact_data[\\\"bytes\\\"]\\n    filename = artifact_data[\\\"filename\\\"]\\n    mime_type = artifact_data[\\\"mime_type\\\"]\\n    \\n    # Optional: Delete from cache after download to clean up mock storage\\n    del ARTIFACT_CACHE[file_id] \\n\\n    return Response(\\n        content=file_bytes,\\n        media_type=mime_type,\\n        headers={\\n            \\\"Content-Disposition\\\": f\\\"attachment; filename=\\\\\\\"{filename}\\\\\\\"\",\\n            \\\"Content-Length\\\": str(len(file_bytes))\\n        }\\n    )\\n\"\n    },\n    {\n      \"path\": \"backend/services/llm_service.py\",\n      \"content\": \"#!/usr/bin/env python3\\nimport asyncio\\nfrom typing import Dict, Any, Optional, List\\nimport google.generativeai as genai\\nimport structlog\\nimport json\\nimport re \\n\\n# CORRECTED: Use parent-level import (..) to find config.py\\nfrom ..config import get_settings \\n\\nlogger = structlog.get_logger()\\nsettings = get_settings()\\n\\n\\n# --- Function Definition for Gemini ---\\nARTIFACT_FUNCTION_SCHEMA = {\\n    \\\"name\\\": \\\"generate_project_artifact\\\",\\n    \\\"description\\\": \\\"Generates a structured Project Management artifact (Excel Risk Register, Word Status Report, or PowerPoint Executive Pitch) based on the user's request and project context.\\\",\\n    \\\"parameters\\\": {\\n        \\\"type\\\": \\\"OBJECT\\\",\\n        \\\"properties\\\": {\\n            \\\"artifact_type\\\": {\\n                \\\"type\\\": \\\"STRING\\\",\\n                \\\"description\\\": \\\"The type of artifact to generate. Must be one of: 'excel_risk_register', 'word_status_report', 'pptx_executive_pitch'.\\\"\\n            },\\n            \\\"project_name\\\": {\\n                \\\"type\\\": \\\"STRING\\\",\\n                \\\"description\\\": \\\"The name of the currently selected project (e.g., 'Stone Hill Spud Prediction').\\\"\\n            },\\n            \\\"summary_content\\\": {\\n                \\\"type\\\": \\\"STRING\\\",\\n                \\\"description\\\": \\\"A brief, 2-3 sentence summary of the artifact's core content, derived from the user's query and RAG context.\\\"\\n            }\\n        },\\n        \\\"required\\\": [\\\"artifact_type\\\", \\\"project_name\\\", \\\"summary_content\\\"]\\n    }\\n}\\n# --------------------------------------\\n\\n\\nclass LLMService:\\n    \\\"\\\"\\\"\\n    Handles interactions with the Gemini LLM, including response generation\\n    and query classification.\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        genai.configure(api_key=settings.gemini_api_key)\\n        self.model = genai.GenerativeModel('gemini-2.5-flash')\\n        \\n        self.generation_config = {\\n            'temperature': 0.1,\\n            'top_p': 0.9,\\n            'top_k': 40,\\n            'max_output_tokens': 2048,\\n        }\\n        \\n        self.tools = [ARTIFACT_FUNCTION_SCHEMA]\\n\\n\\n    async def generate_response(self, prompt: str, history: List[Dict[str, Any]] = None) -> Dict[str, Any]:\\n        \\\"\\\"\\\"\\n        Generates a text response from the LLM, potentially involving function calls.\\n        \\n        Returns:\\n            Dict: {'text': str, 'function_call': Optional[Dict]}\\n        \\\"\\\"\\\"\\n        \\n        full_contents = history if history is not None else []\\n        full_contents.append({\\\"role\\\": \\\"user\\\", \\\"parts\\\": [{\\\"text\\\": prompt}]})\\n        \\n        try:\\n            # Placeholder Logic: Detect keywords that imply artifact generation\\n            if re.search(r'generate|create|draft|export|excel|word|pptx|report|log|pitch', prompt.lower()):\\n                artifact_type = None\\n                if 'excel' in prompt.lower() or 'risk log' in prompt.lower():\\n                    artifact_type = 'excel_risk_register'\\n                elif 'word' in prompt.lower() or 'status report' in prompt.lower():\\n                    artifact_type = 'word_status_report'\\n                elif 'pptx' in prompt.lower() or 'executive pitch' in prompt.lower():\\n                    artifact_type = 'pptx_executive_pitch'\\n                \\n                if artifact_type:\\n                    # Mock the function call result\\n                    function_call_mock = {\\n                        \\\"name\\\": \\\"generate_project_artifact\\\",\\n                        \\\"args\\\": {\\n                            \\\"artifact_type\\\": artifact_type,\\n                            \\\"project_name\\\": \\\"Stone Hill Spud Prediction\\\", \\n                            \\\"summary_content\\\": f\\\"Drafting the artifact based on the current context and the request: '{prompt[:50]}...'\\\"\\n                        }\\n                    }\\n                    return {\\\"text\\\": None, \\\"function_call\\\": function_call_mock}\\n\\n\\n            # Normal generation call (wrapped in a thread)\\n            response = await asyncio.to_thread(\\n                self.model.generate_content,\\n                contents=full_contents,\\n                generation_config=self.generation_config,\\n                tools=self.tools # Passed to enable function calling\\n            )\\n            \\n            return {\\\"text\\\": response.text, \\\"function_call\\\": None}\\n\\n        except Exception as e:\\n            logger.error(\\\"Failed to generate response/handle function call.\\\", error=str(e))\\n            return {\\\"text\\\": \\\"I apologize, but I encountered a service error while trying to process your request.\\\", \\\"function_call\\\": None}\\n\\n\\n    async def classify_query_intent(self, query: str) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Classifies the user query's intent using the LLM.\\\"\\\"\\\"\\n        # Added 'requires_artifact' field to the classification output\\n        classification_prompt = f\\\"\\\"\\\"\\n        Classify the following query into categories and determine if it requires a microsite/dashboard or an explicit structured artifact (Excel/Word/PPTX) response.\\n\\n        Query: \\\"{query}\\\"\\n\\n        Categories:\\n        - project_overview: Questions about project summaries, objectives\\n        - kpi_metrics: Questions about KPIs, metrics, performance data\\n        - value_outcomes: Questions about business value, ROI\\n        - microsite_request: Explicit request to generate a dashboard/microsite.\\n        - artifact_request: Explicit request to generate a structured document (e.g., 'risk log', 'status report', 'pitch deck').\\n\\n        Respond *only* with a single JSON object in this exact format: {{\\\"primary_intent\\\": \\\"category\\\", \\\"requires_microsite\\\": true/false, \\\"requires_artifact\\\": true/false}}\\n        \\\"\\\"\\\"\\n\\n        try:\\n            response_data = await self.generate_response(prompt=classification_prompt)\\n            response_json_string = response_data['text']\\n\\n            # Safely attempt to parse the JSON string response\\n            try:\\n                start = response_json_string.find('{')\\n                end = response_json_string.rfind('}')\\n                if start != -1 and end != -1:\\n                    clean_json_string = response_json_string[start:end+1]\\n                else:\\n                    clean_json_string = response_json_string\\n                    \\n                result = json.loads(clean_json_string)\\n                # Ensure boolean types\\n                if 'requires_microsite' in result and isinstance(result['requires_microsite'], str):\\n                    result['requires_microsite'] = result['requires_microsite'].lower() == 'true'\\n                if 'requires_artifact' in result and isinstance(result['requires_artifact'], str):\\n                    result['requires_artifact'] = result['requires_artifact'].lower() == 'true'\\n                \\n                return result\\n            except json.JSONDecodeError:\\n                logger.warning(\\\"Failed to decode intent classification JSON. Defaulting intent.\\\", response=response_json_string)\\n                return {\\\"primary_intent\\\": \\\"general_inquiry\\\", \\\"requires_microsite\\\": False, \\\"requires_artifact\\\": False}\\n        except Exception as e:\\n            logger.error(\\\"Failed to classify query intent.\\\", error=str(e))\\n            return {\\\"primary_intent\\\": \\\"general_inquiry\\\", \\\"requires_microsite\\\": False, \\\"requires_artifact\\\": False}\\n\"\n    },\n    {\n      \"path\": \"backend/services/rag_service.py\",\n      \"content\": \"#!/usr/bin/env python3\\nimport asyncio\\nfrom typing import List, Dict, Any, Optional\\nfrom sentence_transformers import SentenceTransformer\\nimport structlog\\nfrom qdrant_client.http.exceptions import UnexpectedResponse\\n\\n# CORRECTED: Use parent-level import (..) to find config.py\\nfrom ..config import get_settings\\n# CORRECTED: Use relative imports for sibling services\\nfrom .llm_service import LLMService\\nfrom .qdrant_service import QdrantService \\n\\nlogger = structlog.get_logger()\\nsettings = get_settings()\\n\\n\\nclass RAGService:\\n    def __init__(self):\\n        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\\n        self.llm_service = LLMService()\\n        self.qdrant_service = QdrantService()\\n\\n    def create_embeddings(self, texts: List[str]) -> List[List[float]]:\\n        \\\"\\\"\\\"Generates embeddings for a list of text strings.\\\"\\\"\\\"\\n        embeddings = self.embedding_model.encode(texts, convert_to_numpy=True).tolist()\\n        return embeddings\\n\\n    async def index_document_chunks(self, augmented_chunks: List[Dict[str, Any]]):\\n        if not augmented_chunks:\\n            logger.warning(\\\"No chunks provided for indexing.\\\")\\n            return\\n\\n        texts = [chunk['content'] for chunk in augmented_chunks]\\n        embeddings = await asyncio.to_thread(self.create_embeddings, texts)\\n        \\n        if len(embeddings) != len(augmented_chunks):\\n            logger.error(\\\"Embedding count mismatch with chunk count. Aborting index.\\\")\\n            return\\n\\n        points_data = []\\n        logger.info(\\\"Preparing points for Qdrant indexing.\\\", count=len(augmented_chunks))\\n        \\n        for i, chunk in enumerate(augmented_chunks):\\n            payload = chunk.copy() \\n            \\n            points_data.append({\\n                \\\"vector\\\": embeddings[i],\\n                \\\"payload\\\": payload \\n            })\\n\\n        try:\\n            await self.qdrant_service.index_points(points_data)\\n            logger.info(\\\"Batch indexing complete.\\\", count=len(points_data))\\n        except UnexpectedResponse as e:\\n            logger.error(\\\"Qdrant indexing failed due to unexpected response.\\\", error=str(e))\\n            raise \\n        except Exception as e:\\n            logger.error(\\\"General error during Qdrant indexing.\\\", error=str(e))\\n            raise \\n\\n\\n    async def retrieve_context(self, query: str, tenant_id: str, project_ids: List[str], limit: int = 10) -> List[Dict[str, Any]]:\\n        \\n        standardized_tenant_id = \\\"1\\\" if tenant_id == \\\"demo\\\" else tenant_id\\n        \\n        query_embedding_list = await asyncio.to_thread(self.create_embeddings, [query])\\n        query_embedding = query_embedding_list[0]\\n\\n        retrieved_results = await self.qdrant_service.search_vectors(\\n            query_embedding=query_embedding,\\n            project_ids=project_ids,\\n            tenant_id=standardized_tenant_id,\\n            limit=limit\\n        )\\n\\n        context = []\\n        for result in retrieved_results:\\n            payload = result['payload']\\n            context.append({\\n                \\\"content\\\": payload.get('content', 'Content not available.').strip(),\\n                \\\"score\\\": result.get('score', 0.0),\\n                \\\"source_file\\\": payload.get('source_file', 'Unknown File'),\\n                \\\"project_id\\\": payload.get('project_id', 'Unknown Project'),\\n                \\\"document_id\\\": payload.get('document_id', 'Unknown Doc'),\\n                \\\"context_type\\\": \\\"document_chunk\\\"\\n            })\\n            \\n        logger.info(\\\"Context retrieved.\\\", query=query, retrieved_count=len(context))\\n        return context\\n\\n\\n    async def generate_response(self, query: str, context: List[Dict[str, Any]], history: List[Dict[str, Any]], project_context: Dict[str, Any] = None) -> Dict[str, Any]:\\n        \\\"\\\"\\\"\\n        Generates a final answer using the LLM based on the query, retrieved context, and chat history.\\n        \\\"\\\"\\\"\\n        context_text = \\\"\\\\n\\\\n\\\".join([f\\\"[{ctx['source_file']}/{ctx['project_id']}] {ctx['content']}\\\" for ctx in context])\\n        \\n        has_context = bool(context_text.strip())\\n        \\n        # Format RAG context for the prompt\\n        rag_context_section = f\\\"CONTEXT: {context_text if has_context else 'NO_RAG_CONTEXT_AVAILABLE'}\\\"\\n        \\n        # Format chat history for the prompt (as a summary for grounding)\\n        history_summary_parts = []\\n        for msg in history:\\n            role = msg.get('role', 'user')\\n            # Extract content from Gemini API format parts\\n            content = msg.get('parts', [{}])[0].get('text', '')\\n            history_summary_parts.append(f\\\"{role.upper()}: {content}\\\")\\n        history_summary = \\\"\\\\n\\\".join(history_summary_parts)\\n        \\n        prompt = f\\\"\\\"\\\"\\n        You are an expert financial and technical analyst for the \\\"Analytics RAG Platform\\\" specializing in the oil and gas sector.\\n        \\n        Your primary goal is to provide a helpful, structured response to the user's question, **while respecting the conversation history**.\\n\\n        ---\\n        CONVERSATION_HISTORY: {history_summary if history_summary else \\\"No previous conversation.\\\"}\\n        \\n        {rag_context_section}\\n        \\n        Question: {query}\\n        ---\\n\\n        INSTRUCTION:\\n        1. **Contextualize**: Use the **CONVERSATION_HISTORY** to understand the user's intent. \\n        2. **Citation & Grounding**: If CONTEXT is available, you **MUST** use the facts and terminology found in it. Any direct reference to project-specific data (e.g., spud activity predictions) from the CONTEXT **MUST** be immediately followed by a citation in the format [file/project].\\n        3. **Constraint**: Do not mention that you have or lack context. Do not include the CONTEXT text in your final answer.\\n        4. **Function Call**: Do NOT try to output a function call result here. If a function call is needed, the `LLMService` will handle it before this prompt is executed.\\n        \\n        Your Answer (Format the response professionally, e.g., using markdown lists/headings):\\n        \\\"\\\"\\\"\\n        \\n        # Pass the full history (in Gemini format) for the actual API call\\n        response_data = await self.llm_service.generate_response(prompt, history=history)\\n        response_text = response_data['text']\\n        function_call = response_data.get('function_call')\\n\\n        sources_list = []\\n        unique_sources = set()\\n        for ctx in context:\\n            source_tuple = (ctx[\\\"source_file\\\"], ctx[\\\"project_id\\\"])\\n            if source_tuple not in unique_sources:\\n                unique_sources.add(source_tuple)\\n                sources_list.append({\\n                    \\\"file\\\": ctx[\\\"source_file\\\"], \\n                    \\\"project\\\": ctx[\\\"project_id\\\"], \\n                    \\\"type\\\": ctx[\\\"context_type\\\"],\\n                    \\\"score\\\": ctx[\\\"score\\\"]\\n                })\\n\\n        return {\\n            \\\"response\\\": response_text,\\n            \\\"sources\\\": sources_list,\\n            \\\"context_used\\\": len(context),\\n            \\\"function_call\\\": function_call # Pass function call back to orchestrator\\n        }\"\n    },\n    {\n      \"path\": \"backend/services/agent_orchestrator.py\",\n      \"content\": \"#!/usr/bin/env python3\\nimport asyncio\\nfrom typing import Dict, Any, List\\nimport structlog\\nimport httpx \\nfrom fastapi import Depends\\n\\n# CORRECTED: Use relative imports for modules within the 'backend' package\\nfrom .rag_service import RAGService\\nfrom .llm_service import LLMService\\nfrom .gdrive_service import GoogleDriveService \\nfrom .history_service import HistoryService\\nfrom ..main import get_history_service \\n\\nlogger = structlog.get_logger()\\n\\n# Constants for project name mapping\\nPROJECT_ID_TO_NAME = {\\n    \\\"1\\\": \\\"MARS Analytics Platform\\\", \\n    \\\"6\\\": \\\"Stone Hill Spud Prediction\\\" # Mocking the project ID for the user's use case\\n}\\nDEFAULT_PROJECT_NAME = \\\"Analytics Project\\\"\\n\\n\\nclass AgentOrchestrator:\\n    def __init__(self, history_service: HistoryService):\\n        self.rag_service = RAGService()\\n        self.llm_service = LLMService()\\n        self.history_service = history_service\\n        # Use httpx for internal service calls (e.g., to the local artifacts endpoint)\\n        self.http_client = httpx.AsyncClient(base_url=\\\"http://localhost:8000\\\") \\n\\n    async def process_message(self, user_id: str, session_id: str, message: str, project_context: Dict[str, Any], message_type: str = \\\"chat\\\") -> Dict[str, Any]:\\n        \\n        # 0. Add user message to history immediately\\n        await self.history_service.add_message(session_id, \\\"user\\\", message)\\n        \\n        try:\\n            # 1. Get current conversation history for LLM grounding\\n            history = await self.history_service.format_history_for_prompt(session_id)\\n            \\n            # 2. Get project details\\n            tenant_id = project_context.get(\\\"tenant_id\\\", \\\"demo\\\")\\n            project_ids = project_context.get(\\\"project_ids\\\", [])\\n            selected_project_id = project_ids[0] if project_ids else None\\n            project_name = PROJECT_ID_TO_NAME.get(selected_project_id, DEFAULT_PROJECT_NAME)\\n\\n            # 3. Retrieve context using RAG\\n            context = await self.rag_service.retrieve_context(\\n                query=message,\\n                tenant_id=tenant_id,\\n                project_ids=project_ids,\\n                limit=5\\n            )\\n\\n            # 4. Generate initial response (triggers LLM to decide on text vs function call)\\n            response_data = await self.rag_service.generate_response(\\n                query=message,\\n                context=context,\\n                history=history,\\n                project_context=project_context\\n            )\\n            \\n            response_text = response_data[\\\"response\\\"]\\n            function_call = response_data.get(\\\"function_call\\\")\\n\\n            # --- 5. Function Call Handling ---\\n            if function_call and function_call.get(\\\"name\\\") == \\\"generate_project_artifact\\\":\\n                \\n                args = function_call.get(\\\"args\\\", {})\\n                artifact_type = args.get(\\\"artifact_type\\\")\\n                \\n                logger.info(\\\"Handling function call to generate artifact.\\\", type=artifact_type)\\n                \\n                # Internal API call to the new artifact route\\n                artifact_api_payload = {\\n                    \\\"artifact_type\\\": artifact_type,\\n                    \\\"project_name\\\": project_name,\\n                    \\\"session_id\\\": session_id,\\n                    # Pass the LLM's generated summary and mock data to fill the artifact\\n                    \\\"data\\\": {\\\"summary\\\": args.get(\\\"summary_content\\\")}\\n                }\\n                \\n                try:\\n                    # Calls the local API endpoint which executes file generation and WS messaging\\n                    api_response = await self.http_client.post(\\\"/api/v1/artifacts/generate\\\", json=artifact_api_payload)\\n                    api_response.raise_for_status()\\n                    \\n                    # LLM's original text response is the final text message to the user\\n                    final_response_text = response_text\\n                    \\n                except httpx.HTTPStatusError as e:\\n                    logger.error(\\\"Internal artifact generation failed via API.\\\", error=str(e), status=e.response.status_code)\\n                    final_response_text = f\\\"I failed to generate the requested artifact ({artifact_type.replace('_', ' ')}) due to an internal system error.\\\"\\n                \\n                except Exception as e:\\n                    logger.error(\\\"Unexpected error during artifact generation call.\\\", error=str(e))\\n                    final_response_text = \\\"I encountered an error while trying to generate the artifact.\\\"\\n\\n            else:\\n                # Normal chat response\\n                final_response_text = response_text\\n            \\n            # 6. Add assistant response to history\\n            await self.history_service.add_message(session_id, \\\"assistant\\\", final_response_text)\\n            \\n            # 7. Return the final structured response (without artifact data, which is sent via separate WS message)\\n            return {\\n                \\\"type\\\": \\\"response\\\",\\n                \\\"session_id\\\": session_id,\\n                \\\"response\\\": final_response_text,\\n                \\\"sources\\\": response_data[\\\"sources\\\"],\\n                \\\"intent\\\": await self.llm_service.classify_query_intent(message),\\n                \\\"context_used\\\": response_data[\\\"context_used\\\"],\\n                \\\"timestamp\\\": asyncio.get_event_loop().time()\\n            }\\n\\n        except Exception as e:\\n            error_message = f\\\"I encountered an error processing your request: {type(e).__name__}.\\\"\\n            await self.history_service.add_message(session_id, \\\"assistant\\\", error_message)\\n            \\n            logger.error(\\\"Failed to process message in orchestrator\\\", error=str(e))\\n            return {\\n                \\\"type\\\": \\\"error\\\",\\n                \\\"session_id\\\": session_id,\\n                \\\"error\\\": error_message,\\n                \\\"timestamp\\\": asyncio.get_event_loop().time()\\n            }\"\n    },\n    {\n      \"path\": \"backend/main.py\",\n      \"content\": \"#!/usr/bin/env python3\\nimport logging\\nfrom contextlib import asynccontextmanager\\nfrom typing import Dict, Any\\n\\nimport structlog\\nfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, Depends\\nfrom fastapi.middleware.cors import CORSMiddleware\\nimport redis.asyncio as redis\\nfrom sqlalchemy import text\\nfrom redis.asyncio import Redis \\n\\n# CRITICAL FIX: Changed absolute imports to relative imports\\nfrom .config import get_settings\\n# Import the new function to get the engine\\nfrom .database import init_db, get_db_session, get_initialized_engine \\nfrom .services.auth_service import AuthService\\nfrom .services.agent_orchestrator import AgentOrchestrator\\nfrom .services.history_service import HistoryService # NEW\\nfrom .services.artifact_service import ArtifactService # NEW\\n# NEW: artifacts route import\\nfrom .api.routes import auth, chat, projects, microsite, admin, artifacts \\nfrom .celery_app import celery_app \\n\\n# Configure logging\\nstructlog.configure(\\n    processors=[\\n        structlog.stdlib.filter_by_level,\\n        structlog.stdlib.add_logger_name,\\n        structlog.stdlib.add_log_level,\\n        structlog.processors.TimeStamper(fmt=\\\"iso\\\"),\\n        structlog.processors.JSONRenderer(),\\n    ],\\n    logger_factory=structlog.stdlib.LoggerFactory(),\\n    cache_logger_on_first_use=True,\\n)\\n\\nlogger = structlog.get_logger()\\nsettings = get_settings()\\n\\n# WebSocket connection manager\\nclass ConnectionManager:\\n    def __init__(self, history_service: HistoryService):\\n        self.active_connections: Dict[str, WebSocket] = {}\\n        self.user_sessions: Dict[str, Dict[str, Any]] = {}\\n        self.history_service = history_service\\n\\n    async def connect(self, websocket: WebSocket, session_id: str, user_id: str):\\n        await websocket.accept()\\n        self.active_connections[session_id] = websocket\\n        self.user_sessions[session_id] = {\\\"user_id\\\": user_id}\\n\\n    def disconnect(self, session_id: str):\\n        self.active_connections.pop(session_id, None)\\n        self.user_sessions.pop(session_id, None)\\n\\n    async def send_message(self, session_id: str, message: dict):\\n        if session_id in self.active_connections:\\n            websocket = self.active_connections[session_id]\\n            await websocket.send_json(message)\\n\\nmanager: ConnectionManager \\n\\n# Dependency Injection Helpers (NEW)\\ndef get_history_service() -> HistoryService:\\n    return app.state.history_service\\n\\ndef get_artifact_service() -> ArtifactService:\\n    return app.state.artifact_service\\n\\ndef get_agent_orchestrator() -> AgentOrchestrator:\\n    return app.state.agent_orchestrator\\n# ------------------------------------\\n\\n\\n@asynccontextmanager\\nasync def lifespan(app: FastAPI):\\n    # Startup\\n    logger.info(\\\"Starting Analytics RAG Platform\\\")\\n    await init_db()\\n    \\n    app.state.db_engine = get_initialized_engine()\\n    app.state.redis = redis.from_url(settings.redis_url, decode_responses=True)\\n    \\n    # NEW: Initialize new services\\n    app.state.history_service = HistoryService(app.state.redis)\\n    app.state.artifact_service = ArtifactService()\\n    \\n    # Initialize connection manager and agent orchestrator with dependencies\\n    global manager\\n    manager = ConnectionManager(app.state.history_service)\\n    app.state.agent_orchestrator = AgentOrchestrator(app.state.history_service)\\n    \\n    app.state.auth_service = AuthService()\\n    logger.info(\\\"Application startup complete\\\")\\n\\n    yield\\n\\n    # Shutdown\\n    logger.info(\\\"Shutting down\\\")\\n    await app.state.db_engine.dispose()\\n    await app.state.redis.close()\\n\\n\\n# Create FastAPI application\\napp = FastAPI(\\n    title=\\\"Analytics RAG Platform\\\",\\n    description=\\\"Enterprise RAG system for analytics companies\\\",\\n    version=\\\"1.0.0\\\",\\n    lifespan=lifespan\\n)\\n\\n# Add CORS middleware\\napp.add_middleware(\\n    CORSMiddleware,\\n    allow_origins=[\\\"http://localhost:3000\\\", \\\"http://localhost:5173\\\"],\\n    allow_credentials=True,\\n    allow_methods=[\\\"*\\\"],\\n    allow_headers=[\\\"*\\\"],\\n)\\n\\n# Include API routes\\napp.include_router(auth.router, prefix=\\\"/api/v1/auth\\\", tags=[\\\"Authentication\\\"])\\napp.include_router(chat.router, prefix=\\\"/api/v1/chat\\\", tags=[\\\"Chat\\\"])\\napp.include_router(projects.router, prefix=\\\"/api/v1/projects\\\", tags=[\\\"Projects\\\"])\\napp.include_router(microsite.router, prefix=\\\"/api/v1/microsite\\\", tags=[\\\"Microsite\\\"])\\napp.include_router(admin.router, prefix=\\\"/api/v1/admin\\\", tags=[\\\"Administration\\\"])\\napp.include_router(artifacts.router, prefix=\\\"/api/v1/artifacts\\\", tags=[\\\"Artifact Generation\\\"]) # NEW\\n\\n\\n@app.get(\\\"/\\\")\\nasync def root():\\n    return {\\n        \\\"status\\\": \\\"healthy\\\",\\n        \\\"service\\\": \\\"Analytics RAG Platform\\\",\\n        \\\"version\\\": \\\"1.0.0\\\"\\n    }\\n\\n\\n@app.get(\\\"/health\\\")\\nasync def health_check():\\n    try:\\n        async with app.state.db_engine.begin() as conn:\\n            await conn.execute(text(\\\"SELECT 1\\\"))\\n            \\n        await app.state.redis.ping()\\n\\n        return {\\n            \\\"status\\\": \\\"healthy\\\",\\n            \\\"database\\\": \\\"connected\\\",\\n            \\\"redis\\\": \\\"connected\\\"\\n        }\\n    except Exception as e:\\n        logger.error(\\\"Health check failed\\\", error=str(e))\\n        raise HTTPException(status_code=503, detail=\\\"Service unavailable\\\")\\n\\n\\n@app.websocket(\\\"/ws/{session_id}\\\")\\nasync def websocket_endpoint(websocket: WebSocket, session_id: str):\\n    user_id = \\\"1\\\"\\n\\n    await manager.connect(websocket, session_id, user_id)\\n\\n    try:\\n        while True:\\n            data = await websocket.receive_json()\\n\\n            # Process message through agent orchestrator\\n            orchestrator = get_agent_orchestrator() \\n            response = await orchestrator.process_message(\\n                user_id=user_id,\\n                session_id=session_id,\\n                message=data[\\\"message\\\"],\\n                project_context=data.get(\\\"project_context\\\", {}),\\n                message_type=data.get(\\\"type\\\", \\\"chat\\\")\\n            )\\n\\n            # Only send the standard 'response'/'error' message back here\\n            # 'artifact_generated' messages are sent directly from the artifacts endpoint via manager\\n            if response.get(\\\"type\\\") in [\\\"response\\\", \\\"error\\\"]:\\n                await manager.send_message(session_id, response)\\n\\n    except WebSocketDisconnect:\\n        manager.disconnect(session_id)\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    import uvicorn\\n    uvicorn.run(\\\"main:app\\\", host=\\\"0.0.0.0\\\", port=8000, reload=True)\"\n    },\n    {\n      \"path\": \"frontend/src/types/index.ts\",\n      \"content\": \"export interface User {\\n  id: string // MODIFIED: In the demo, this now maps to the DB ID (e.g., '1')\\n  username: string\\n  name: string\\n  email: string\\n  tenant_id: string\\n}\\n\\nexport interface Project {\\n  id: string\\n  name: string\\n  description: string\\n  status: string\\n  folder_id: string\\n  created_at: string\\n  updated_at: string\\n}\\n\\n// NEW: Artifact Metadata Interface\\nexport interface ArtifactData {\\n  id: string\\n  filename: string\\n  mime_type: string\\n  download_url: string\\n}\\n\\nexport interface Message {\\n  id: string\\n  // Added new types for conversational response and artifact message\\n  type: 'user' | 'assistant' | 'artifact_generated' | 'error' \\n  content: string // Used for 'user' and 'assistant' text responses\\n  timestamp: string\\n  sources?: Source[]\\n  microsite?: MicrositeData\\n  artifact?: ArtifactData // NEW: Field for structured artifact files\\n}\\n\\nexport interface Source {\\n  file: string\\n  project: string\\n  type: string\\n  score?: number\\n}\\n\\nexport interface MicrositeData {\\n  title: string\\n  url: string\\n  data: any\\n}\"\n    },\n    {\n      \"path\": \"frontend/src/components/Chat/ChatInterface.tsx\",\n      \"content\": \"import React, { useState, useEffect, useRef } from 'react'\\nimport { Send, Loader2 } from 'lucide-react'\\nimport MessageBubble from './MessageBubble'\\nimport MicrositePreview from './MicrositePreview'\\nimport { Message, User } from '../../types'\\n\\ninterface ChatInterfaceProps {\\n  selectedProject: string // CRITICAL: This MUST be the numeric ID (e.g., \\\"6\\\"), not the name (\\\"Diatomite\\\")\\n  user: User | null\\n}\\n\\nexport default function ChatInterface({ selectedProject, user }: ChatInterfaceProps) {\\n  const [messages, setMessages] = useState<Message[]>([])\\n  const [inputMessage, setInputMessage] = useState('')\\n  const [isLoading, setIsLoading] = useState(false)\\n  const [websocket, setWebsocket] = useState<WebSocket | null>(null)\\n  const messagesEndRef = useRef<HTMLDivElement>(null)\\n\\n  const scrollToBottom = () => {\\n    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' })\\n  }\\n\\n  useEffect(() => {\\n    scrollToBottom()\\n  }, [messages])\\n\\n  useEffect(() => {\\n    // Initialize WebSocket connection\\n    const wsUrl = import.meta.env.VITE_WS_URL || 'ws://localhost:8000'\\n    const sessionId = `session_${Date.now()}`\\n    const ws = new WebSocket(`${wsUrl}/ws/${sessionId}`)\\n\\n    ws.onopen = () => {\\n      console.log('WebSocket connected')\\n      setWebsocket(ws)\\n      // Send initial welcome/system message to user\\n      const initialMessage: Message = {\\n        id: `sys_msg_${Date.now()}`,\\n        type: 'assistant',\\n        content: `Hello ${user?.name || 'User'}! I'm your PMO Assistant. Ask me about your projects or request an artifact (e.g., \\\"Generate an Excel risk log\\\").`,\\n        timestamp: new Date().toISOString()\\n      }\\n      setMessages(prev => [...prev, initialMessage])\\n    }\\n\\n    ws.onmessage = (event) => {\\n      const data = JSON.parse(event.data)\\n\\n      if (data.type === 'response') {\\n        // Standard chat response (could be the result of a normal query or a function call confirmation)\\n        const assistantMessage: Message = {\\n          id: `msg_${Date.now()}_resp`,\\n          type: 'assistant',\\n          content: data.response,\\n          timestamp: new Date().toISOString(),\\n          sources: data.sources,\\n          microsite: data.microsite\\n        }\\n\\n        setMessages(prev => [...prev, assistantMessage])\\n        setIsLoading(false)\\n      } else if (data.type === 'artifact_generated') {\\n        // NEW: Artifact message sent directly from the artifacts endpoint (via manager.send_message)\\n        const artifactMessage: Message = {\\n          id: `msg_${Date.now()}_art`,\\n          type: 'artifact_generated',\\n          content: `Your requested artifact, \\\"${data.artifact.filename}\\\", is ready! Click the link below to download.`,\\n          timestamp: new Date().toISOString(),\\n          artifact: data.artifact\\n        }\\n        \\n        setMessages(prev => [...prev, artifactMessage])\\n\\n      } else if (data.type === 'error') {\\n        const errorMessage: Message = {\\n          id: `msg_${Date.now()}_err`,\\n          type: 'assistant',\\n          content: data.error,\\n          timestamp: new Date().toISOString()\\n        }\\n\\n        setMessages(prev => [...prev, errorMessage])\\n        setIsLoading(false)\\n      }\\n    }\\n\\n    ws.onclose = () => {\\n      console.log('WebSocket disconnected')\\n    }\\n\\n    ws.onerror = (error) => {\\n      console.error('WebSocket error:', error)\\n      setIsLoading(false)\\n    }\\n\\n    return () => {\\n      ws.close()\\n    }\\n  }, [user]) // Re-run effect if user changes (auth state)\\n\\n  const sendMessage = async () => {\\n    if (!inputMessage.trim() || !websocket || isLoading) return\\n\\n    const userMessage: Message = {\\n      id: `msg_${Date.now()}_user`,\\n      type: 'user',\\n      content: inputMessage,\\n      timestamp: new Date().toISOString()\\n    }\\n\\n    setMessages(prev => [...prev, userMessage])\\n    setIsLoading(true)\\n    \\n    // CRITICAL FIX: Ensure the project ID is passed as a string and ONLY if it exists.\\n    const projectIds = selectedProject ? [String(selectedProject)] : [];\\n\\n    // Send message via WebSocket\\n    websocket.send(JSON.stringify({\\n      message: inputMessage,\\n      project_context: {\\n        tenant_id: user?.tenant_id || 'demo',\\n        project_ids: projectIds, // Using the ensured array of string IDs\\n        selected_project: selectedProject\\n      },\\n      type: 'chat'\\n    }))\\n\\n    setInputMessage('')\\n  }\\n\\n  const handleKeyPress = (e: React.KeyboardEvent) => {\\n    if (e.key === 'Enter' && !e.shiftKey) {\\n      e.preventDefault()\\n      sendMessage()\\n    }\\n  }\\n\\n  return (\\n    <div className=\\\"flex flex-col h-[600px]\\\">\\n      {/* Messages Area */}\\n      <div className=\\\"flex-1 overflow-y-auto p-4 space-y-4\\\">\\n        {messages.length === 0 && (\\n          <div className=\\\"text-center text-gray-500 mt-8\\\">\\n            <p className=\\\"text-lg font-medium\\\">Welcome to Analytics RAG Platform</p>\\n            <p className=\\\"text-sm mt-2\\\">\\n              Ask questions about your projects, KPIs, or request dashboard generation\\n            </p>\\n            <div className=\\\"mt-4 text-sm text-gray-400\\\">\\n              Example queries:\\n              <ul className=\\\"mt-2 space-y-1\\\">\\n                <li>\u2022 \\\"Generate an Excel risk log for the Stone Hill project\\\"</li>\\n                <li>\u2022 \\\"Draft a Word status report based on the latest documents\\\"</li>\\n                <li>\u2022 \\\"What is the predicted spud success rate?\\\"</li>\\n              </ul>\\n            </div>\\n          </div>\\n        )}\\n\\n        {messages.map((message) => (\\n          <div key={message.id}>\\n            <MessageBubble message={message} />\\n            {/* Microsite is rendered below the bubble if present */}\\n            {message.microsite && (\\n              <MicrositePreview micrositeData={message.microsite} />\\n            )}\\n          </div>\\n        ))}\\n\\n        {isLoading && (\\n          <div className=\\\"flex items-center space-x-2 text-gray-500\\\">\\n            <Loader2 className=\\\"h-4 w-4 animate-spin\\\" />\\n            <span>Processing your request...</span>\\n          </div>\\n        )}\\n\\n        <div ref={messagesEndRef} />\\n      </div>\\n\\n      {/* Input Area */}\\n      <div className=\\\"border-t border-gray-200 p-4\\\">\\n        <div className=\\\"flex space-x-2\\\">\\n          <textarea\\n            value={inputMessage}\\n            onChange={(e) => setInputMessage(e.target.value)}\\n            onKeyPress={handleKeyPress}\\n            placeholder={selectedProject \\n              ? `Ask about ${selectedProject} or request insights (e.g., 'Generate risk log')...`\\n              : \\\"Ask about your projects or request insights...\\\"\\n            }\\n            className=\\\"flex-1 resize-none border border-gray-300 rounded-lg px-3 py-2 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent\\\"\\n            rows={2}\\n            disabled={isLoading}\\n          />\\n          <button\\n            onClick={sendMessage}\\n            disabled={!inputMessage.trim() || isLoading}\\n            className=\\\"bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2 disabled:opacity-50 disabled:cursor-not-allowed flex items-center\\\"\\n          >\\n            <Send className=\\\"h-4 w-4\\\" />\\n          </button>\\n        </div>\\n\\n        {selectedProject && (\\n          <div className=\\\"mt-2 text-xs text-gray-500\\\">\\n            Active project: <span className=\\\"font-medium\\\">{selectedProject}</span>\\n          </div>\\n        )}\\n      </div>\\n    </div>\\n  )\\n}\"\n    },\n    {\n      \"path\": \"frontend/src/components/Chat/MessageBubble.tsx\",\n      \"content\": \"import React from 'react'\\nimport { User, Bot, ExternalLink, Download, FileText, FileSpreadsheet, Presentation } from 'lucide-react'\\nimport { Message } from '../../types'\\n\\ninterface MessageBubbleProps {\\n  message: Message\\n}\\n\\nexport default function MessageBubble({ message }: MessageBubbleProps) {\\n  const isUser = message.type === 'user'\\n  const isArtifact = message.type === 'artifact_generated'\\n  \\n  const getFileIcon = (mimeType: string) => {\\n    if (mimeType.includes('spreadsheet')) return <FileSpreadsheet className=\\\"h-4 w-4 text-green-600\\\" />\\n    if (mimeType.includes('wordprocessing')) return <FileText className=\\\"h-4 w-4 text-blue-600\\\" />\\n    if (mimeType.includes('presentation')) return <Presentation className=\\\"h-4 w-4 text-orange-600\\\" />\\n    return <Download className=\\\"h-4 w-4\\\" />\\n  }\\n\\n  return (\\n    <div className={`chat-message ${isUser ? 'user-message' : isArtifact ? 'assistant-message' : 'assistant-message'}`}>\\n      <div className=\\\"flex items-start space-x-3\\\">\\n        <div className={`flex-shrink-0 w-8 h-8 rounded-full flex items-center justify-center ${\\n          isUser ? 'bg-blue-600 text-white' : 'bg-gray-600 text-white'\\n        }`}>\\n          {isUser ? <User className=\\\"h-4 w-4\\\" /> : <Bot className=\\\"h-4 w-4\\\" />}\\n        </div>\\n\\n        <div className=\\\"flex-1\\\">\\n          <div className=\\\"flex items-center space-x-2 mb-1\\\">\\n            <span className=\\\"text-sm font-medium text-gray-900\\\">\\n              {isUser ? 'You' : 'Assistant'}\\n            </span>\\n            <span className=\\\"text-xs text-gray-500\\\">\\n              {new Date(message.timestamp).toLocaleTimeString()}\\n            </span>\\n          </div>\\n\\n          {/* Render content text for standard messages */}\\n          {message.content && (\\n            <div className=\\\"text-gray-800 whitespace-pre-wrap\\\">\\n              {message.content}\\n            </div>\\n          )}\\n\\n          {/* NEW: Render Artifact Download Link */}\\n          {isArtifact && message.artifact && (\\n            <a \\n              // The download URL is a relative API path which the browser will resolve correctly\\n              href={message.artifact.download_url} \\n              target=\\\"_blank\\\" \\n              download \\n              className=\\\"mt-3 inline-flex items-center space-x-2 text-sm font-medium text-purple-600 hover:text-purple-800 transition-colors bg-purple-50 p-3 rounded-lg border border-purple-200 shadow-sm\\\"\\n              rel=\\\"noopener noreferrer\\\"\\n            >\\n              {getFileIcon(message.artifact.mime_type)}\\n              <span>Download: {message.artifact.filename}</span>\\n            </a>\\n          )}\\n\\n          {/* Render Sources */}\\n          {message.sources && message.sources.length > 0 && (\\n            <div className=\\\"mt-3 pt-3 border-t border-gray-200\\\">\\n              <div className=\\\"text-xs font-medium text-gray-700 mb-2\\\">Sources:</div>\\n              <div className=\\\"space-y-1\\\">\\n                {message.sources.map((source, index) => (\\n                  <div\\n                    key={index}\\n                    className=\\\"flex items-center space-x-2 text-xs text-gray-600\\\"\\n                  >\\n                    <ExternalLink className=\\\"h-3 w-3\\\" />\\n                    <span className=\\\"font-medium\\\">{source.file}</span>\\n                    <span className=\\\"text-gray-400\\\">\\u2022</span>\\n                    <span>{source.project}</span>\\n                    {source.score && (\\n                      <>\\n                        <span className=\\\"text-gray-400\\\">\\u2022</span>\\n                        <span>Score: {(source.score * 100).toFixed(0)}%</span>\\n                      </>\\n                    )}\\n                  </div>\\n                ))}\\n              </div>\\n            </div>\\n          )}\\n        </div>\\n      </div>\\n    </div>\\n  )\\n}\"\n    }\n  ]\n}\n\n# Load the JSON content to ensure it's processed as a Python dictionary\n# This handles the internal escaped characters correctly.\nwith open('update.json', 'r') as f: data = json.load(f)\n# data = RAG_SERVICE_UPDATE_JSON_CONTENT\n\n# --- Execution Function ---\ndef write_updates_from_json(data: Dict[str, Any]):\n    \"\"\"\n    Reads the file content from the provided data dictionary and writes it to \n    the specified paths, relying on the Python JSON parser to handle string escapes.\n    \"\"\"\n    print(\"\u2728 Starting file update process based on JSON content... \u2728\\n\")\n    \n    for file_data in data.get(\"files\", []):\n        file_path = file_data.get(\"path\")\n        content = file_data.get(\"content\")\n        \n        if not file_path or content is None:\n            print(\"Skipping file entry with missing path or content.\")\n            continue\n            \n        # 1. Ensure the directory exists (e.g., create backend/services)\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        \n        try:\n            # 2. Write the content to the file path.\n            # The 'content' variable is a standard Python string after JSON parsing, \n            # so it contains the correct newline (\\n) and quote characters.\n            with open(file_path, 'w', encoding='utf-8') as f:\n                f.write(content)\n            \n            # Print success message for confirmation\n            print(f\"\u2705 Successfully wrote and updated: {file_path}\")\n            \n        except IOError as e:\n            print(f\"\u274c Failed to write file {file_path}: {e}\")\n\n    print(\"\\n\\nAll files have been updated using the content directly read from the JSON structure.\")\n    print(\"The code now reflects the complete artifact generation and WebSocket feature.\")\n\n\n# Run the function with the provided JSON content\nwrite_updates_from_json(data)\n"
    },
    {
      "path": "backend/celery_app.py",
      "filename": "celery_app.py",
      "content": "#!/usr/bin/env python3\nfrom celery import Celery\n# CORRECTED: Use relative import for config\nfrom .config import get_settings \n\n# Load settings to get the Redis URL\nsettings = get_settings()\n\n# Initialize the Celery application\n# This is placed here to avoid circular dependencies with main.py\ncelery_app = Celery(\n    \"analytics_rag\",\n    broker=settings.redis_url,\n    backend=settings.redis_url,\n    # CRITICAL FIX: Changed task module path to absolute path relative to the top-level package.\n    # Celery will now look for 'backend.tasks.document_tasks'.\n    include=['backend.tasks.document_tasks'] \n)\n\n# Optional: Configuration for Celery (adjust as needed)\ncelery_app.conf.update(\n    task_track_started=True,\n    result_expires=3600,\n    timezone='UTC'\n)\n"
    },
    {
      "path": "backend/config.py",
      "filename": "config.py",
      "content": "#!/usr/bin/env python3\nimport os\nimport base64\nimport json\nfrom functools import lru_cache\nfrom typing import Dict, Any\nfrom pydantic_settings import BaseSettings\nfrom pydantic import Field\nfrom dotenv import load_dotenv\nimport sys # Added sys import for path checks if necessary\n\n# --- CRITICAL FIX: EXPLICIT ABSOLUTE PATH LOADING ---\n# Paste the absolute path to your project root here, followed by '/.env'\n# Example: /home/ameyaumesh/rag-folder/code_repo_clean_exploded/.env\nABSOLUTE_ENV_PATH = \"/home/ameyaumesh/rag-folder/code_repo_clean_exploded/backend/.env\" \n\nif not os.path.exists(ABSOLUTE_ENV_PATH):\n    print(f\"FATAL ERROR: .env file not found at: {ABSOLUTE_ENV_PATH}\")\n    sys.exit(1) # Exit if the .env file is not found\n\nload_dotenv(ABSOLUTE_ENV_PATH)\n\nclass Settings(BaseSettings):\n    # Environment\n    environment: str = Field(default=\"development\", env=\"ENVIRONMENT\")\n    log_level: str = Field(default=\"INFO\", env=\"LOG_LEVEL\")\n\n    # API Keys\n    gemini_api_key: str = Field(..., env=\"GEMINI_API_KEY\")\n    \n    # --- NEW: Google OAuth Settings for User-Specific Drive Access ---\n    GOOGLE_OAUTH_CLIENT_ID: str = Field(..., env=\"GOOGLE_OAUTH_CLIENT_ID\")\n    GOOGLE_OAUTH_CLIENT_SECRET: str = Field(..., env=\"GOOGLE_OAUTH_CLIENT_SECRET\")\n    # Drive Readonly Scope is sufficient for RAG synchronization\n    GOOGLE_OAUTH_SCOPES: str = Field(default=\"https://www.googleapis.com/auth/drive.readonly\", env=\"GOOGLE_OAUTH_SCOPES\")\n    # This MUST match the URL registered in Google Cloud Console\n    GOOGLE_OAUTH_REDIRECT_URI: str = Field(default=\"http://localhost:8000/api/v1/auth/google/callback\", env=\"GOOGLE_OAUTH_REDIRECT_URI\")\n\n    # Database URLs\n    database_url: str = Field(..., env=\"DATABASE_URL\")\n    redis_url: str = Field(..., env=\"REDIS_URL\")\n    qdrant_url: str = Field(..., env=\"QDRANT_URL\")\n    neo4j_url: str = Field(..., env=\"NEO4J_URL\")\n    neo4j_user: str = Field(default=\"neo4j\", env=\"NEO4J_USER\")\n    neo4j_password: str = Field(..., env=\"NEO4J_PASSWORD\")\n\n    # Security\n    jwt_secret: str = Field(..., env=\"JWT_SECRET\")\n    encryption_key: str = Field(..., env=\"ENCRYPTION_KEY\")\n\n    class Config:\n        case_sensitive = False\n\n\n@lru_cache()\ndef get_settings() -> Settings:\n    \"\"\"Returns a cached singleton instance of the settings.\"\"\"\n    return Settings()\n"
    },
    {
      "path": "backend/database.py",
      "filename": "database.py",
      "content": "#!/usr/bin/env python3\nimport asyncio\nfrom contextlib import asynccontextmanager\nfrom typing import AsyncGenerator, Optional\nfrom sqlalchemy.ext.asyncio import AsyncSession, AsyncEngine, create_async_engine, async_sessionmaker\nfrom sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column\nfrom sqlalchemy import Column, Integer, String, DateTime, Boolean, Text, JSON, MetaData\nfrom sqlalchemy.sql import func\nimport structlog\nimport re\n\nfrom .config import get_settings\n\nlogger = structlog.get_logger()\nsettings = get_settings()\n\n# Database engine and session\nengine: Optional[AsyncEngine] = None\nSessionLocal: Optional[async_sessionmaker[AsyncSession]] = None\n\nclass Base(DeclarativeBase):\n    metadata = MetaData()\n\n    id: Mapped[int] = mapped_column(Integer, primary_key=True, index=True)\n    created_at: Mapped[DateTime] = mapped_column(DateTime(timezone=True), server_default=func.now())\n    updated_at: Mapped[DateTime] = mapped_column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())\n\n\n# Minimal User Model\nclass User(Base):\n    __tablename__ = \"users\"\n\n    # Existing fields\n    username: Mapped[str] = mapped_column(String(100), unique=True, index=True, nullable=False)\n    email: Mapped[str] = mapped_column(String(255), unique=True, nullable=False)\n    hashed_password: Mapped[str] = mapped_column(String(255), nullable=False)\n    tenant_id: Mapped[int] = mapped_column(Integer, nullable=True)\n    is_active: Mapped[bool] = mapped_column(Boolean, default=True)\n    \n    # NEW: Fields for Google Drive OAuth token\n    gdrive_refresh_token: Mapped[Optional[str]] = mapped_column(Text, nullable=True)\n    gdrive_linked_at: Mapped[Optional[DateTime]] = mapped_column(DateTime(timezone=True), nullable=True)\n\n\nasync def init_db():\n    global engine, SessionLocal\n\n    # CRITICAL FIX: Ensure the URL uses the async driver (asyncpg) and the correct SSL parameter ('ssl').\n    # 1. Strip all potential driver prefixes and use the base postgresql:// prefix.\n    # 2. Add the explicit +asyncpg driver name.\n    # 3. Swap the synchronous parameter 'sslmode' (from .env) to the asynchronous parameter 'ssl'.\n    \n    url_base = re.sub(r\"postgresql(\\+\\w+)?:\", \"postgresql:\", settings.database_url)\n    \n    # Ensure the URL is clean before swapping\n    async_db_url = url_base.replace(\"postgresql://\", \"postgresql+asyncpg://\", 1)\n    async_db_url = async_db_url.replace(\"sslmode=\", \"ssl=\", 1) # Swap parameter name for asyncpg compatibility\n\n    engine = create_async_engine(\n        async_db_url,\n        echo=settings.environment == \"development\",\n        pool_size=20,\n        max_overflow=30\n    )\n\n    SessionLocal = async_sessionmaker(\n        bind=engine,\n        class_=AsyncSession,\n        expire_on_commit=False\n    )\n    \n    # Validation attempt\n    async with engine.begin() as conn:\n        await conn.execute(func.now()) \n    logger.info(\"Database engine initialized and connection validated.\")\n\ndef get_initialized_engine() -> AsyncEngine:\n    if engine is None:\n        raise RuntimeError(\"Database engine has not been initialized.\")\n    return engine\n\n\n@asynccontextmanager\nasync def get_db_session() -> AsyncGenerator[AsyncSession, None]:\n    if not SessionLocal:\n        logger.error(\"Attempted to get DB session before initialization succeeded.\")\n        raise RuntimeError(\"Database not initialized\")\n\n    async with SessionLocal() as session:\n        try:\n            yield session\n        except Exception:\n            await session.rollback()\n            raise\n        finally:\n            await session.close()\n\n\nasync def get_db() -> AsyncGenerator[AsyncSession, None]:\n    async with get_db_session() as session:\n        yield session\n"
    },
    {
      "path": "backend/Dockerfile",
      "filename": "Dockerfile",
      "content": "FROM python:3.11-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    gcc \\\n    g++ \\\n    libpq-dev \\\n    # NEW: Needed for certain google-auth-oauthlib dependencies (though often satisfied)\n    libssl-dev \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements first for better caching\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Create logs directory\nRUN mkdir -p logs\n\n# Expose port\nEXPOSE 8000\n\n# Command to run the application\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--reload\"]\n"
    },
    {
      "path": "backend/main.py",
      "filename": "main.py",
      "content": "#!/usr/bin/env python3\nimport logging\nfrom contextlib import asynccontextmanager\nfrom typing import Dict, Any\n\nimport structlog\nfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport redis.asyncio as redis\nfrom sqlalchemy import text\nfrom redis.asyncio import Redis \n\n# CRITICAL FIX: Changed absolute imports to relative imports\nfrom .config import get_settings\n# Import the new function to get the engine\nfrom .database import init_db, get_db_session, get_initialized_engine \nfrom .services.auth_service import AuthService\nfrom .services.agent_orchestrator import AgentOrchestrator\nfrom .services.history_service import HistoryService # NEW\nfrom .services.artifact_service import ArtifactService # NEW\n# NEW: artifacts route import\nfrom .api.routes import auth, chat, projects, microsite, admin, artifacts \nfrom .celery_app import celery_app \n\n# Configure logging\nstructlog.configure(\n    processors=[\n        structlog.stdlib.filter_by_level,\n        structlog.stdlib.add_logger_name,\n        structlog.stdlib.add_log_level,\n        structlog.processors.TimeStamper(fmt=\"iso\"),\n        structlog.processors.JSONRenderer(),\n    ],\n    logger_factory=structlog.stdlib.LoggerFactory(),\n    cache_logger_on_first_use=True,\n)\n\nlogger = structlog.get_logger()\nsettings = get_settings()\n\n# WebSocket connection manager\nclass ConnectionManager:\n    def __init__(self, history_service: HistoryService):\n        self.active_connections: Dict[str, WebSocket] = {}\n        self.user_sessions: Dict[str, Dict[str, Any]] = {}\n        self.history_service = history_service\n\n    async def connect(self, websocket: WebSocket, session_id: str, user_id: str):\n        await websocket.accept()\n        self.active_connections[session_id] = websocket\n        self.user_sessions[session_id] = {\"user_id\": user_id}\n\n    def disconnect(self, session_id: str):\n        self.active_connections.pop(session_id, None)\n        self.user_sessions.pop(session_id, None)\n\n    async def send_message(self, session_id: str, message: dict):\n        if session_id in self.active_connections:\n            websocket = self.active_connections[session_id]\n            await websocket.send_json(message)\n\nmanager: ConnectionManager \n\n# Dependency Injection Helpers (Needed for local function dependency injection)\ndef get_artifact_service() -> ArtifactService:\n    # Access artifact service from app state\n    return app.state.artifact_service\n\ndef get_agent_orchestrator() -> AgentOrchestrator:\n    # Access agent orchestrator from app state\n    return app.state.agent_orchestrator\n\ndef get_connection_manager() -> ConnectionManager:\n    # Access the manager instance from app state\n    return app.state.manager\n# ------------------------------------\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup\n    logger.info(\"Starting Analytics RAG Platform\")\n    await init_db()\n    \n    app.state.db_engine = get_initialized_engine()\n    app.state.redis = redis.from_url(settings.redis_url, decode_responses=True)\n    \n    # NEW: Initialize services\n    app.state.history_service = HistoryService(app.state.redis)\n    app.state.artifact_service = ArtifactService()\n    \n    # Initialize connection manager and agent orchestrator with dependencies\n    global manager\n    manager = ConnectionManager(app.state.history_service)\n    app.state.manager = manager # CRITICAL FIX: Attach manager to app state\n    app.state.agent_orchestrator = AgentOrchestrator(app.state.history_service)\n    \n    app.state.auth_service = AuthService()\n    logger.info(\"Application startup complete\")\n\n    yield\n\n    # Shutdown\n    logger.info(\"Shutting down\")\n    await app.state.db_engine.dispose()\n    await app.state.redis.close()\n\n\n# Create FastAPI application\napp = FastAPI(\n    title=\"Analytics RAG Platform\",\n    description=\"Enterprise RAG system for analytics companies\",\n    version=\"1.0.0\",\n    lifespan=lifespan\n)\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"http://localhost:3000\", \"http://localhost:5173\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Include API routes\napp.include_router(auth.router, prefix=\"/api/v1/auth\", tags=[\"Authentication\"])\napp.include_router(chat.router, prefix=\"/api/v1/chat\", tags=[\"Chat\"])\napp.include_router(projects.router, prefix=\"/api/v1/projects\", tags=[\"Projects\"])\napp.include_router(microsite.router, prefix=\"/api/v1/microsite\", tags=[\"Microsite\"])\napp.include_router(admin.router, prefix=\"/api/v1/admin\", tags=[\"Administration\"])\napp.include_router(artifacts.router, prefix=\"/api/v1/artifacts\", tags=[\"Artifact Generation\"])\n\n\n@app.get(\"/\")\nasync def root():\n    return {\n        \"status\": \"healthy\",\n        \"service\": \"Analytics RAG Platform\",\n        \"version\": \"1.0.0\"\n    }\n\n\n@app.get(\"/health\")\nasync def health_check():\n    try:\n        async with app.state.db_engine.begin() as conn:\n            await conn.execute(text(\"SELECT 1\"))\n            \n        await app.state.redis.ping()\n\n        return {\n            \"status\": \"healthy\",\n            \"database\": \"connected\",\n            \"redis\": \"connected\"\n        }\n    except Exception as e:\n        logger.error(\"Health check failed\", error=str(e))\n        raise HTTPException(status_code=503, detail=\"Service unavailable\")\n\n\n@app.websocket(\"/ws/{session_id}\")\nasync def websocket_endpoint(websocket: WebSocket, session_id: str):\n    user_id = \"1\"\n\n    await manager.connect(websocket, session_id, user_id)\n\n    try:\n        while True:\n            data = await websocket.receive_json()\n\n            # Process message through agent orchestrator\n            orchestrator = get_agent_orchestrator() \n            response = await orchestrator.process_message(\n                user_id=user_id,\n                session_id=session_id,\n                message=data[\"message\"],\n                project_context=data.get(\"project_context\", {}),\n                message_type=data.get(\"type\", \"chat\")\n            )\n\n            # Only send the standard 'response'/'error' message back here\n            # 'artifact_generated' messages are sent directly from the artifacts endpoint via manager\n            if response.get(\"type\") in [\"response\", \"error\"]:\n                await manager.send_message(session_id, response)\n\n    except WebSocketDisconnect:\n        manager.disconnect(session_id)\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n"
    },
    {
      "path": "backend/__init__.py",
      "filename": "__init__.py",
      "content": "# Backend package\n"
    },
    {
      "path": "backend/agents/__init__.py",
      "filename": "__init__.py",
      "content": "# Python package\n"
    },
    {
      "path": "backend/api/__init__.py",
      "filename": "__init__.py",
      "content": "# Python package\n"
    },
    {
      "path": "backend/api/routes/admin.py",
      "filename": "admin.py",
      "content": "#!/usr/bin/env python3\nfrom fastapi import APIRouter\nfrom typing import Dict, Any\n\nrouter = APIRouter()\n\n\n@router.get(\"/health\")\nasync def admin_health():\n    \"\"\"Admin health check\"\"\"\n    return {\n        \"status\": \"healthy\",\n        \"services\": {\n            \"database\": \"connected\",\n            \"redis\": \"connected\", \n            \"qdrant\": \"connected\",\n            \"neo4j\": \"connected\"\n        }\n    }\n\n\n@router.get(\"/stats\")\nasync def get_system_stats():\n    \"\"\"Get system statistics\"\"\"\n    return {\n        \"total_projects\": 5,\n        \"total_documents\": 152,\n        \"total_users\": 12,\n        \"active_sessions\": 3,\n        \"storage_used\": \"2.1 GB\",\n        \"last_sync\": \"2025-10-02T07:30:00Z\"\n    }\n"
    },
    {
      "path": "backend/api/routes/artifacts.py",
      "filename": "artifacts.py",
      "content": "#!/usr/bin/env python3\nfrom fastapi import APIRouter, HTTPException, status, Depends, Request\nfrom fastapi.responses import Response, JSONResponse\nfrom typing import Dict, Any, Union\nimport structlog\nimport uuid\n\n# Use multi-level parent relative imports\nfrom ...services.artifact_service import ArtifactService\n# CRITICAL FIX: DO NOT import from main here. We will access services via Request.app.state\n\nrouter = APIRouter()\nlogger = structlog.get_logger()\n\n# --- Mock Data Store (In a real system, this would be S3 or GCS) ---\n# Stores temporary artifact bytes with a UUID key and metadata\nARTIFACT_CACHE: Dict[str, Dict[str, Union[bytes, str]]] = {} \n# -------------------------------------------------------------------\n\ndef add_artifact_to_cache(file_bytes: bytes, filename: str, mime_type: str) -> Dict[str, str]:\n    \"\"\"Stores the generated file in a temporary cache and returns metadata.\"\"\"\n    file_id = str(uuid.uuid4())\n    ARTIFACT_CACHE[file_id] = {\n        \"bytes\": file_bytes,\n        \"filename\": filename,\n        \"mime_type\": mime_type\n    }\n    \n    # Return metadata for the chat response\n    return {\n        \"id\": file_id,\n        \"filename\": filename,\n        \"mime_type\": mime_type,\n        \"size_bytes\": str(len(file_bytes))\n    }\n\n@router.post(\"/generate\")\nasync def generate_artifact_endpoint(\n    request_data: Dict[str, Union[str, Dict[str, Any]]],\n    request: Request # Inject the request object to access app state\n):\n    \"\"\"\n    Called internally by the AgentOrchestrator to create a file \n    and send a temporary download link via WebSocket.\n    \"\"\"\n    # CRITICAL FIX: Access services directly from app state and the global manager instance\n    artifact_service: ArtifactService = request.app.state.artifact_service\n    manager = request.app.state.manager # manager is now stored on app.state in main.py\n\n    artifact_type = request_data.get(\"artifact_type\")\n    project_name = request_data.get(\"project_name\")\n    session_id = request_data.get(\"session_id\")\n    data = request_data.get(\"data\", {})\n\n    if not all([artifact_type, project_name, session_id]):\n        raise HTTPException(status_code=400, detail=\"Missing artifact_type, project_name, or session_id.\")\n\n    try:\n        file_bytes = await artifact_service.generate_artifact(artifact_type, project_name, data)\n        \n        if file_bytes is None:\n            raise HTTPException(status_code=500, detail=\"Artifact generation failed internally.\")\n\n        # Determine file metadata based on type\n        mime_type = \"application/octet-stream\"\n        if artifact_type == \"excel_risk_register\":\n            filename = f\"{project_name}_Risk_Register.xlsx\"\n            mime_type = \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n        elif artifact_type == \"word_status_report\":\n            filename = f\"{project_name}_Status_Report.docx\"\n            mime_type = \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n        elif artifact_type == \"pptx_executive_pitch\":\n            filename = f\"{project_name}_Executive_Pitch.pptx\"\n            mime_type = \"application/vnd.openxmlformats-officedocument.presentationml.presentation\"\n        else:\n            filename = f\"{project_name}_Artifact.bin\"\n\n        metadata = add_artifact_to_cache(file_bytes, filename, mime_type)\n        \n        # Send a direct WebSocket message to the user about the generated artifact\n        await manager.send_message(session_id, {\n            \"type\": \"artifact_generated\",\n            \"session_id\": session_id,\n            \"artifact\": {\n                \"id\": metadata[\"id\"],\n                \"filename\": metadata[\"filename\"],\n                \"mime_type\": metadata[\"mime_type\"],\n                \"download_url\": f\"/api/v1/artifacts/download/{metadata['id']}\" # Relative path\n            }\n        })\n\n        # Return a simple confirmation to the orchestrator\n        return {\"status\": \"success\", \"artifact_id\": metadata[\"id\"]}\n\n    except Exception as e:\n        logger.error(\"Artifact generation endpoint failed.\", error=str(e))\n        raise HTTPException(status_code=500, detail=f\"Failed to generate artifact: {e}\")\n\n@router.get(\"/download/{file_id}\")\nasync def download_artifact_endpoint(file_id: str):\n    \"\"\"Retrieves and serves the temporary artifact file.\"\"\"\n    if file_id not in ARTIFACT_CACHE:\n        raise HTTPException(status_code=404, detail=\"File not found or expired.\")\n\n    # Retrieve data from cache\n    artifact_data = ARTIFACT_CACHE[file_id]\n    file_bytes = artifact_data[\"bytes\"]\n    filename = artifact_data[\"filename\"]\n    mime_type = artifact_data[\"mime_type\"]\n    \n    # Optional: Delete from cache after download to clean up mock storage\n    del ARTIFACT_CACHE[file_id] \n\n    return Response(\n        content=file_bytes,\n        media_type=mime_type,\n        headers={\n            \"Content-Disposition\": f\"attachment; filename=\\\"{filename}\\\"\",\n            \"Content-Length\": str(len(file_bytes))\n        }\n    )\n"
    },
    {
      "path": "backend/api/routes/auth.py",
      "filename": "auth.py",
      "content": "#!/usr/bin/env python3\nfrom fastapi import APIRouter, Depends, HTTPException, status, Request\nfrom fastapi.security import HTTPBearer\nfrom pydantic import BaseModel\nfrom typing import Dict, Any\nfrom starlette.responses import RedirectResponse\nimport httpx\nfrom sqlalchemy import update, text \nfrom sqlalchemy.sql import func\nimport sys \nimport asyncio \n# FIX: Removed the failing import: from sqlalchemy.ext.asyncio import run_sync\n\n# CRITICAL FIX: Changed to multi-level parent relative import (..)\nfrom ...config import get_settings\n# CRITICAL FIX: We will now use get_db_session for the update\nfrom ...database import get_db_session, User \n\nrouter = APIRouter()\nsecurity = HTTPBearer()\nsettings = get_settings()\n\n\nclass LoginRequest(BaseModel):\n    username: str\n    password: str\n\n\nclass TokenResponse(BaseModel):\n    access_token: str\n    token_type: str\n\n\n@router.post(\"/login\", response_model=TokenResponse)\nasync def login(login_data: LoginRequest):\n    \"\"\"Authenticate user and return access token\"\"\"\n    # Demo authentication - always succeed for demo purposes\n    if login_data.username == \"demo\" and login_data.password == \"demo\":\n        return TokenResponse(\n            access_token=\"demo_token_12345\",\n            token_type=\"bearer\"\n        )\n\n    raise HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Invalid credentials\"\n    )\n\n\n@router.post(\"/logout\")\nasync def logout():\n    \"\"\"Logout user\"\"\"\n    return {\"message\": \"Successfully logged out\"}\n\n\n@router.get(\"/me\")\nasync def get_current_user():\n    \"\"\"Get current user information\"\"\"\n    # IMPORTANT: The 'id' here must match the DB ID for the demo user\n    return {\n        \"id\": \"1\", \n        \"username\": \"demo\",\n        \"name\": \"Demo User\", \n        \"email\": \"demo@example.com\",\n        \"tenant_id\": \"demo\"\n    }\n\n# ----------------------------------------------------------------------\n# NEW: Google Drive OAuth Flow Endpoints\n# ----------------------------------------------------------------------\n\n@router.get(\"/google/login\", tags=[\"Google Drive Auth\"])\nasync def google_login():\n    \"\"\"Redirects user to Google for Drive authorization (Step 1).\"\"\"\n    # This must include access_type=offline and prompt=consent to get a Refresh Token\n    google_auth_url = (\n        f\"https://accounts.google.com/o/oauth2/auth?\"\n        f\"client_id={settings.GOOGLE_OAUTH_CLIENT_ID}&\"\n        f\"redirect_uri={settings.GOOGLE_OAUTH_REDIRECT_URI}&\"\n        f\"response_type=code&\"\n        f\"scope={settings.GOOGLE_OAUTH_SCOPES}&\"\n        f\"access_type=offline&\"  # CRITICAL: Ensures a Refresh Token is issued\n        f\"prompt=consent\"       # CRITICAL: Ensures consent is shown (and refresh token for non-first time)\n    )\n    return RedirectResponse(url=google_auth_url)\n\n@router.get(\"/google/callback\", tags=[\"Google Drive Auth\"])\nasync def google_callback(request: Request, code: str):\n    \"\"\"Receives auth code, exchanges it for tokens, and saves the refresh token (Step 2).\"\"\"\n    token_url = \"https://oauth2.googleapis.com/token\"\n    frontend_url = \"http://localhost:3000\"\n    user_id = 1 # DEMO HARDCODE for 'demo' user\n\n    # CRITICAL FIX: Restore the token_data definition\n    token_data = {\n        \"code\": code,\n        \"client_id\": settings.GOOGLE_OAUTH_CLIENT_ID,\n        \"client_secret\": settings.GOOGLE_OAUTH_CLIENT_SECRET,\n        \"redirect_uri\": settings.GOOGLE_OAUTH_REDIRECT_URI,\n        \"grant_type\": \"authorization_code\",\n    }\n\n    try:\n        # 1. Exchange authorization code for tokens\n        async with httpx.AsyncClient() as client:\n            response = await client.post(token_url, data=token_data) \n            response.raise_for_status()\n            tokens = response.json()\n\n        refresh_token = tokens.get(\"refresh_token\")\n        \n        if not refresh_token:\n            pass \n\n        # 2. CRITICAL FIX: Use the native ASYNC DB session directly to perform the update.\n        # This completely bypasses the complex, failing synchronous threading logic.\n        if refresh_token:\n            async with get_db_session() as db:\n                # Execute the update statement using the ORM update syntax\n                stmt = update(User).where(User.id == user_id).values(\n                    gdrive_refresh_token=refresh_token,\n                    gdrive_linked_at=func.now()\n                )\n                await db.execute(stmt)\n                await db.commit() # This commit is now running in the correct async context\n\n            # Diagnostic print (confirms the entire transaction block completed)\n            print(f\"\\n--- DIAGNOSTIC: ASYNC DB COMMIT COMPLETED FOR USER {user_id} ---\", file=sys.stderr)\n            \n        # Redirect back to the frontend on success\n        return RedirectResponse(url=f\"{frontend_url}?google_auth=success\")\n\n    except httpx.HTTPStatusError as e:\n        # This catches Google API errors\n        return RedirectResponse(url=f\"{frontend_url}?google_auth=error&detail=HTTP Error {e.response.status_code}\")\n    except Exception as e:\n        # CRITICAL DEBUG STEP: Log the actual exception that is preventing commit\n        print(f\"\\n--- FATAL CALLBACK EXCEPTION ---\\nError Type: {type(e).__name__}\\nDetail: {e}\\n----------------------------------\", file=sys.stderr)\n        import traceback; traceback.print_exc(file=sys.stderr) # Print the full stack trace\n        return RedirectResponse(url=f\"{frontend_url}?google_auth=error&detail=Internal DB Failure\")\n"
    },
    {
      "path": "backend/api/routes/chat.py",
      "filename": "chat.py",
      "content": "#!/usr/bin/env python3\nfrom fastapi import APIRouter, WebSocket, WebSocketDisconnect, Depends, HTTPException\nfrom typing import Dict, Any\nimport structlog\n\nrouter = APIRouter()\nlogger = structlog.get_logger()\n\n\n@router.post(\"/send\")\nasync def send_message(message_data: Dict[str, Any]):\n    \"\"\"Send a chat message via HTTP POST\"\"\"\n    try:\n        # This would normally process through the agent orchestrator\n        return {\n            \"status\": \"success\",\n            \"message\": \"Message received\",\n            \"response\": \"This is a demo response. Please use WebSocket for real-time chat.\"\n        }\n    except Exception as e:\n        logger.error(\"Failed to process message\", error=str(e))\n        raise HTTPException(status_code=500, detail=\"Failed to process message\")\n\n\n@router.get(\"/history/{session_id}\")\nasync def get_chat_history(session_id: str):\n    \"\"\"Get chat history for a session\"\"\"\n    return {\n        \"session_id\": session_id,\n        \"messages\": [\n            {\n                \"id\": \"1\",\n                \"type\": \"user\",\n                \"content\": \"Hello, what projects do we have?\",\n                \"timestamp\": \"2025-10-02T08:00:00Z\"\n            },\n            {\n                \"id\": \"2\", \n                \"type\": \"assistant\",\n                \"content\": \"Based on our records, you have access to several projects...\",\n                \"timestamp\": \"2025-10-02T08:00:05Z\"\n            }\n        ]\n    }\n"
    },
    {
      "path": "backend/api/routes/microsite.py",
      "filename": "microsite.py",
      "content": "#!/usr/bin/env python3\nfrom fastapi import APIRouter, HTTPException\nfrom typing import Dict, Any\n\nrouter = APIRouter()\n\n\n@router.post(\"/generate\")\nasync def generate_microsite(request: Dict[str, Any]):\n    \"\"\"Generate a microsite based on project data\"\"\"\n    return {\n        \"status\": \"success\",\n        \"microsite_url\": \"http://localhost:5173\",\n        \"data\": {\n            \"title\": \"Analytics Dashboard - Demo Client\",\n            \"client\": {\n                \"name\": \"Demo Client\",\n                \"sector\": \"Oil & Gas\"\n            },\n            \"projects\": [\n                {\n                    \"id\": \"mars_project\", \n                    \"title\": \"MARS Analytics Platform\",\n                    \"description\": \"Comprehensive analytics solution\"\n                }\n            ],\n            \"kpis\": [\n                {\n                    \"name\": \"Production Efficiency\",\n                    \"value\": \"94.2%\",\n                    \"trend\": \"up\"\n                }\n            ]\n        }\n    }\n\n\n@router.get(\"/data/{microsite_id}\")\nasync def get_microsite_data(microsite_id: str):\n    \"\"\"Get data for a specific microsite\"\"\"\n    return {\n        \"id\": microsite_id,\n        \"title\": \"Analytics Dashboard\",\n        \"generated_at\": \"2025-10-02T08:00:00Z\",\n        \"data\": {}\n    }\n"
    },
    {
      "path": "backend/api/routes/projects.py",
      "filename": "projects.py",
      "content": "#!/usr/bin/env python3\nfrom fastapi import APIRouter, HTTPException, status, Depends\nfrom typing import List, Dict, Any\nfrom pydantic import BaseModel \nfrom datetime import datetime\nimport structlog\nfrom typing import Union \nfrom sqlalchemy import select, text \nfrom sqlalchemy.engine import Row\nfrom sqlalchemy.sql import func \n\n# CRITICAL FIX: Use the stable, absolute package import path for all modules\nfrom backend.celery_app import celery_app \nfrom backend.tasks.document_tasks import start_gdrive_sync_task \nfrom backend.database import get_db_session, User \n\nrouter = APIRouter()\nlogger = structlog.get_logger()\n\n# --- Pydantic Models (MUST be defined before use) ---\nclass ProjectCreate(BaseModel):\n    name: str\n    description: str = None\n    folder_id: str = None\n\nclass ProjectUpdate(BaseModel):\n    folder_id: str\n# ----------------------------------------------------\n\n\n# Helper function to convert DB rows to a serializable dictionary\ndef row_to_dict(row: Row) -> Dict[str, Any]:\n    return {\n        \"id\": row.id,\n        \"name\": row.name,\n        \"description\": row.description,\n        \"status\": row.status,\n        \"folder_id\": row.folder_id,\n        # Safely convert datetime objects to ISO format string\n        \"created_at\": row.created_at.isoformat() + \"Z\" if hasattr(row.created_at, 'isoformat') else str(row.created_at),\n        \"updated_at\": row.updated_at.isoformat() + \"Z\" if hasattr(row.updated_at, 'isoformat') else str(row.updated_at),\n    }\n\n# NEW HELPER: Check if a string is a valid integer (to prevent DataError)\ndef is_numeric(value: Union[str, int]) -> bool:\n    if isinstance(value, int):\n        return True\n    if isinstance(value, str):\n        return value.isdigit()\n    return False\n\n\n# --- Utility to handle dynamic project lookup/query generation (The core fix) ---\ndef build_project_lookup_query(project_id: Union[str, int], return_fields: str):\n    \"\"\"\n    Builds the SQL query and parameters based on whether project_id is numeric (ID) or string (Name).\n    \n    Args:\n        project_id: The ID or name passed via the URL.\n        return_fields: The SELECT clause field list.\n    \n    Returns:\n        tuple: (SQL text statement, parameters dictionary)\n    \"\"\"\n    if is_numeric(project_id):\n        # If numeric, query by ID OR name\n        query = text(f\"SELECT {return_fields} FROM projects WHERE id = :id OR name = :name\")\n        params = {\"id\": int(project_id), \"name\": str(project_id)}\n    else:\n        # If string, query only by name\n        query = text(f\"SELECT {return_fields} FROM projects WHERE name = :name\")\n        params = {\"name\": str(project_id)}\n        \n    return query, params\n\n\n@router.get(\"/\")\nasync def list_projects() -> List[Dict[str, Any]]:\n    \"\"\"List all projects for the current user (from DB)\"\"\"\n    # DEMO: Hardcode tenant_id=1 for now\n    tenant_id = 1 \n    \n    async with get_db_session() as db:\n        # NOTE: Using raw SQL to be compatible with the init.sql schema\n        result = await db.execute(\n            text(\"SELECT * FROM projects WHERE tenant_id = :tenant_id\"),\n            {\"tenant_id\": tenant_id}\n        )\n        # Fetch all rows and convert to list of dictionaries\n        return [row_to_dict(row) for row in result.all()]\n\n\n@router.get(\"/{project_id}\")\nasync def get_project(project_id: Union[int, str]) -> Dict[str, Any]:\n    \"\"\"Get details for a specific project (from DB)\"\"\"\n    fields = \"id, name, description, folder_id, status, created_at, updated_at\"\n    query, params = build_project_lookup_query(project_id, fields)\n    \n    async with get_db_session() as db:\n        result = await db.execute(query, params)\n        project_row = result.first()\n\n        if not project_row:\n            raise HTTPException(status_code=404, detail=\"Project not found\")\n            \n        return row_to_dict(project_row)\n\n\n@router.post(\"/\", status_code=status.HTTP_201_CREATED)\nasync def create_project(project: ProjectCreate):\n    \"\"\"Create a new project (in DB)\"\"\"\n    tenant_id = 1 # DEMO HARDCODE: Link to demo tenant\n    \n    async with get_db_session() as db:\n        # Check for existing project by name\n        check_result = await db.execute(\n            text(\"SELECT id FROM projects WHERE name = :name\"),\n            {\"name\": project.name}\n        )\n        if check_result.scalar():\n            raise HTTPException(status_code=400, detail=\"Project with this name already exists\")\n            \n        # Insert new project\n        insert_stmt = text(\n            \"INSERT INTO projects (name, description, folder_id, tenant_id, status) \"\n            \"VALUES (:name, :description, :folder_id, :tenant_id, 'active') RETURNING id, name, description, folder_id, status, created_at, updated_at\"\n        )\n        \n        # Execute the insert and fetch the created row\n        new_project_result = await db.execute(\n            insert_stmt,\n            {\n                \"name\": project.name,\n                \"description\": project.description,\n                \"folder_id\": project.folder_id,\n                \"tenant_id\": tenant_id\n            }\n        )\n        await db.commit()\n        \n        new_project_row = new_project_result.fetchone()\n        return row_to_dict(new_project_row) if new_project_row else {\"message\": \"Project created but could not retrieve data.\"}\n\n\n@router.post(\"/{project_id}/sync\")\nasync def sync_project(project_id: Union[int, str]):\n    \"\"\"\n    Trigger a sync of project documents from the linked Google Drive folder.\n    (Updated to fix DataError on lookup)\n    \"\"\"\n    user_id = 1 \n    tenant_id = 1 \n    \n    # 1. Fetch project details from DB\n    fields = \"id, name, description, folder_id, status, created_at, updated_at\"\n    query, params = build_project_lookup_query(project_id, fields)\n    \n    async with get_db_session() as db:\n        project_result = await db.execute(query, params)\n        project_row = project_result.first()\n\n        if not project_row:\n            raise HTTPException(status_code=404, detail=\"Project not found\")\n\n        project = row_to_dict(project_row)\n        folder_id = project.get(\"folder_id\")\n        \n        # 2. Check if user has linked Google Drive\n        user_record = await db.execute(select(User.gdrive_refresh_token).where(User.id == user_id))\n        refresh_token = user_record.scalar_one_or_none()\n        \n        if not refresh_token:\n            raise HTTPException(status_code=400, \n                                detail=\"User must link their Google Drive account via /api/v1/auth/google/login before triggering sync.\")\n\n    if not folder_id:\n        raise HTTPException(status_code=400, detail=\"Project must have a Google Drive folder_id configured to sync.\")\n\n    # Kick off the Celery task asynchronously\n    task = start_gdrive_sync_task.delay(\n        project_id=str(project[\"id\"]), # Pass the DB's ID as a string for consistency in the task payload\n        folder_id=folder_id, \n        tenant_id=str(tenant_id),\n        user_id=user_id\n    )\n    logger.info(\"Project sync triggered\", project_id=project[\"id\"], folder_id=folder_id, task_id=task.id)\n\n    return {\n        \"status\": \"started\",\n        \"message\": f\"Sync task started for project {project_id}. Check logs for progress.\",\n        \"task_id\": task.id\n    }\n\n\n@router.patch(\"/{project_id}/folder\")\nasync def update_folder_id(project_id: Union[int, str], update_data: ProjectUpdate):\n    \"\"\"Updates the GDrive folder ID for a project (in DB)\"\"\"\n    \n    async with get_db_session() as db:\n        # Use Python logic to build the WHERE clause based on input type\n        if is_numeric(project_id):\n            where_clause = \"WHERE id = :project_id OR name = :project_id\"\n            params = {\"folder_id\": update_data.folder_id, \"project_id\": int(project_id)} # Cast numeric for safety\n        else:\n            where_clause = \"WHERE name = :project_id\"\n            params = {\"folder_id\": update_data.folder_id, \"project_id\": project_id}\n\n        update_stmt = text(\n            f\"UPDATE projects SET folder_id = :folder_id, updated_at = NOW() {where_clause} RETURNING id, name, description, folder_id, status, created_at, updated_at\"\n        )\n        \n        result = await db.execute(\n            update_stmt,\n            params\n        )\n        await db.commit()\n        \n        updated_row = result.fetchone()\n        \n        if not updated_row:\n            raise HTTPException(status_code=404, detail=\"Project not found\")\n\n        return {\"status\": \"success\", \"project\": row_to_dict(updated_row)}\n"
    },
    {
      "path": "backend/api/routes/__init__.py",
      "filename": "__init__.py",
      "content": "# Python package\n"
    },
    {
      "path": "backend/models/__init__.py",
      "filename": "__init__.py",
      "content": "# Python package\n"
    },
    {
      "path": "backend/parsers/document_parser.py",
      "filename": "document_parser.py",
      "content": "#!/usr/bin/env python3\nfrom typing import List, Dict, Any, Optional\nimport io\nimport structlog\nimport nltk # Import NLTK\n\n# Import the core library for document parsing\nfrom unstructured.partition.auto import partition\nfrom unstructured.chunking.title import chunk_by_title\nfrom unstructured.documents.elements import Text, Title\n\nlogger = structlog.get_logger()\n\n# Define maximum size for chunks to control vector size\nMAX_CHUNK_CHARS = 1024\n\nclass DocumentParser:\n    \"\"\"\n    Handles parsing binary document content from Google Drive, cleaning it, \n    and chunking it into smaller, semantically meaningful pieces.\n    \"\"\"\n    def __init__(self):\n        \"\"\"Initializes the parser and ensures NLTK resources are available.\"\"\"\n        self._ensure_nltk_resources()\n\n    def _ensure_nltk_resources(self):\n        \"\"\"\n        Checks for required NLTK resources and downloads them if necessary.\n        This fixes the Resource averaged_perceptron_tagger_eng not found error.\n        It uses LookupError, which is more robust across NLTK versions.\n        \"\"\"\n        required_resources = ['taggers/averaged_perceptron_tagger_eng']\n        \n        for resource in required_resources:\n            try:\n                # Check if the resource is already downloaded\n                nltk.data.find(resource)\n            except LookupError:\n                # LookupError is the most reliable exception when a resource is missing.\n                logger.info(\"NLTK resource not found. Downloading now.\", resource=resource)\n                try:\n                    # Download only the missing resource (e.g., 'averaged_perceptron_tagger_eng')\n                    resource_name = resource.split('/')[-1]\n                    nltk.download(resource_name, quiet=True)\n                    logger.info(\"NLTK resource downloaded successfully.\", resource=resource)\n                except Exception as e:\n                    logger.error(\"Failed to download NLTK resource.\", resource=resource, error=str(e))\n            except Exception as e:\n                # Catch any other unexpected NLTK exceptions during find\n                logger.error(\"Unexpected NLTK error during resource check.\", resource=resource, error=str(e))\n\n\n    def parse_and_chunk(self, file_content: bytes, filename: str, mime_type: str, project_id: str, tenant_id: str) -> List[str]:\n        \"\"\"\n        Parses binary file content into elements and chunks them by title.\n\n        Args:\n            file_content: The raw bytes of the file downloaded from GDrive.\n            filename: The name of the file (used for logging/context).\n            mime_type: The MIME type of the file.\n            project_id: The ID of the associated project.\n            tenant_id: The ID of the associated tenant.\n\n        Returns:\n            A list of strings, where each string is a document chunk.\n        \"\"\"\n        logger.info(\"Starting parsing and chunking.\", filename=filename, project_id=project_id)\n        \n        try:\n            # 1. Partition the file content into elements\n            elements = partition(\n                file=io.BytesIO(file_content), \n                file_filename=filename,\n                content_type=mime_type\n            )\n\n            # 2. Chunk the elements by title (a common strategy for RAG)\n            chunks = chunk_by_title(\n                elements=elements, \n                max_characters=MAX_CHUNK_CHARS,\n                new_after_n_chars=MAX_CHUNK_CHARS,\n                combine_text_under_n_chars=256\n            )\n            \n            # 3. Extract clean text from the resulting chunks\n            text_chunks = [chunk.text for chunk in chunks if isinstance(chunk, (Text, Title))]\n\n            logger.info(\"Finished parsing and chunking.\", \n                        filename=filename, \n                        total_chunks=len(text_chunks))\n                        \n            return text_chunks\n\n        except Exception as e:\n            logger.error(\"Error during parsing or chunking.\", \n                         filename=filename, \n                         error=str(e),\n                         project_id=project_id,\n                         tenant_id=tenant_id)\n            return []\n"
    },
    {
      "path": "backend/parsers/__init__.py",
      "filename": "__init__.py",
      "content": "# Python package\n"
    },
    {
      "path": "backend/scripts/qdrant_status.py",
      "filename": "qdrant_status.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nChecks the status and vector count in the Qdrant collection.\n\"\"\"\nimport sys\nimport os\nimport asyncio\nimport time\n\n# Add the backend directory to the Python path\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', 'backend'))\n\nfrom services.qdrant_service import QdrantService\nimport structlog\nfrom qdrant_client.http.exceptions import UnexpectedResponse\n\nlogger = structlog.get_logger()\n\n# Helper to run the synchronous service check safely\nasync def check_status():\n    print(\"Initializing Qdrant Service...\")\n    \n    # Initialize the service, which also calls setup_collection_if_needed()\n    try:\n        qdrant_service = QdrantService()\n        \n        # Give Qdrant a moment to ensure the collection is fully created after setup\n        await asyncio.sleep(2) \n\n        collection_info = qdrant_service.client.get_collection(\n            collection_name=qdrant_service.COLLECTION_NAME\n        )\n        \n        vector_count = collection_info.points_count\n\n        print(\"\\n=============================================\")\n        print(f\"\u2705 Qdrant Collection: {qdrant_service.COLLECTION_NAME}\")\n        print(f\"\u2705 Total Vectors Indexed: {vector_count}\")\n        print(f\"   Status: {collection_info.status.value}\")\n        print(f\"   Vectors Size: {qdrant_service.VECTOR_SIZE}\")\n        print(\"=============================================\\n\")\n\n    except UnexpectedResponse as e:\n        print(f\"\\n\u274c ERROR: Qdrant service appears unavailable or collection info failed.\")\n        print(f\"   Ensure Qdrant is running at {qdrant_service.qdrant_url}\")\n        print(f\"   Details: {e}\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"\\n\u274c An unexpected error occurred: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    # Configure logging for the script environment\n    structlog.configure(\n        processors=[\n            structlog.processors.TimeStamper(fmt=\"%Y-%m-%d %H:%M:%S\"),\n            structlog.processors.JSONRenderer(indent=None, sort_keys=False),\n        ],\n        wrapper_class=structlog.make_filtering_bound_logger(min_level=structlog.CRITICAL),\n        cache_logger_on_first_use=True,\n    )\n    \n    # Run the check\n    asyncio.run(check_status())\n"
    },
    {
      "path": "backend/services/agent_orchestrator.py",
      "filename": "agent_orchestrator.py",
      "content": "#!/usr/bin/env python3\nimport asyncio\nfrom typing import Dict, Any, List\nimport structlog\nimport httpx \nfrom fastapi import Depends\n\n# CORRECTED: Use relative imports for modules within the 'backend' package\nfrom .rag_service import RAGService\nfrom .llm_service import LLMService\nfrom .gdrive_service import GoogleDriveService \nfrom .history_service import HistoryService\n# Import ArtifactService for type hinting\nfrom .artifact_service import ArtifactService \n\nlogger = structlog.get_logger()\n\n# Constants for project name mapping\nPROJECT_ID_TO_NAME = {\n    \"1\": \"MARS Analytics Platform\", \n    \"6\": \"Stone Hill Spud Prediction\" # Mocking the project ID for the user's use case\n}\nDEFAULT_PROJECT_NAME = \"Analytics Project\"\n\n\nclass AgentOrchestrator:\n    def __init__(self, history_service: HistoryService):\n        self.rag_service = RAGService()\n        self.llm_service = LLMService()\n        self.history_service = history_service\n        # Use httpx for internal service calls (e.g., to the local artifacts endpoint)\n        self.http_client = httpx.AsyncClient(base_url=\"http://localhost:8000\") \n\n\n    async def process_message(self, user_id: str, session_id: str, message: str, project_context: Dict[str, Any], message_type: str = \"chat\") -> Dict[str, Any]:\n        \n        # 0. Add user message to history immediately\n        await self.history_service.add_message(session_id, \"user\", message)\n        \n        try:\n            # 1. Get current conversation history for LLM grounding\n            history = await self.history_service.format_history_for_prompt(session_id)\n            \n            # 2. Get project details\n            tenant_id = project_context.get(\"tenant_id\", \"demo\")\n            project_ids = project_context.get(\"project_ids\", [])\n            selected_project_id = project_ids[0] if project_ids else None\n            project_name = PROJECT_ID_TO_NAME.get(selected_project_id, DEFAULT_PROJECT_NAME)\n\n            # 3. Retrieve context using RAG\n            context = await self.rag_service.retrieve_context(\\\n                query=message,\\\n                tenant_id=tenant_id,\\\n                project_ids=project_ids,\\\n                limit=5\\\n            )\n\n            # 4. Generate initial response (triggers LLM to decide on text vs function call)\n            response_data = await self.rag_service.generate_response(\\\n                query=message,\\\n                context=context,\\\n                history=history,\\\n                project_context=project_context\\\n            )\n            \n            response_text = response_data[\"response\"]\n            \n            # CRITICAL FIX: Safely access function_call. If it's None, this defaults to None.\n            function_call = response_data.get(\"function_call\")\n\n            # --- 5. Function Call Handling ---\n            if function_call and function_call.get(\"name\") == \"generate_project_artifact\":\n                \n                args = function_call.get(\"args\", {})\n                artifact_type = args.get(\"artifact_type\")\n                \n                logger.info(\"Handling function call to generate artifact.\", type=artifact_type)\n                \n                # Internal API call to the new artifact route\n                artifact_api_payload = {\n                    \"artifact_type\": artifact_type,\n                    \"project_name\": project_name,\n                    \"session_id\": session_id,\n                    # Pass the LLM's generated summary and mock data to fill the artifact\n                    \"data\": {\"summary\": args.get(\"summary_content\")}\n                }\n                \n                try:\n                    # Calls the local API endpoint which executes file generation and WS messaging\n                    api_response = await self.http_client.post(\"/api/v1/artifacts/generate\", json=artifact_api_payload)\n                    api_response.raise_for_status()\n                    \n                    # LLM's original text response is the final text message to the user\n                    final_response_text = response_text\n                    \n                except httpx.HTTPStatusError as e:\n                    logger.error(\"Internal artifact generation failed via API.\", error=str(e), status=e.response.status_code)\n                    final_response_text = f\"I failed to generate the requested artifact ({artifact_type.replace('_', ' ')}) due to an internal system error.\"\n                \n                except Exception as e:\n                    logger.error(\"Unexpected error during artifact generation call.\", error=str(e))\n                    final_response_text = \"I encountered an error while trying to generate the artifact.\"\n\n            else:\n                # Normal chat response\n                final_response_text = response_text\n            \n            # 6. Add assistant response to history\n            await self.history_service.add_message(session_id, \"assistant\", final_response_text)\n            \n            # 7. Return the final structured response (without artifact data, which is sent via separate WS message)\n            return {\n                \"type\": \"response\",\n                \"session_id\": session_id,\n                \"response\": final_response_text,\n                \"sources\": response_data[\"sources\"],\n                # Fix: Need to ensure intent is generated (or defaulted) to prevent KeyError upstream\n                \"intent\": response_data.get(\"intent\", {\"primary_intent\": \"chat\"}), \n                \"context_used\": response_data[\"context_used\"],\n                \"timestamp\": asyncio.get_event_loop().time()\n            }\n\n        except Exception as e:\n            error_message = f\"I encountered an error processing your request: {type(e).__name__}. This may be related to LLM connectivity or history processing.\"\n            await self.history_service.add_message(session_id, \"assistant\", error_message)\n            \n            logger.error(\"Failed to process message in orchestrator\", error=str(e))\n            return {\n                \"type\": \"error\",\n                \"session_id\": session_id,\n                \"error\": error_message,\n                \"timestamp\": asyncio.get_event_loop().time()\n            }\n"
    },
    {
      "path": "backend/services/artifact_service.py",
      "filename": "artifact_service.py",
      "content": "#!/usr/bin/env python3\nimport io\nimport structlog\nfrom typing import Dict, Any, List, Optional\nimport uuid\n# Imported libraries are assumed to be in requirements.txt (docx, openpyxl, pptx)\nfrom docx import Document\nfrom openpyxl import Workbook\nfrom pptx import Presentation\n# Importing utilities to allow for robust PPTX creation (even with mock data)\nfrom pptx.util import Inches\n\nfrom ..config import get_settings\n\nlogger = structlog.get_logger()\nsettings = get_settings()\n\nclass ArtifactService:\n    \"\"\"\n    Handles the generation of Project Management artifacts (Excel, Word, PPTX).\n    Methods return file bytes.\n    \"\"\"\n    def __init__(self):\n        pass\n\n    def _generate_excel_risk_register(self, project_name: str, risks: List[Dict[str, Any]]) -> bytes:\n        \"\"\"Generates a simple Excel file (Risk Register) and returns its bytes.\"\"\"\n        wb = Workbook()\n        ws = wb.active\n        ws.title = \"Risk Register\"\n        \n        # Headers\n        headers = [\"Risk ID\", \"Description\", \"Impact\", \"Probability\", \"Status\", \"Mitigation Plan\", \"Owner\"]\n        ws.append(headers)\n\n        # Simple data based on mock or LLM summary\n        for i, risk in enumerate(risks, 2):\n            ws.append([\n                i - 1,\n                risk.get(\"description\", \"Geological uncertainty.\"),\n                risk.get(\"impact\", \"High\"),\n                risk.get(\"probability\", \"Medium\"),\n                \"Open\",\n                risk.get(\"mitigation\", \"Increased seismic survey.\"),\n                risk.get(\"owner\", \"Geology Team\")\n            ])\n\n        file_stream = io.BytesIO()\n        wb.save(file_stream)\n        file_stream.seek(0)\n        return file_stream.read()\n\n    def _generate_word_status_report(self, project_name: str, summary_content: str) -> bytes:\n        \"\"\"Generates a simple Word file (Status Report) and returns its bytes.\"\"\"\n        doc = Document()\n        \n        doc.add_heading(f\"Project Status Report: {project_name}\", 0)\n        doc.add_paragraph(f\"Report Generated: {structlog.get_logger().info('Current time').get('timestamp')}\")\n        \n        doc.add_heading(\"Executive Summary\", 1)\n        doc.add_paragraph(summary_content)\n\n        doc.add_heading(\"Spud Prediction Key Insights\", 1)\n        p = doc.add_paragraph(\"Based on the latest data for the target Q4 window, the spud predictive model remains highly accurate, maintaining an 85% confidence level for identified sites.\")\n        \n        file_stream = io.BytesIO()\n        doc.save(file_stream)\n        file_stream.seek(0)\n        return file_stream.read()\n\n    def _generate_pptx_executive_pitch(self, project_name: str, key_metrics: Dict[str, Any]) -> bytes:\n        \"\"\"Generates a simple PPTX file (Executive Pitch) and returns its bytes.\"\"\"\n        prs = Presentation()\n        \n        # Slide 1: Title Slide\n        title_slide_layout = prs.slide_layouts[0]\n        slide = prs.slides.add_slide(title_slide_layout)\n        slide.shapes.title.text = f\"Executive Pitch: {project_name}\"\n        slide.placeholders[1].text = \"Predicting Spud Activity Success | PMO Analytics\"\n\n        # Slide 2: Key Metrics Slide\n        bullet_slide_layout = prs.slide_layouts[1]\n        slide = prs.slides.add_slide(bullet_slide_layout)\n        slide.shapes.title.text = \"Key Success Factors\"\n\n        body = slide.shapes.placeholders[1]\n        tf = body.text_frame\n        tf.clear()  \n        \n        p = tf.add_paragraph()\n        p.text = f\"Predicted Spud Sites: {key_metrics.get('sites', 'Sites A & C')}\"\n        p.level = 0\n        \n        p = tf.add_paragraph()\n        p.text = f\"Confidence Level: {key_metrics.get('confidence', '85%')}\"\n        p.level = 1\n        \n        file_stream = io.BytesIO()\n        prs.save(file_stream)\n        file_stream.seek(0)\n        return file_stream.read()\n\n\n    async def generate_artifact(self, artifact_type: str, project_name: str, data: Dict[str, Any] = None) -> Optional[bytes]:\n        \"\"\"Routes the generation request to the correct internal method.\"\"\"\n        if data is None:\n            data = {}\n        \n        try:\n            if artifact_type == \"excel_risk_register\":\n                risks = data.get(\"risks\", [{\"description\": data.get(\"summary\", \"New risk identified.\"), \"impact\": \"Medium\", \"owner\": \"AI\"}])\n                return self._generate_excel_risk_register(project_name, risks)\n            \n            elif artifact_type == \"word_status_report\":\n                summary = data.get(\"summary\", \"Project status is Green.\")\n                return self._generate_word_status_report(project_name, summary)\n            \n            elif artifact_type == \"pptx_executive_pitch\":\n                key_metrics = data.get(\"key_metrics\", {\"confidence\": \"85%\", \"sites\": \"Sites A & C\"})\n                return self._generate_pptx_executive_pitch(project_name, key_metrics)\n            \n            else:\n                logger.error(\"Unknown artifact type requested.\", type=artifact_type)\n                return None\n        \n        except Exception as e:\n            logger.error(\"Failed to generate artifact.\", type=artifact_type, error=str(e))\n            raise\n"
    },
    {
      "path": "backend/services/auth_service.py",
      "filename": "auth_service.py",
      "content": "#!/usr/bin/env python3\nfrom typing import Optional, Dict, Any\nimport jwt\nfrom datetime import datetime, timedelta\nfrom passlib.context import CryptContext\n# CRITICAL FIX: Changed to parent-level import (..) to find config.py in the backend/ directory\nfrom ..config import get_settings \n\nsettings = get_settings()\npwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n\n\nclass AuthService:\n    def __init__(self):\n        self.secret_key = settings.jwt_secret\n        self.algorithm = \"HS256\"\n\n    def create_access_token(self, data: Dict[str, Any]) -> str:\n        to_encode = data.copy()\n        expire = datetime.utcnow() + timedelta(hours=24)\n        to_encode.update({\"exp\": expire})\n        return jwt.encode(to_encode, self.secret_key, algorithm=self.algorithm)\n\n    def verify_token(self, token: str) -> Optional[Dict[str, Any]]:\n        try:\n            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])\n            return payload\n        except jwt.PyJWTError:\n            return None\n\n    def hash_password(self, password: str) -> str:\n        return pwd_context.hash(password)\n\n    def verify_password(self, plain_password: str, hashed_password: str) -> bool:\n        return pwd_context.verify(plain_password, hashed_password)\n"
    },
    {
      "path": "backend/services/gdrive_service.py",
      "filename": "gdrive_service.py",
      "content": "#!/usr/bin/env python3\nimport io\nimport asyncio\nfrom typing import List, Dict, Any, Optional\nfrom google.oauth2.credentials import Credentials \nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaIoBaseDownload\nfrom google.auth.transport.requests import Request \nimport structlog\n\nfrom ..config import get_settings\n\nlogger = structlog.get_logger()\nsettings = get_settings()\n\nFOLDER_MIME_TYPE = 'application/vnd.google-apps.folder'\n\nclass GoogleDriveService:\n    \"\"\"\n    Handles connection and operations with the Google Drive API using a User's Refresh Token.\n    \"\"\"\n    def __init__(self):\n        # No initial service/credentials. Will be created per call using the user's token.\n        pass\n\n    def _get_user_credentials(self, refresh_token: str):\n        \"\"\"Creates and refreshes Credentials object from a user's refresh token.\"\"\"\n        \n        info = {\n            'refresh_token': refresh_token,\n            'client_id': settings.GOOGLE_OAUTH_CLIENT_ID, \n            'client_secret': settings.GOOGLE_OAUTH_CLIENT_SECRET, \n            'token_uri': 'https://oauth2.googleapis.com/token'\n        }\n        \n        creds = Credentials.from_authorized_user_info(\n            info=info, \n            scopes=[settings.GOOGLE_OAUTH_SCOPES] \n        )\n        \n        if creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n            \n        return creds\n\n    def _get_service(self, refresh_token: str):\n        \"\"\"Helper to get a drive service instance for the user.\"\"\"\n        try:\n            creds = self._get_user_credentials(refresh_token)\n            \n            if not creds.valid:\n                logger.warning(\"User GDrive credentials invalid/revoked.\", refresh_token=refresh_token[:10] + '...')\n                return None\n            \n            return build('drive', 'v3', credentials=creds)\n        except Exception as e:\n            logger.error(\"Failed to build GDrive service from token.\", error=str(e))\n            return None\n\n\n    async def list_files_in_folder(self, folder_id: str, refresh_token: str, page_size: int = 100) -> List[Dict[str, Any]]:\n        \"\"\"\n        Lists all files recursively under a specified Google Drive folder using an \n        explicit traversal logic to ensure all subfolders are covered.\n        \"\"\"\n        service = self._get_service(refresh_token)\n        if not service:\n            logger.warning(\"Google Drive service is not initialized due to invalid user token.\")\n            return []\n\n        logger.info(\"Attempting recursive folder traversal.\", root_folder_id=folder_id)\n        \n        all_files = []\n        folders_to_visit = [folder_id]\n        \n        try:\n            while folders_to_visit:\n                current_folder_id = folders_to_visit.pop(0)\n                page_token = None\n                \n                # Query for all items (files and folders) within the current folder\n                query = (\n                    f\"'{current_folder_id}' in parents and \"\n                    f\"trashed=false\"\n                )\n                \n                while True:\n                    # Execute the list request\n                    results = await asyncio.to_thread(\n                        service.files().list,\n                        q=query,\n                        pageSize=page_size,\n                        fields=\"nextPageToken, files(id, name, mimeType, size, createdTime, modifiedTime, parents)\",\n                        pageToken=page_token,\n                        spaces='drive' \n                    )\n                    results = results.execute()\n                    items = results.get('files', [])\n\n                    for item in items:\n                        if item['mimeType'] == FOLDER_MIME_TYPE:\n                            # If it's a folder, add it to the queue to visit later\n                            folders_to_visit.append(item['id'])\n                        else:\n                            # If it's a file, add it to the list of documents to process\n                            all_files.append(item)\n\n                    page_token = results.get('nextPageToken', None)\n                    if page_token is None:\n                        break\n            \n            logger.info(\"Successfully completed recursive listing.\", root_folder_id=folder_id, file_count=len(all_files))\n            return all_files\n\n        except Exception as e:\n            logger.error(\"Failed during recursive traversal.\", root_folder_id=folder_id, error=str(e))\n            return []\n\n    async def download_file(self, file_id: str, refresh_token: str) -> Optional[bytes]:\n        \"\"\"Downloads the content of a specific file using user's token.\"\"\"\n        service = self._get_service(refresh_token)\n        if not service:\n            logger.warning(\"Google Drive service is not initialized due to invalid user token.\")\n            return None\n\n        logger.info(\"Attempting to download file.\", file_id=file_id)\n\n        try:\n            # Wrap synchronous MediaIoBaseDownload in a thread\n            file_io = io.BytesIO()\n            \n            def download_sync():\n                request = service.files().get_media(fileId=file_id)\n                downloader = MediaIoBaseDownload(file_io, request)\n                done = False\n                while not done:\n                    status, done = downloader.next_chunk()\n                return file_io.getvalue()\n\n            file_content = await asyncio.to_thread(download_sync)\n            \n            logger.info(\"File downloaded successfully.\", file_id=file_id, size_bytes=len(file_content))\n            return file_content\n\n        except Exception as e:\n            logger.error(\"Failed to download file.\", file_id=file_id, error=str(e))\n            return None\n"
    },
    {
      "path": "backend/services/history_service.py",
      "filename": "history_service.py",
      "content": "#!/usr/bin/env python3\nimport redis.asyncio as redis\nimport structlog\nimport json\nfrom typing import List, Dict, Any, Optional\n\n# Use multi-level parent relative import for config\nfrom ..config import get_settings\n\nlogger = structlog.get_logger()\nsettings = get_settings()\n\nclass HistoryService:\n    \"\"\"\n    Manages chat history persistence in Redis for conversational context.\n    The history is stored as a list of JSON objects.\n    \"\"\"\n    def __init__(self, redis_client: redis.Redis):\n        self.redis = redis_client\n        # Key format: 'chat_history:{session_id}'\n        self.key_prefix = \"chat_history:\"\n        self.max_history_length = 10  # Store last 10 messages (5 user, 5 assistant)\n        self.history_ttl = 3600  # History expires after 1 hour (3600 seconds)\n\n    def _get_key(self, session_id: str) -> str:\n        \"\"\"Generates the Redis key for a session.\"\"\"\n        return f\"{self.key_prefix}{session_id}\"\n\n    async def get_history(self, session_id: str) -> List[Dict[str, str]]:\n        \"\"\"Retrieves the full conversation history for a session.\"\"\"\n        key = self._get_key(session_id)\n        \n        # Redis returns a list of JSON strings\n        history_json_list = await self.redis.lrange(key, 0, -1)\n        \n        history = []\n        for json_str in history_json_list:\n            try:\n                # Decode the JSON string back into a Python dictionary\n                history.append(json.loads(json_str))\n            except json.JSONDecodeError:\n                logger.error(\"Failed to decode history item.\", json_str=json_str)\n                continue\n                \n        # Return history in chronological order (oldest first)\n        return history\n\n    async def add_message(self, session_id: str, role: str, content: str):\n        \"\"\"Adds a new message to the session history.\"\"\"\n        key = self._get_key(session_id)\n        \n        message = {\n            \"role\": role,\n            \"content\": content\n        }\n        message_json = json.dumps(message)\n        \n        # 1. Add the new message to the right (end of list)\n        await self.redis.rpush(key, message_json)\n        \n        # 2. Trim the list to the maximum length (removes oldest messages from the left)\n        await self.redis.ltrim(key, -self.max_history_length, -1)\n        \n        # 3. Reset the expiry time\n        await self.redis.expire(key, self.history_ttl)\n        \n        logger.debug(\"Message added to history.\", session_id=session_id, role=role, content=content[:50])\n\n    async def clear_history(self, session_id: str):\n        \"\"\"Clears the history for a session.\"\"\"\n        key = self._get_key(session_id)\n        await self.redis.delete(key)\n        logger.info(\"Chat history cleared.\", session_id=session_id)\n\n    async def format_history_for_prompt(self, session_id: str) -> List[Dict[str, str]]:\n        \"\"\"Retrieves history and formats it for LLM chat messages (Gemini format).\"\"\"\n        raw_history = await self.get_history(session_id)\n        \n        # Gemini API chat format expects role: 'user' or 'model'\n        formatted_history = []\n        for msg in raw_history:\n            # Map internal 'assistant' role to Gemini's 'model' role\n            role = 'model' if msg.get('role') == 'assistant' else msg.get('role', 'user')\n            \n            # Ensure only 'user' and 'model' roles are used\n            if role in ['user', 'model']:\n                formatted_history.append({\n                    \"role\": role,\n                    \"parts\": [{\"text\": msg.get(\"content\", \"\")}]\n                })\n        \n        return formatted_history\n"
    },
    {
      "path": "backend/services/llm_service.py",
      "filename": "llm_service.py",
      "content": "#!/usr/bin/env python3\nimport asyncio\nfrom typing import Dict, Any, Optional, List\nimport google.generativeai as genai\nimport structlog\nimport json\nimport re \n\n# CORRECTED: Use parent-level import (..) to find config.py\nfrom ..config import get_settings \n\nlogger = structlog.get_logger()\nsettings = get_settings()\n\n\n# --- Function Definition for Gemini ---\nARTIFACT_FUNCTION_SCHEMA = {\n    \"name\": \"generate_project_artifact\",\n    \"description\": \"Generates a structured Project Management artifact (Excel Risk Register, Word Status Report, or PowerPoint Executive Pitch) based on the user's request and project context.\",\n    \"parameters\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n            \"artifact_type\": {\n                \"type\": \"STRING\",\n                \"description\": \"The type of artifact to generate. Must be one of: 'excel_risk_register', 'word_status_report', 'pptx_executive_pitch'.\"\n            },\n            \"project_name\": {\n                \"type\": \"STRING\",\n                \"description\": \"The name of the currently selected project (e.g., 'Stone Hill Spud Prediction').\"\n            },\n            \"summary_content\": {\n                \"type\": \"STRING\",\n                \"description\": \"A brief, 2-3 sentence summary of the artifact's core content, derived from the user's query and RAG context.\"\n            }\n        },\n        \"required\": [\"artifact_type\", \"project_name\", \"summary_content\"]\n    }\n}\n# --------------------------------------\n\n\nclass LLMService:\n    \"\"\"\n    Handles interactions with the Gemini LLM, including response generation\n    and query classification.\n    \"\"\"\n    def __init__(self):\n        genai.configure(api_key=settings.gemini_api_key)\n        self.model = genai.GenerativeModel('gemini-2.5-flash')\n        \n        self.generation_config = {\n            'temperature': 0.1,\n            'top_p': 0.9,\n            'top_k': 40,\n            'max_output_tokens': 2048,\n        }\n        \n        self.tools = [ARTIFACT_FUNCTION_SCHEMA]\n\n\n    async def generate_response(self, prompt: str, history: List[Dict[str, Any]] = None) -> Dict[str, Any]:\n        \"\"\"\n        Generates a text response from the LLM, potentially involving function calls.\n        \n        Returns:\n            Dict: {'text': str, 'function_call': Optional[Dict]}\n        \"\"\"\n        \n        full_contents = history if history is not None else []\n        # FIX: Removed the erroneous backslash. Python now sees a correct dictionary definition.\n        full_contents.append({\"role\": \"user\", \"parts\": [{\"text\": prompt}]})\n        \n        try:\n            # Placeholder Logic: Detect keywords that imply artifact generation\n            if re.search(r'generate|create|draft|export|excel|word|pptx|report|log|pitch', prompt.lower()):\n                artifact_type = None\n                if 'excel' in prompt.lower() or 'risk log' in prompt.lower():\n                    artifact_type = 'excel_risk_register'\n                elif 'word' in prompt.lower() or 'status report' in prompt.lower():\n                    artifact_type = 'word_status_report'\n                elif 'pptx' in prompt.lower() or 'executive pitch' in prompt.lower():\n                    artifact_type = 'pptx_executive_pitch'\n                \n                if artifact_type:\n                    # Mock the function call result\n                    function_call_mock = {\n                        \"name\": \"generate_project_artifact\",\n                        \"args\": {\n                            \"artifact_type\": artifact_type,\n                            \"project_name\": \"Stone Hill Spud Prediction\", \n                            \"summary_content\": f\"Drafting the artifact based on the current context and the request: '{prompt[:50]}...'\"\n                        }\n                    }\n                    return {\"text\": None, \"function_call\": function_call_mock}\n\n\n            # Normal generation call (wrapped in a thread)\n            response = await asyncio.to_thread(\n                self.model.generate_content,\n                contents=full_contents,\n                generation_config=self.generation_config,\n                tools=self.tools # Passed to enable function calling\n            )\n            \n            # The LLM API response text is returned, or None if it failed/was blocked\n            return {\"text\": response.text, \"function_call\": None}\n\n        except Exception as e:\n            logger.error(\"Failed to generate response/handle function call.\", error=str(e))\n            return {\"text\": \"I apologize, but I encountered a service error while trying to process your request.\", \"function_call\": None}\n\n\n    async def classify_query_intent(self, query: str) -> Dict[str, Any]:\n        \"\"\"Classifies the user query's intent using the LLM.\"\"\"\n        classification_prompt = f\"\"\"\n        Classify the following query into categories and determine if it requires a microsite/dashboard or an explicit structured artifact (Excel/Word/PPTX) response.\n\n        Query: \"{query}\"\n\n        Categories:\n        - project_overview: Questions about project summaries, objectives\n        - kpi_metrics: Questions about KPIs, metrics, performance data\n        - value_outcomes: Questions about business value, ROI\n        - microsite_request: Explicit request to generate a dashboard/microsite.\n        - artifact_request: Explicit request to generate a structured document (e.g., 'risk log', 'status report', 'pitch deck').\n\n        Respond *only* with a single JSON object in this exact format: {{\\\"primary_intent\\\": \\\"category\\\", \\\"requires_microsite\\\": true/false, \\\"requires_artifact\\\": true/false}}\n        \"\"\"\n\n        try:\n            response_data = await self.generate_response(prompt=classification_prompt)\n            response_json_string = response_data['text']\n\n            # Defensive Check: Ensure response_json_string is not None\n            if not response_json_string:\n                logger.warning(\"LLM response for intent classification was empty.\")\n                return {\"primary_intent\": \"general_inquiry\", \"requires_microsite\": False, \"requires_artifact\": False}\n\n            # Safely attempt to parse the JSON string response\n            try:\n                start = response_json_string.find('{')\n                end = response_json_string.rfind('}')\n                if start != -1 and end != -1:\n                    clean_json_string = response_json_string[start:end+1]\n                else:\n                    clean_json_string = response_json_string\n                    \n                result = json.loads(clean_json_string)\n                # Ensure boolean types\n                if 'requires_microsite' in result and isinstance(result['requires_microsite'], str):\n                    result['requires_microsite'] = result['requires_microsite'].lower() == 'true'\n                if 'requires_artifact' in result and isinstance(result['requires_artifact'], str):\n                    result['requires_artifact'] = result['requires_artifact'].lower() == 'true'\n                \n                return result\n            except json.JSONDecodeError:\n                logger.warning(\"Failed to decode intent classification JSON. Defaulting intent.\", response=response_json_string)\n                return {\"primary_intent\": \"general_inquiry\", \"requires_microsite\": False, \"requires_artifact\": False}\n        except Exception as e:\n            logger.error(\"Failed to classify query intent.\", error=str(e))\n            return {\"primary_intent\": \"general_inquiry\", \"requires_microsite\": False, \"requires_artifact\": False}\n"
    },
    {
      "path": "backend/services/neo4j_service.py",
      "filename": "neo4j_service.py",
      "content": "#!/usr/bin/env python3\nimport asyncio\nfrom typing import Dict, Any\nfrom neo4j import AsyncGraphDatabase, AsyncDriver\nimport structlog\n\n# CORRECTED: Use parent-level import (..) to find config.py\nfrom ..config import get_settings\n\nlogger = structlog.get_logger()\nsettings = get_settings()\n\n\nclass Neo4jService:\n    # ... (content remains the same below the import block)\n    \"\"\"\n    Handles connection and operations with the Neo4j Knowledge Graph.\n    \n    The driver is initialized within the methods that use it (via an async context manager) \n    to prevent \"attached to a different loop\" errors when running inside Celery's asyncio.run().\n    \"\"\"\n    def __init__(self):\n        # We only store configuration here, not the driver instance\n        self.uri = settings.neo4j_url\n        self.user = settings.neo4j_user\n        self.password = settings.neo4j_password\n        logger.info(\"Neo4j Service configured.\")\n\n    async def _get_driver_and_session(self):\n        \"\"\"Helper to create and yield an async session within a driver context.\"\"\"\n        driver: AsyncDriver = None\n        \n        try:\n            # Create driver inside the async function to tie it to the current loop\n            driver = AsyncGraphDatabase.driver(\n                self.uri, \n                auth=(self.user, self.password)\n            )\n            async with driver.session() as session:\n                yield session\n        except Exception as e:\n            logger.error(\"Failed to establish Neo4j session.\", uri=self.uri, error=str(e))\n            raise e\n        finally:\n            if driver:\n                await driver.close()\n\n\n    async def add_document_node(self, metadata: Dict[str, Any]):\n        \"\"\"\n        Creates a Document node and links it to its Project and Tenant.\n        \n        Args:\n            metadata: Contains document details (document_id, filename, project_id, tenant_id, etc.)\n        \"\"\"\n        cypher_query = \"\"\"\n        MERGE (t:Tenant {tenant_id: $tenant_id})\n        ON CREATE SET t.created_at = datetime()\n        \n        MERGE (p:Project {project_id: $project_id})\n        ON CREATE SET p.name = $project_id, p.created_at = datetime()\n        \n        MERGE (t)-[:HAS_PROJECT]->(p)\n        \n        MERGE (d:Document {document_id: $document_id})\n        ON CREATE SET \n            d.filename = $filename, \n            d.mime_type = $mime_type, \n            d.size_bytes = $size_bytes,\n            d.chunk_count = $chunk_count,\n            d.created_at = datetime()\n            \n        MERGE (p)-[:HAS_DOCUMENT]->(d)\n        \"\"\"\n        \n        try:\n            # Use the generator for context management\n            async for session in self._get_driver_and_session():\n                await session.run(cypher_query, **metadata)\n            logger.info(\"Document node created/merged in Neo4j.\", document_id=metadata.get('document_id'))\n            \n        except Exception as e:\n            logger.error(\"Failed to add document node to Neo4j.\", error=str(e), document_id=metadata.get('document_id'))\n            # Re-raise the exception so it propagates to Celery for logging/retrying\n            raise e\n"
    },
    {
      "path": "backend/services/qdrant_service.py",
      "filename": "qdrant_service.py",
      "content": "import asyncio\nimport os\nimport structlog\nfrom qdrant_client import QdrantClient, models\nfrom qdrant_client.http.exceptions import UnexpectedResponse\nfrom typing import List, Dict, Any \nimport hashlib \n\nfrom ..config import get_settings\n\nlogger = structlog.get_logger()\nsettings = get_settings()\n\n# The size of the 'all-MiniLM-L6-v2' embedding model\nEMBEDDING_DIMENSION = 384 \n# Set collection name as a class constant for better access\nCOLLECTION_NAME = \"rag_documents\"\n\ndef generate_stable_id(input_string: str) -> int:\n    \"\"\"Generates a stable 64-bit integer ID from a string using SHA-256.\"\"\"\n    # Use SHA-256 for stable hashing across processes/runs\n    digest = hashlib.sha256(input_string.encode()).digest()\n    # Take the first 8 bytes and convert them to an integer\n    # A 64-bit int is safe for Qdrant point IDs\n    return int.from_bytes(digest[:8], byteorder='big')\n\nclass QdrantService:\n    def __init__(self, collection_name: str = COLLECTION_NAME):\n        self.collection_name = collection_name\n        self.client = QdrantClient(url=settings.qdrant_url)\n        self.setup_collection_if_needed() \n\n    def setup_collection_if_needed(self):\n        \"\"\"\n        Ensures the Qdrant collection exists.\n        The aggressive deletion logic is REMOVED to allow data persistence.\n        \"\"\"\n        try:\n            # Check if the collection exists\n            self.client.get_collection(collection_name=self.collection_name)\n            logger.info(\"Qdrant collection found. Skipping recreation.\")\n            \n        except UnexpectedResponse:\n            # Collection was not found, so create it\n            logger.info(\"Qdrant collection not found. Creating now.\", collection_name=self.collection_name)\n            \n            # 1. Create the collection\n            try:\n                self.client.recreate_collection(\n                    collection_name=self.collection_name,\n                    vectors_config=models.VectorParams(\n                        size=EMBEDDING_DIMENSION,\n                        distance=models.Distance.COSINE\n                    ),\n                )\n                logger.info(\"Qdrant collection created successfully\", dimension=EMBEDDING_DIMENSION)\n\n                # 2. Create payload indices for filtering (project_id and tenant_id)\n                self.client.create_payload_index(\n                    collection_name=self.collection_name, \n                    field_name=\"project_id\", \n                    field_schema=models.PayloadSchemaType.KEYWORD\n                )\n                self.client.create_payload_index(\n                    collection_name=self.collection_name, \n                    field_name=\"tenant_id\", \n                    field_schema=models.PayloadSchemaType.KEYWORD\n                )\n                logger.info(\"Created payload indices for project_id and tenant_id.\")\n                \n            except Exception as e:\n                logger.error(\"Failed to create Qdrant collection or indices\", error=str(e))\n                raise\n        except Exception as e:\n             logger.error(\"General error during Qdrant initialization check\", error=str(e))\n             raise # Re-raise any other unexpected error\n\n    # RENAMED to index_points for clarity and to align with RAGService\n    async def index_points(self, points_data: List[Dict[str, Any]]):\n        \"\"\"Indexes document chunks into Qdrant using the pre-calculated embeddings.\"\"\"\n        if not points_data:\n            return\n\n        points = []\n        for point_data in points_data:\n            # Ensure the structure matches PointStruct\n            points.append(\n                models.PointStruct(\n                    # Use stable hash for ID generation\n                    id=generate_stable_id(point_data[\"payload\"][\"chunk_id\"]), \n                    vector=point_data[\"vector\"],\n                    payload=point_data[\"payload\"] \n                )\n            )\n\n        # Upload points to the collection\n        operation_info = await asyncio.to_thread(\n            self.client.upsert,\n            collection_name=self.collection_name,\n            wait=True,\n            points=points\n        )\n        logger.info(\"Indexed chunks to Qdrant\", status=operation_info.status.value, count=len(points))\n        return operation_info\n    \n    \n    async def search_vectors(self, query_embedding: list[float], project_ids: list[str], tenant_id: str, limit: int = 5) -> list[dict[str, any]]:\n        \"\"\"Performs vector search with project and tenant filtering.\"\"\"\n\n        # Define the filter to restrict search to specific project IDs AND the tenant ID\n        # Only allow results that match the tenant_id AND one of the project_ids\n        search_filter = models.Filter(\n            must=[\n                models.FieldCondition(\n                    key=\"tenant_id\",\n                    match=models.MatchValue(value=tenant_id)\n                )\n            ],\n            should=[\n                models.FieldCondition(\n                    key=\"project_id\",\n                    match=models.MatchValue(value=pid)\n                ) for pid in project_ids if pid # Filter out empty project IDs\n            ]\n        )\n        \n        # If no project IDs are passed, remove the 'should' filter to search all of the tenant's data\n        if not project_ids or all(not pid for pid in project_ids):\n            search_filter.should = []\n\n        # Perform the search\n        search_result = await asyncio.to_thread(\n            self.client.search,\n            collection_name=self.collection_name,\n            query_vector=query_embedding, \n            query_filter=search_filter, # Use the constructed filter\n            limit=limit,\n            with_payload=True # Include the content/metadata in the result\n        )\n\n        results = []\n        for hit in search_result:\n            results.append({\n                \"score\": hit.score,\n                # The payload now correctly contains the original chunk data\n                \"payload\": hit.payload \n            })\n        \n        return results\n"
    },
    {
      "path": "backend/services/rag_service.py",
      "filename": "rag_service.py",
      "content": "#!/usr/bin/env python3\nimport asyncio\nfrom typing import List, Dict, Any, Optional\nfrom sentence_transformers import SentenceTransformer\nimport structlog\nfrom qdrant_client.http.exceptions import UnexpectedResponse\n\n# CORRECTED: Use parent-level import (..) to find config.py\nfrom ..config import get_settings\n# CORRECTED: Use relative imports for sibling services\nfrom .llm_service import LLMService\nfrom .qdrant_service import QdrantService \n\nlogger = structlog.get_logger()\nsettings = get_settings()\n\n\nclass RAGService:\n    def __init__(self):\n        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.llm_service = LLMService()\n        self.qdrant_service = QdrantService()\n\n    def create_embeddings(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Generates embeddings for a list of text strings.\"\"\"\n        embeddings = self.embedding_model.encode(texts, convert_to_numpy=True).tolist()\n        return embeddings\n\n    async def index_document_chunks(self, augmented_chunks: List[Dict[str, Any]]):\n        if not augmented_chunks:\n            logger.warning(\"No chunks provided for indexing.\")\n            return\n\n        texts = [chunk['content'] for chunk in augmented_chunks]\n        embeddings = await asyncio.to_thread(self.create_embeddings, texts)\n        \n        if len(embeddings) != len(augmented_chunks):\n            logger.error(\"Embedding count mismatch with chunk count. Aborting index.\")\n            return\n\n        points_data = []\n        logger.info(\"Preparing points for Qdrant indexing.\", count=len(augmented_chunks))\n        \n        for i, chunk in enumerate(augmented_chunks):\n            payload = chunk.copy() \n            \n            points_data.append({\n                \"vector\": embeddings[i],\n                \"payload\": payload \n            })\n\n        try:\n            await self.qdrant_service.index_points(points_data)\n            logger.info(\"Batch indexing complete.\", count=len(points_data))\n        except UnexpectedResponse as e:\n            logger.error(\"Qdrant indexing failed due to unexpected response.\", error=str(e))\n            raise \n        except Exception as e:\n            logger.error(\"General error during Qdrant indexing.\", error=str(e))\n            raise \n\n\n    async def retrieve_context(self, query: str, tenant_id: str, project_ids: List[str], limit: int = 10) -> List[Dict[str, Any]]:\n        \n        # CRITICAL FIX: Standardize tenant_id to the expected indexed value ('1')\n        standardized_tenant_id = \"1\" if tenant_id == \"demo\" else tenant_id\n        \n        query_embedding_list = await asyncio.to_thread(self.create_embeddings, [query])\n        query_embedding = query_embedding_list[0]\n\n        retrieved_results = await self.qdrant_service.search_vectors(\\\n            query_embedding=query_embedding,\\\n            project_ids=project_ids,\\\n            tenant_id=standardized_tenant_id,\\\n            limit=limit\\\n        )\n\n        context = []\n        for result in retrieved_results:\n            payload = result['payload']\n            context.append({\n                \"content\": payload.get('content', 'Content not available.').strip(),\n                \"score\": result.get('score', 0.0),\n                \"source_file\": payload.get('source_file', 'Unknown File'),\n                \"project_id\": payload.get('project_id', 'Unknown Project'),\n                \"document_id\": payload.get('document_id', 'Unknown Doc'),\n                \"context_type\": \"document_chunk\"\n            })\n            \n        logger.info(\"Context retrieved.\", query=query, retrieved_count=len(context))\n        return context\n\n\n    async def generate_response(self, query: str, context: List[Dict[str, Any]], history: List[Dict[str, Any]], project_context: Dict[str, Any] = None) -> Dict[str, Any]:\n        \"\"\"\n        Generates a final answer using the LLM based on the query, retrieved context, and chat history.\n        \"\"\"\n        context_text = \"\\n\\n\".join([f\"[{ctx['source_file']}/{ctx['project_id']}] {ctx['content']}\" for ctx in context])\n        \n        has_context = bool(context_text.strip())\n        \n        # Format RAG context for the prompt\n        rag_context_section = f\"CONTEXT: {context_text if has_context else 'NO_RAG_CONTEXT_AVAILABLE'}\"\n        \n        # Format chat history for the prompt (as a summary for grounding)\n        history_summary_parts = []\n        for msg in history:\n            role = msg.get('role', 'user')\n            # Extract content from Gemini API format parts\n            content = msg.get('parts', [{}])[0].get('text', '')\n            history_summary_parts.append(f\"{role.upper()}: {content}\")\n        history_summary = \"\\n\".join(history_summary_parts)\n        \n        prompt = f\"\"\"\n        You are an expert financial and technical analyst for the \"Analytics RAG Platform\" specializing in the oil and gas sector.\n        \n        Your primary goal is to provide a helpful, structured response to the user's question, **while respecting the conversation history**.\n\n        ---\n        CONVERSATION_HISTORY: {history_summary if history_summary else \"No previous conversation.\"}\n        \n        {rag_context_section}\n        \n        Question: {query}\n        ---\n\n        INSTRUCTION:\n        1. **Contextualize**: Use the **CONVERSATION_HISTORY** to understand the user's intent. \n        2. **Citation & Grounding**: If CONTEXT is available, you **MUST** use the facts and terminology found in it. Any direct reference to project-specific data (e.g., spud activity predictions) from the CONTEXT **MUST** be immediately followed by a citation in the format [file/project].\n        3. **Constraint**: Do not mention that you have or lack context. Do not include the CONTEXT text in your final answer.\n        4. **Function Call**: Do NOT try to output a function call result here. If a function call is needed, the `LLMService` will handle it before this prompt is executed.\n        \n        Your Answer (Format the response professionally, e.g., using markdown lists/headings):\n        \"\"\"\n        \n        # Pass the full history (in Gemini format) for the actual API call\n        response_data = await self.llm_service.generate_response(prompt, history=history)\n        \n        # CRITICAL FIX: Ensure the keys are safely retrieved, as response_data might be minimal during a function call\n        response_text = response_data.get('text', '') # Defaults to empty string if LLM returns None (e.g. for function call)\n        function_call = response_data.get('function_call')\n\n        sources_list = []\n        unique_sources = set()\n        for ctx in context:\n            source_tuple = (ctx[\"source_file\"], ctx[\"project_id\"])\n            if source_tuple not in unique_sources:\n                unique_sources.add(source_tuple)\n                sources_list.append({\n                    \"file\": ctx[\"source_file\"], \n                    \"project\": ctx[\"project_id\"], \n                    \"type\": ctx[\"context_type\"],\n                    \"score\": ctx[\"score\"]\n                })\n\n        return {\n            \"response\": response_text,\n            \"sources\": sources_list,\n            \"context_used\": len(context),\n            \"function_call\": function_call\n        }\n"
    },
    {
      "path": "backend/services/__init__.py",
      "filename": "__init__.py",
      "content": "# Python package\n"
    },
    {
      "path": "backend/tasks/document_tasks.py",
      "filename": "document_tasks.py",
      "content": "import asyncio\nfrom typing import Dict, Any, List\nimport os\nimport structlog\n# NEW IMPORTS\nfrom celery.signals import worker_process_init \nfrom sqlalchemy import select, create_engine\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy.engine.base import Engine as SyncEngine\n\n# CRITICAL FIX: Use parent-level relative imports (..)\nfrom ..celery_app import celery_app\nfrom ..database import get_db_session, User, init_db, get_initialized_engine # Import init_db and get_initialized_engine\nfrom ..services.gdrive_service import GoogleDriveService \nfrom ..services.rag_service import RAGService         \nfrom ..services.neo4j_service import Neo4jService     \nfrom ..parsers.document_parser import DocumentParser   \nfrom ..config import get_settings # Needed to access DB URL for sync engine\n\nlogger = structlog.get_logger()\nsettings = get_settings()\n\n# Initialize services outside of tasks for reuse\ngdrive_service = GoogleDriveService()\nrag_service = RAGService()\nneo4j_service = Neo4jService()\n\n# Engine instance must be accessible for sync access\ncelery_db_engine: SyncEngine | None = None\n\n@worker_process_init.connect\ndef setup_db_for_celery(**kwargs):\n    global celery_db_engine\n    \n    # CRITICAL FIX: Initialize a SYNCHRONOUS engine for Celery tasks only.\n    # This avoids the \"different loop\" error by not using the async driver.\n    try:\n        db_url_sync = settings.database_url.replace(\"postgresql+asyncpg\", \"postgresql+psycopg2\")\n        \n        celery_db_engine = create_engine(\n            db_url_sync,\n            echo=False, \n            connect_args={\"sslmode\": \"disable\"}\n        )\n        # We don't need to run init_db() here, as the sync engine is enough for queries.\n        \n        with celery_db_engine.connect():\n             logger.info(\"Celery worker SYNCHRONOUS DB engine initialized successfully.\")\n             \n    except Exception as e:\n        logger.error(\"FATAL: Celery worker failed to initialize DB engine.\", error=str(e))\n        raise # Fail hard if the DB can't connect\n\n\n# Helper function to run the token fetching logic synchronously\ndef _fetch_user_refresh_token_sync(user_id: int) -> str | None:\n    \"\"\"Synchronous part to fetch refresh token from DB using the stable sync engine.\"\"\"\n    \n    if celery_db_engine is None:\n        raise RuntimeError(\"Celery DB engine was not initialized on worker startup.\")\n        \n    try:\n        with Session(celery_db_engine) as session:\n            # Execute the query using the synchronous session\n            result = session.scalar(select(User.gdrive_refresh_token).where(User.id == user_id))\n            # Commit not needed as this is a read, but session handles context.\n            return result\n    except Exception as e:\n        logger.error(\"Error during synchronous token fetch.\", error=str(e))\n        raise # Re-raise for task retry\n\n\n@celery_app.task(name='document.process_document', bind=True, max_retries=3)\ndef process_document_task(self, file_meta: Dict[str, Any], project_id: str, tenant_id: str, refresh_token: str):\n    \"\"\"\n    Downloads, parses, chunks, embeds, and indexes a single document.\n    \"\"\"\n    document_id = file_meta.get('id')\n    filename = file_meta.get('name')\n    mime_type = file_meta.get('mimeType')\n\n    logger.info(\"Starting document processing.\", document_id=document_id, filename=filename)\n\n    try:\n        # 1. Download File Content (Async call must be run synchronously and pass the token)\n        file_content_coroutine = gdrive_service.download_file(file_meta['id'], refresh_token)\n        file_content = asyncio.run(file_content_coroutine)\n\n        if not file_content:\n            logger.error(\"Skipping file: Failed to download content (check user's GDrive token/permissions).\", document_id=document_id)\n            return\n\n        # 2. Parse and Chunk Document\n        parser = DocumentParser()\n        chunks = parser.parse_and_chunk(\n            file_content=file_content,\n            filename=filename,\n            mime_type=mime_type,\n            project_id=project_id,\n            tenant_id=tenant_id\n        )\n\n        if not chunks:\n            logger.warning(\"Skipping file: No content chunks extracted.\", document_id=document_id)\n            return\n\n        # Augment chunks with metadata required for indexing\n        augmented_chunks = []\n        for i, chunk in enumerate(chunks):\n            augmented_chunks.append({\n                \"content\": chunk,\n                \"document_id\": document_id,\n                \"project_id\": project_id,\n                \"tenant_id\": tenant_id,\n                \"source_file\": filename,\n                \"chunk_id\": f\"{document_id}-{i}\",\n            })\n\n        # 3. Embed and Index in Qdrant (Vector DB)\n        index_coroutine = rag_service.index_document_chunks(augmented_chunks)\n        asyncio.run(index_coroutine) \n        logger.info(\"Successfully indexed document chunks in Qdrant.\", document_id=document_id, chunk_count=len(augmented_chunks))\n\n        # 4. Index Metadata in Neo4j (Knowledge Graph)\n        document_metadata = {\n            \"document_id\": document_id, \n            \"filename\": filename,\n            \"project_id\": project_id,\n            \"tenant_id\": tenant_id,\n            \"mime_type\": mime_type,\n            \"size_bytes\": file_meta.get('size'),\n            \"chunk_count\": len(augmented_chunks)\n        }\n        # Async call must be run synchronously\n        asyncio.run(neo4j_service.add_document_node(document_metadata))\n        logger.info(\"Successfully indexed document metadata in Neo4j.\", document_id=document_id)\n\n    except Exception as exc:\n        logger.error(\"Failed to process document.\", document_id=document_id, error=str(exc))\n        # Retry the task upon failure\n        raise self.retry(exc=exc, countdown=5)\n\n\n@celery_app.task(name='document.start_gdrive_sync')\ndef start_gdrive_sync_task(project_id: str, folder_id: str, tenant_id: str, user_id: int):\n    \"\"\"\n    Main entry point for Google Drive synchronization. Lists files and dispatches\n    individual processing tasks.\n    \"\"\"\n    logger.info(\"Starting GDrive sync task\", folder_id=folder_id, project_id=project_id, tenant_id=tenant_id, user_id=user_id)\n    \n    # 1. Fetch the user's Refresh Token from the database (Synchronous wrapper)\n    try:\n        # FINAL FIX: Use the stable synchronous fetch utility\n        refresh_token = _fetch_user_refresh_token_sync(user_id)\n    except Exception as e:\n        logger.error(\"Failed to retrieve user token from database.\", error=str(e), user_id=user_id)\n        return # Abort sync\n    \n    if not refresh_token:\n        logger.error(\"Gdrive sync failed: No refresh token found for user.\", user_id=user_id)\n        return # Abort sync\n\n    # 2. List files (Async call must be run synchronously and pass the token)\n    files_metadata_coroutine = gdrive_service.list_files_in_folder(folder_id, refresh_token)\n    \n    # Run the coroutine synchronously to get the list of files\n    try:\n        files_metadata: List[Dict[str, Any]] = asyncio.run(files_metadata_coroutine)\n    except Exception as e:\n        logger.error(\"Failed to list files from GDrive.\", error=str(e), folder_id=folder_id)\n        return\n\n    if not files_metadata:\n        logger.warning(\"No files found or unable to list files.\", folder_id=folder_id)\n        return\n\n    logger.info(\"Found files to process.\", file_count=len(files_metadata), project_id=project_id)\n\n    # 3. Dispatch tasks for each document\n    for file_meta in files_metadata:\n        # Check if file type is supported (e.g., skip folders)\n        if file_meta.get('mimeType') != 'application/vnd.google-apps.folder':\n            # MODIFIED: Pass the refresh_token to the document processing task\n            process_document_task.delay(\n                file_meta=file_meta,\n                project_id=project_id,\n                tenant_id=tenant_id,\n                refresh_token=refresh_token # Pass token to next task\n            )\n            logger.info(\"Dispatched processing task.\", file_id=file_meta['id'], filename=file_meta['name'])\n        else:\n            logger.info(\"Skipping folder in sync.\", filename=file_meta['name'])\n"
    },
    {
      "path": "backend/utils/__init__.py",
      "filename": "__init__.py",
      "content": "# Python package\n"
    },
    {
      "path": "frontend/Dockerfile",
      "filename": "Dockerfile",
      "content": "FROM node:18-alpine\n\nWORKDIR /app\n\n# Copy package files\nCOPY package*.json ./\nRUN npm install\n\n# Copy source code\nCOPY . .\n\n# Expose port\nEXPOSE 3000\n\n# Start development server\nCMD [\"npm\", \"run\", \"dev\", \"--\", \"--host\", \"0.0.0.0\", \"--port\", \"3000\"]\n"
    },
    {
      "path": "frontend/index.html",
      "filename": "index.html",
      "content": "<!doctype html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <link rel=\"icon\" type=\"image/svg+xml\" href=\"/vite.svg\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>Analytics RAG Platform</title>\n  </head>\n  <body>\n    <div id=\"root\"></div>\n    <script type=\"module\" src=\"/src/main.tsx\"></script>\n  </body>\n</html>\n"
    },
    {
      "path": "frontend/postcss.config.js",
      "filename": "postcss.config.js",
      "content": "export default {\n  plugins: {\n    tailwindcss: {},\n    autoprefixer: {},\n  },\n}\n"
    },
    {
      "path": "frontend/tailwind.config.js",
      "filename": "tailwind.config.js",
      "content": "/** @type {import('tailwindcss').Config} */\nexport default {\n  content: [\n    \"./index.html\",\n    \"./src/**/*.{js,ts,jsx,tsx}\",\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\n"
    },
    {
      "path": "frontend/vite.config.ts",
      "filename": "vite.config.ts",
      "content": "import { defineConfig } from 'vite'\nimport react from '@vitejs/plugin-react'\n\nexport default defineConfig({\n  plugins: [react()],\n  server: {\n    host: '0.0.0.0',\n    allowedHosts: ['.ngrok-free.dev'],\n    port: 3000,\n    watch: {\n      usePolling: true\n    }\n  }\n})\n"
    },
    {
      "path": "frontend/src/App.tsx",
      "filename": "App.tsx",
      "content": "import React, { useState, useEffect } from 'react'\nimport ChatInterface from './components/Chat/ChatInterface'\nimport AuthWrapper from './components/Auth/AuthWrapper'\nimport ProjectSelector from './components/Dashboard/ProjectSelector'\nimport { MessageCircle, Database, Settings } from 'lucide-react'\n\ninterface User {\n  id: string\n  name: string\n  email: string\n}\n\nfunction App() {\n  const [user, setUser] = useState<User | null>(null)\n  const [selectedProject, setSelectedProject] = useState<string>('')\n  const [isAuthenticated, setIsAuthenticated] = useState(false)\n\n  useEffect(() => {\n    // Check for existing authentication\n    const token = localStorage.getItem('access_token')\n    if (token) {\n      setIsAuthenticated(true)\n      // NOTE: In a real app, this should fetch the /api/v1/auth/me endpoint\n      setUser({\n        id: '1',\n        name: 'Demo User',\n        email: 'demo@example.com'\n      })\n    }\n  }, [])\n\n  // NEW: Handle OAuth callback redirect (from /api/v1/auth/google/callback)\n  useEffect(() => {\n    const urlParams = new URLSearchParams(window.location.search)\n    const authStatus = urlParams.get('google_auth')\n    const detail = urlParams.get('detail')\n\n    if (authStatus === 'success') {\n      alert('Google Drive linked successfully! You can now sync your projects.')\n      // Clean up URL\n      window.history.replaceState({}, document.title, window.location.pathname)\n    } else if (authStatus === 'error') {\n      alert(`Google Drive linking failed: ${detail}`)\n      // Clean up URL\n      window.history.replaceState({}, document.title, window.location.pathname)\n    }\n  }, [])\n\n  if (!isAuthenticated) {\n    return (\n      <AuthWrapper \n        onAuthSuccess={(user) => {\n          setUser(user)\n          setIsAuthenticated(true)\n        }}\n      />\n    )\n  }\n\n  return (\n    <div className=\"min-h-screen bg-gray-100\">\n      {/* Header */}\n      <header className=\"bg-white shadow-sm border-b\">\n        <div className=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n          <div className=\"flex justify-between items-center h-16\">\n            <div className=\"flex items-center\">\n              <Database className=\"h-8 w-8 text-blue-600 mr-3\" />\n              <h1 className=\"text-xl font-semibold text-gray-900\">\n                Analytics RAG Platform\n              </h1>\n            </div>\n\n            <div className=\"flex items-center space-x-4\">\n              <ProjectSelector \n                selectedProject={selectedProject}\n                onProjectChange={setSelectedProject}\n              />\n\n              <div className=\"text-sm text-gray-700\">\n                Welcome, {user?.name}\n              </div>\n\n              <button\n                onClick={() => {\n                  localStorage.removeItem('access_token')\n                  setIsAuthenticated(false)\n                  setUser(null)\n                }}\n                className=\"text-gray-500 hover:text-gray-700\"\n              >\n                <Settings className=\"h-5 w-5\" />\n              </button>\n            </div>\n          </div>\n        </div>\n      </header>\n\n      {/* Main Content */}\n      <main className=\"max-w-7xl mx-auto py-6 px-4 sm:px-6 lg:px-8\">\n        <div className=\"grid grid-cols-1 lg:grid-cols-4 gap-6\">\n          {/* Chat Interface - Main Area */}\n          <div className=\"lg:col-span-3\">\n            <div className=\"bg-white rounded-lg shadow\">\n              <div className=\"p-6 border-b border-gray-200\">\n                <div className=\"flex items-center\">\n                  <MessageCircle className=\"h-5 w-5 text-blue-600 mr-2\" />\n                  <h2 className=\"text-lg font-medium text-gray-900\">\n                    Executive Chat Assistant\n                  </h2>\n                </div>\n                <p className=\"mt-1 text-sm text-gray-600\">\n                  Ask questions about your projects, KPIs, and generate insights\n                </p>\n              </div>\n\n              <ChatInterface \n                selectedProject={selectedProject}\n                user={user}\n              />\n            </div>\n          </div>\n\n          {/* Sidebar */}\n          <div className=\"lg:col-span-1\">\n            <div className=\"bg-white rounded-lg shadow p-6\">\n              <h3 className=\"text-lg font-medium text-gray-900 mb-4\">\n                Quick Actions\n              </h3>\n\n              <div className=\"space-y-3\">\n                <button \n                  onClick={() => window.location.href = 'http://localhost:8000/api/v1/auth/google/login'}\n                  className=\"w-full text-left px-3 py-2 text-sm text-blue-700 bg-blue-50 hover:bg-blue-100 rounded border border-blue-200\"\n                >\n                  Link Google Drive (OAuth)\n                </button>\n                \n                <button className=\"w-full text-left px-3 py-2 text-sm text-gray-700 hover:bg-gray-50 rounded\">\n                  View Project Summary\n                </button>\n                <button className=\"w-full text-left px-3 py-2 text-sm text-gray-700 hover:bg-gray-50 rounded\">\n                  Generate KPI Report\n                </button>\n                <button className=\"w-full text-left px-3 py-2 text-sm text-gray-700 hover:bg-gray-50 rounded\">\n                  Create Microsite\n                </button>\n              </div>\n            </div>\n          </div>\n        </div>\n      </main>\n    </div>\n  )\n}\n\nexport default App\n"
    },
    {
      "path": "frontend/src/index.css",
      "filename": "index.css",
      "content": "@tailwind base;\n@tailwind components;\n@tailwind utilities;\n\n@layer base {\n  html {\n    font-family: \"Inter\", sans-serif;\n  }\n}\n\n@layer components {\n  .chat-message {\n    @apply p-4 rounded-lg mb-4 max-w-4xl;\n  }\n\n  .user-message {\n    @apply bg-blue-50 border border-blue-200 ml-8;\n  }\n\n  .assistant-message {\n    @apply bg-gray-50 border border-gray-200 mr-8;\n  }\n}\n"
    },
    {
      "path": "frontend/src/main.tsx",
      "filename": "main.tsx",
      "content": "import React from 'react'\nimport ReactDOM from 'react-dom/client'\nimport App from './App.tsx'\nimport './index.css'\n\nReactDOM.createRoot(document.getElementById('root')!).render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n)\n"
    },
    {
      "path": "frontend/src/components/Auth/AuthWrapper.tsx",
      "filename": "AuthWrapper.tsx",
      "content": "import React from 'react'\nimport LoginForm from './LoginForm'\nimport { User } from '../../types'\n\ninterface AuthWrapperProps {\n  onAuthSuccess: (user: User) => void\n}\n\nexport default function AuthWrapper({ onAuthSuccess }: AuthWrapperProps) {\n  return (\n    <div className=\"min-h-screen bg-gray-50 flex flex-col justify-center py-12 sm:px-6 lg:px-8\">\n      <div className=\"sm:mx-auto sm:w-full sm:max-w-md\">\n        <div className=\"text-center\">\n          <h2 className=\"text-3xl font-extrabold text-gray-900\">\n            Analytics RAG Platform\n          </h2>\n          <p className=\"mt-2 text-sm text-gray-600\">\n            Sign in to access your analytics dashboard\n          </p>\n        </div>\n      </div>\n\n      <div className=\"mt-8 sm:mx-auto sm:w-full sm:max-w-md\">\n        <div className=\"bg-white py-8 px-4 shadow sm:rounded-lg sm:px-10\">\n          <LoginForm onSuccess={onAuthSuccess} />\n        </div>\n      </div>\n    </div>\n  )\n}\n"
    },
    {
      "path": "frontend/src/components/Auth/LoginForm.tsx",
      "filename": "LoginForm.tsx",
      "content": "import React, { useState } from 'react'\nimport { Loader2 } from 'lucide-react'\nimport apiService from '../../services/api'\nimport { User } from '../../types'\n\ninterface LoginFormProps {\n  onSuccess: (user: User) => void\n}\n\nexport default function LoginForm({ onSuccess }: LoginFormProps) {\n  const [username, setUsername] = useState('demo')\n  const [password, setPassword] = useState('demo')\n  const [isLoading, setIsLoading] = useState(false)\n  const [error, setError] = useState('')\n\n  const handleSubmit = async (e: React.FormEvent) => {\n    e.preventDefault()\n    setIsLoading(true)\n    setError('')\n\n    try {\n      await apiService.login(username, password)\n      const user = await apiService.getCurrentUser()\n      onSuccess(user)\n    } catch (err) {\n      setError('Invalid credentials. Please try again.')\n    } finally {\n      setIsLoading(false)\n    }\n  }\n\n  return (\n    <form className=\"space-y-6\" onSubmit={handleSubmit}>\n      <div>\n        <label htmlFor=\"username\" className=\"block text-sm font-medium text-gray-700\">\n          Username\n        </label>\n        <div className=\"mt-1\">\n          <input\n            id=\"username\"\n            name=\"username\"\n            type=\"text\"\n            required\n            value={username}\n            onChange={(e) => setUsername(e.target.value)}\n            className=\"appearance-none block w-full px-3 py-2 border border-gray-300 rounded-md placeholder-gray-400 focus:outline-none focus:ring-blue-500 focus:border-blue-500\"\n          />\n        </div>\n      </div>\n\n      <div>\n        <label htmlFor=\"password\" className=\"block text-sm font-medium text-gray-700\">\n          Password\n        </label>\n        <div className=\"mt-1\">\n          <input\n            id=\"password\"\n            name=\"password\"\n            type=\"password\"\n            required\n            value={password}\n            onChange={(e) => setPassword(e.target.value)}\n            className=\"appearance-none block w-full px-3 py-2 border border-gray-300 rounded-md placeholder-gray-400 focus:outline-none focus:ring-blue-500 focus:border-blue-500\"\n          />\n        </div>\n      </div>\n\n      {error && (\n        <div className=\"text-red-600 text-sm\">{error}</div>\n      )}\n\n      <div>\n        <button\n          type=\"submit\"\n          disabled={isLoading}\n          className=\"w-full flex justify-center py-2 px-4 border border-transparent rounded-md shadow-sm text-sm font-medium text-white bg-blue-600 hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 disabled:opacity-50 disabled:cursor-not-allowed flex items-center\"\n        >\n          {isLoading ? (\n            <>\n              <Loader2 className=\"animate-spin -ml-1 mr-3 h-5 w-5\" />\n              Signing in...\n            </>\n          ) : (\n            'Sign in'\n          )}\n        </button>\n      </div>\n\n      <div className=\"text-xs text-gray-500 text-center\">\n        Demo credentials: username \"demo\", password \"demo\"\n      </div>\n    </form>\n  )\n}\n"
    },
    {
      "path": "frontend/src/components/Chat/ChatInterface.tsx",
      "filename": "ChatInterface.tsx",
      "content": "import React, { useState, useEffect, useRef } from 'react'\nimport { Send, Loader2 } from 'lucide-react'\nimport MessageBubble from './MessageBubble'\nimport MicrositePreview from './MicrositePreview'\nimport { Message, User } from '../../types'\n\ninterface ChatInterfaceProps {\n  selectedProject: string // CRITICAL: This MUST be the numeric ID (e.g., \"6\"), not the name (\"Diatomite\")\n  user: User | null\n}\n\nexport default function ChatInterface({ selectedProject, user }: ChatInterfaceProps) {\n  const [messages, setMessages] = useState<Message[]>([])\n  const [inputMessage, setInputMessage] = useState('')\n  const [isLoading, setIsLoading] = useState(false)\n  const [websocket, setWebsocket] = useState<WebSocket | null>(null)\n  const messagesEndRef = useRef<HTMLDivElement>(null)\n\n  const scrollToBottom = () => {\n    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' })\n  }\n\n  useEffect(() => {\n    scrollToBottom()\n  }, [messages])\n\n  useEffect(() => {\n    // Initialize WebSocket connection\n    const wsUrl = import.meta.env.VITE_WS_URL || 'ws://localhost:8000'\n    const sessionId = `session_${Date.now()}`\n    const ws = new WebSocket(`${wsUrl}/ws/${sessionId}`)\n\n    ws.onopen = () => {\n      console.log('WebSocket connected')\n      setWebsocket(ws)\n      // Send initial welcome/system message to user\n      const initialMessage: Message = {\n        id: `sys_msg_${Date.now()}`,\n        type: 'assistant',\n        content: `Hello ${user?.name || 'User'}! I'm your PMO Assistant. Ask me about your projects or request an artifact (e.g., \"Generate an Excel risk log\").`,\n        timestamp: new Date().toISOString()\n      }\n      setMessages(prev => [...prev, initialMessage])\n    }\n\n    ws.onmessage = (event) => {\n      const data = JSON.parse(event.data)\n\n      if (data.type === 'response') {\n        // Standard chat response (could be the result of a normal query or a function call confirmation)\n        const assistantMessage: Message = {\n          id: `msg_${Date.now()}_resp`,\n          type: 'assistant',\n          content: data.response,\n          timestamp: new Date().toISOString(),\n          sources: data.sources,\n          microsite: data.microsite\n        }\n\n        setMessages(prev => [...prev, assistantMessage])\n        setIsLoading(false)\n      } else if (data.type === 'artifact_generated') {\n        // NEW: Artifact message sent directly from the artifacts endpoint (via manager.send_message)\n        const artifactMessage: Message = {\n          id: `msg_${Date.now()}_art`,\n          type: 'artifact_generated',\n          content: `Your requested artifact, \"${data.artifact.filename}\", is ready! Click the link below to download.`,\n          timestamp: new Date().toISOString(),\n          artifact: data.artifact\n        }\n        \n        setMessages(prev => [...prev, artifactMessage])\n\n      } else if (data.type === 'error') {\n        const errorMessage: Message = {\n          id: `msg_${Date.now()}_err`,\n          type: 'assistant',\n          content: data.error,\n          timestamp: new Date().toISOString()\n        }\n\n        setMessages(prev => [...prev, errorMessage])\n        setIsLoading(false)\n      }\n    }\n\n    ws.onclose = () => {\n      console.log('WebSocket disconnected')\n    }\n\n    ws.onerror = (error) => {\n      console.error('WebSocket error:', error)\n      setIsLoading(false)\n    }\n\n    return () => {\n      ws.close()\n    }\n  }, [user]) // Re-run effect if user changes (auth state)\n\n  const sendMessage = async () => {\n    if (!inputMessage.trim() || !websocket || isLoading) return\n\n    const userMessage: Message = {\n      id: `msg_${Date.now()}_user`,\n      type: 'user',\n      content: inputMessage,\n      timestamp: new Date().toISOString()\n    }\n\n    setMessages(prev => [...prev, userMessage])\n    setIsLoading(true)\n    \n    // CRITICAL FIX: Ensure the project ID is passed as a string and ONLY if it exists.\n    const projectIds = selectedProject ? [String(selectedProject)] : [];\n\n    // Send message via WebSocket\n    websocket.send(JSON.stringify({\n      message: inputMessage,\n      project_context: {\n        tenant_id: user?.tenant_id || 'demo',\n        project_ids: projectIds, // Using the ensured array of string IDs\n        selected_project: selectedProject\n      },\n      type: 'chat'\n    }))\n\n    setInputMessage('')\n  }\n\n  const handleKeyPress = (e: React.KeyboardEvent) => {\n    if (e.key === 'Enter' && !e.shiftKey) {\n      e.preventDefault()\n      sendMessage()\n    }\n  }\n\n  return (\n    <div className=\"flex flex-col h-[600px]\">\n      {/* Messages Area */}\n      <div className=\"flex-1 overflow-y-auto p-4 space-y-4\">\n        {messages.length === 0 && (\n          <div className=\"text-center text-gray-500 mt-8\">\n            <p className=\"text-lg font-medium\">Welcome to Analytics RAG Platform</p>\n            <p className=\"text-sm mt-2\">\n              Ask questions about your projects, KPIs, or request dashboard generation\n            </p>\n            <div className=\"mt-4 text-sm text-gray-400\">\n              Example queries:\n              <ul className=\"mt-2 space-y-1\">\n                <li>\u2022 \"Generate an Excel risk log for the Stone Hill project\"</li>\n                <li>\u2022 \"Draft a Word status report based on the latest documents\"</li>\n                <li>\u2022 \"What is the predicted spud success rate?\"</li>\n              </ul>\n            </div>\n          </div>\n        )}\n\n        {messages.map((message) => (\n          <div key={message.id}>\n            <MessageBubble message={message} />\n            {/* Microsite is rendered below the bubble if present */}\n            {message.microsite && (\n              <MicrositePreview micrositeData={message.microsite} />\n            )}\n          </div>\n        ))}\n\n        {isLoading && (\n          <div className=\"flex items-center space-x-2 text-gray-500\">\n            <Loader2 className=\"h-4 w-4 animate-spin\" />\n            <span>Processing your request...</span>\n          </div>\n        )}\n\n        <div ref={messagesEndRef} />\n      </div>\n\n      {/* Input Area */}\n      <div className=\"border-t border-gray-200 p-4\">\n        <div className=\"flex space-x-2\">\n          <textarea\n            value={inputMessage}\n            onChange={(e) => setInputMessage(e.target.value)}\n            onKeyPress={handleKeyPress}\n            placeholder={selectedProject \n              ? `Ask about ${selectedProject} or request insights (e.g., 'Generate risk log')...`\n              : \"Ask about your projects or request insights...\"\n            }\n            className=\"flex-1 resize-none border border-gray-300 rounded-lg px-3 py-2 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent\"\n            rows={2}\n            disabled={isLoading}\n          />\n          <button\n            onClick={sendMessage}\n            disabled={!inputMessage.trim() || isLoading}\n            className=\"bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2 disabled:opacity-50 disabled:cursor-not-allowed flex items-center\"\n          >\n            <Send className=\"h-4 w-4\" />\n          </button>\n        </div>\n\n        {selectedProject && (\n          <div className=\"mt-2 text-xs text-gray-500\">\n            Active project: <span className=\"font-medium\">{selectedProject}</span>\n          </div>\n        )}\n      </div>\n    </div>\n  )\n}\n"
    },
    {
      "path": "frontend/src/components/Chat/MessageBubble.tsx",
      "filename": "MessageBubble.tsx",
      "content": "import React from 'react'\nimport { User, Bot, ExternalLink, Download, FileText, FileSpreadsheet, Presentation } from 'lucide-react'\nimport { Message } from '../../types'\n\ninterface MessageBubbleProps {\n  message: Message\n}\n\nexport default function MessageBubble({ message }: MessageBubbleProps) {\n  const isUser = message.type === 'user'\n  const isArtifact = message.type === 'artifact_generated'\n  \n  const getFileIcon = (mimeType: string) => {\n    if (mimeType.includes('spreadsheet')) return <FileSpreadsheet className=\"h-4 w-4 text-green-600\" />\n    if (mimeType.includes('wordprocessing')) return <FileText className=\"h-4 w-4 text-blue-600\" />\n    if (mimeType.includes('presentation')) return <Presentation className=\"h-4 w-4 text-orange-600\" />\n    return <Download className=\"h-4 w-4\" />\n  }\n\n  return (\n    <div className={`chat-message ${isUser ? 'user-message' : isArtifact ? 'assistant-message' : 'assistant-message'}`}>\n      <div className=\"flex items-start space-x-3\">\n        {/* FIX: Removed the explicit \\n which caused the Unicode error in JSX/Babel parsing */}\n        <div className={`flex-shrink-0 w-8 h-8 rounded-full flex items-center justify-center ${\n          isUser ? 'bg-blue-600 text-white' : 'bg-gray-600 text-white'\n        }`}>\n          {isUser ? <User className=\"h-4 w-4\" /> : <Bot className=\"h-4 w-4\" />}\n        </div>\n\n        <div className=\"flex-1\">\n          <div className=\"flex items-center space-x-2 mb-1\">\n            <span className=\"text-sm font-medium text-gray-900\">\n              {isUser ? 'You' : 'Assistant'}\n            </span>\n            <span className=\"text-xs text-gray-500\">\n              {new Date(message.timestamp).toLocaleTimeString()}\n            </span>\n          </div>\n\n          {/* Render content text for standard messages */}\n          {message.content && (\n            <div className=\"text-gray-800 whitespace-pre-wrap\">\n              {message.content}\n            </div>\n          )}\n\n          {/* NEW: Render Artifact Download Link */}\n          {isArtifact && message.artifact && (\n            <a \n              // The download URL is a relative API path which the browser will resolve correctly\n              href={message.artifact.download_url} \n              target=\"_blank\" \n              download \n              className=\"mt-3 inline-flex items-center space-x-2 text-sm font-medium text-purple-600 hover:text-purple-800 transition-colors bg-purple-50 p-3 rounded-lg border border-purple-200 shadow-sm\"\n              rel=\"noopener noreferrer\"\n            >\n              {getFileIcon(message.artifact.mime_type)}\n              <span>Download: {message.artifact.filename}</span>\n            </a>\n          )}\n\n          {/* Render Sources */}\n          {message.sources && message.sources.length > 0 && (\n            <div className=\"mt-3 pt-3 border-t border-gray-200\">\n              <div className=\"text-xs font-medium text-gray-700 mb-2\">Sources:</div>\n              <div className=\"space-y-1\">\n                {message.sources.map((source, index) => (\n                  <div\n                    key={index}\n                    className=\"flex items-center space-x-2 text-xs text-gray-600\"\n                  >\n                    <ExternalLink className=\"h-3 w-3\" />\n                    <span className=\"font-medium\">{source.file}</span>\n                    <span className=\"text-gray-400\">\u2022</span>\n                    <span>{source.project}</span>\n                    {source.score && (\n                      <>\n                        <span className=\"text-gray-400\">\u2022</span>\n                        <span>Score: {(source.score * 100).toFixed(0)}%</span>\n                      </>\n                    )}\n                  </div>\n                ))}\\n              </div>\n            </div>\n          )}\\n        </div>\n      </div>\n    </div>\n  )\n}\n"
    },
    {
      "path": "frontend/src/components/Chat/MicrositePreview.tsx",
      "filename": "MicrositePreview.tsx",
      "content": "import React, { useState } from 'react'\nimport { ExternalLink, Maximize2, Minimize2 } from 'lucide-react'\nimport { MicrositeData } from '../../types'\n\ninterface MicrositePreviewProps {\n  micrositeData: MicrositeData\n}\n\nexport default function MicrositePreview({ micrositeData }: MicrositePreviewProps) {\n  const [isExpanded, setIsExpanded] = useState(false)\n\n  const micrositeUrl = micrositeData.url || 'http://localhost:5173'\n\n  return (\n    <div className={`mt-4 border border-gray-200 rounded-lg overflow-hidden ${\n      isExpanded ? 'fixed inset-4 z-50 bg-white' : ''\n    }`}>\n      <div className=\"bg-gray-50 px-4 py-2 border-b border-gray-200 flex items-center justify-between\">\n        <div className=\"flex items-center space-x-2\">\n          <ExternalLink className=\"h-4 w-4 text-gray-600\" />\n          <span className=\"text-sm font-medium text-gray-900\">\n            Generated Dashboard\n          </span>\n          <span className=\"text-xs text-gray-500\">\n            {micrositeData.title}\n          </span>\n        </div>\n\n        <div className=\"flex items-center space-x-2\">\n          <button\n            onClick={() => setIsExpanded(!isExpanded)}\n            className=\"text-gray-600 hover:text-gray-900 p-1\"\n          >\n            {isExpanded ? (\n              <Minimize2 className=\"h-4 w-4\" />\n            ) : (\n              <Maximize2 className=\"h-4 w-4\" />\n            )}\n          </button>\n\n          <a\n            href={micrositeUrl}\n            target=\"_blank\"\n            rel=\"noopener noreferrer\"\n            className=\"text-blue-600 hover:text-blue-800 text-xs font-medium\"\n          >\n            Open Full View\n          </a>\n        </div>\n      </div>\n\n      <div className={isExpanded ? 'h-full' : 'h-96'}>\n        <iframe\n          src={micrositeUrl}\n          className=\"w-full h-full border-0\"\n          title=\"Generated Microsite\"\n          sandbox=\"allow-scripts allow-same-origin\"\n        />\n      </div>\n\n      {isExpanded && (\n        <div \n          className=\"fixed inset-0 bg-black bg-opacity-50 -z-10\"\n          onClick={() => setIsExpanded(false)}\n        />\n      )}\n    </div>\n  )\n}\n"
    },
    {
      "path": "frontend/src/components/Dashboard/ProjectSelector.tsx",
      "filename": "ProjectSelector.tsx",
      "content": "import React, { useState, useEffect } from 'react'\nimport { ChevronDown, Folder } from 'lucide-react'\nimport apiService from '../../services/api'\nimport { Project } from '../../types' // Assuming Project type is imported\n\ninterface ProjectSelectorProps {\n  selectedProject: string\n  onProjectChange: (projectId: string) => void\n}\n\nexport default function ProjectSelector({ selectedProject, onProjectChange }: ProjectSelectorProps) {\n  const [projects, setProjects] = useState<Project[]>([])\n  const [isOpen, setIsOpen] = useState(false)\n  const [isLoading, setIsLoading] = useState(true)\n\n  useEffect(() => {\n    loadProjects()\n  }, [])\n\n  const loadProjects = async () => {\n    try {\n      const projectsData = await apiService.getProjects()\n      setProjects(projectsData)\n\n      // CRITICAL FIX: Only set the default if a project hasn't been selected YET.\n      // This prevents the initial load from firing repeated state updates \n      // if selectedProject starts as an empty string.\n      if (projectsData.length > 0 && selectedProject === '') {\n        // Ensure we pass the ID as a string, matching Qdrant indexing\n        onProjectChange(String(projectsData[0].id))\n      }\n    } catch (error) {\n      console.error('Failed to load projects:', error)\n    } finally {\n      setIsLoading(false)\n    }\n  }\n\n  // Helper to find the project data for display\n  const selectedProjectData = projects.find(p => String(p.id) === selectedProject)\n\n  if (isLoading) {\n    return (\n      <div className=\"flex items-center space-x-2 text-gray-500\">\n        <div className=\"animate-spin rounded-full h-4 w-4 border-b-2 border-gray-500\"></div>\n        <span className=\"text-sm\">Loading projects...</span>\n      </div>\n    )\n  }\n\n  return (\n    <div className=\"relative\">\n      <button\n        onClick={() => setIsOpen(!isOpen)}\n        className=\"flex items-center space-x-2 bg-white border border-gray-300 rounded-md px-3 py-2 text-sm font-medium text-gray-700 hover:bg-gray-50 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500\"\n      >\n        <Folder className=\"h-4 w-4\" />\n        <span>\n          {selectedProjectData ? selectedProjectData.name : 'Select Project'}\n        </span>\n        <ChevronDown className=\"h-4 w-4\" />\n      </button>\n\n      {isOpen && (\n        <div className=\"absolute right-0 mt-1 w-64 bg-white border border-gray-200 rounded-md shadow-lg z-10\">\n          <div className=\"py-1\">\n            {projects.map((project) => (\n              <button\n                key={project.id}\n                onClick={() => {\n                  // Ensure ID is passed as a string\n                  onProjectChange(String(project.id)) \n                  setIsOpen(false)\n                }}\n                className={`w-full text-left px-4 py-2 text-sm hover:bg-gray-100 ${\n                  selectedProject === String(project.id) ? 'bg-blue-50 text-blue-700' : 'text-gray-700'\n                }`}\n              >\n                <div className=\"font-medium\">{project.name}</div>\n                <div className=\"text-xs text-gray-500 truncate\">\n                  {project.description}\n                </div>\n              </button>\n            ))}\n          </div>\n        </div>\n      )}\n    </div>\n  )\n}\n"
    },
    {
      "path": "frontend/src/services/api.ts",
      "filename": "api.ts",
      "content": "const API_BASE_URL = import.meta.env.VITE_API_URL || 'http://localhost:8000'\n\nclass ApiService {\n  private baseUrl: string\n  private token: string | null\n\n  constructor() {\n    this.baseUrl = API_BASE_URL\n    this.token = localStorage.getItem('access_token')\n  }\n\n  private async request(endpoint: string, options: RequestInit = {}): Promise<any> {\n    const url = `${this.baseUrl}${endpoint}`\n    const headers = {\n      'Content-Type': 'application/json',\n      ...(this.token && { Authorization: `Bearer ${this.token}` }),\n      ...options.headers,\n    }\n\n    const response = await fetch(url, {\n      ...options,\n      headers,\n    })\n\n    if (!response.ok) {\n      // MODIFIED: Include status in error for better debugging in frontend logic\n      const errorDetail = await response.json().catch(() => ({ detail: response.statusText }))\n      throw new Error(`HTTP error! status: ${response.status}. Detail: ${errorDetail.detail}`)\n    }\n\n    return response.json()\n  }\n\n  // Auth endpoints\n  async login(username: string, password: string) {\n    const response = await this.request('/api/v1/auth/login', {\n      method: 'POST',\n      body: JSON.stringify({ username, password }),\n    })\n\n    this.token = response.access_token\n    localStorage.setItem('access_token', this.token!)\n\n    return response\n  }\n\n  async logout() {\n    await this.request('/api/v1/auth/logout', { method: 'POST' })\n    this.token = null\n    localStorage.removeItem('access_token')\n  }\n\n  async getCurrentUser() {\n    return this.request('/api/v1/auth/me')\n  }\n\n  // Project endpoints\n  async getProjects() {\n    return this.request('/api/v1/projects/')\n  }\n\n  async getProject(projectId: string) {\n    return this.request(`/api/v1/projects/${projectId}`)\n  }\n\n  // Chat endpoints\n  async sendMessage(message: string, projectContext: any) {\n    return this.request('/api/v1/chat/send', {\n      method: 'POST',\n      body: JSON.stringify({ message, project_context: projectContext }),\n    })\n  }\n\n  async getChatHistory(sessionId: string) {\n    return this.request(`/api/v1/chat/history/${sessionId}`)\n  }\n\n  // NEW: Google OAuth endpoints are handled via direct browser redirect (see App.tsx)\n}\n\nexport const apiService = new ApiService()\nexport default apiService\n"
    },
    {
      "path": "frontend/src/types/index.ts",
      "filename": "index.ts",
      "content": "export interface User {\n  id: string // MODIFIED: In the demo, this now maps to the DB ID (e.g., '1')\n  username: string\n  name: string\n  email: string\n  tenant_id: string\n}\n\nexport interface Project {\n  id: string\n  name: string\n  description: string\n  status: string\n  folder_id: string\n  created_at: string\n  updated_at: string\n}\n\n// NEW: Artifact Metadata Interface\nexport interface ArtifactData {\n  id: string\n  filename: string\n  mime_type: string\n  download_url: string\n}\n\nexport interface Message {\n  id: string\n  // Added new types for conversational response and artifact message\n  type: 'user' | 'assistant' | 'artifact_generated' | 'error' \n  content: string // Used for 'user' and 'assistant' text responses\n  timestamp: string\n  sources?: Source[]\n  microsite?: MicrositeData\n  artifact?: ArtifactData // NEW: Field for structured artifact files\n}\n\nexport interface Source {\n  file: string\n  project: string\n  type: string\n  score?: number\n}\n\nexport interface MicrositeData {\n  title: string\n  url: string\n  data: any\n}\n"
    },
    {
      "path": "microsite/Dockerfile",
      "filename": "Dockerfile",
      "content": "FROM node:18-alpine\n\nWORKDIR /app\n\nCOPY package*.json ./\nRUN npm install\n\nCOPY . .\n\nEXPOSE 5173\n\nCMD [\"npm\", \"run\", \"dev\", \"--\", \"--host\", \"0.0.0.0\", \"--port\", \"5173\"]\n"
    },
    {
      "path": "microsite/index.html",
      "filename": "index.html",
      "content": "<!doctype html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>Analytics Dashboard</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n  </head>\n  <body>\n    <div id=\"root\"></div>\n    <script type=\"module\" src=\"/src/main.tsx\"></script>\n  </body>\n</html>\n"
    },
    {
      "path": "microsite/vite.config.ts",
      "filename": "vite.config.ts",
      "content": "import { defineConfig } from 'vite'\nimport react from '@vitejs/plugin-react'\n\nexport default defineConfig({\n  plugins: [react()],\n  server: {\n    host: '0.0.0.0',\n    port: 5173\n  }\n})\n"
    },
    {
      "path": "microsite/src/App.tsx",
      "filename": "App.tsx",
      "content": "import React, { useState, useEffect } from 'react'\nimport Dashboard from './components/Dashboard'\nimport { BarChart3, TrendingUp, DollarSign, Activity } from 'lucide-react'\n\n// Sample data - would be fetched from API in production\nconst sampleData = {\n  title: \"Analytics Dashboard - Demo Client\",\n  client: {\n    name: \"Demo Oil & Gas Company\",\n    sector: \"Oil & Gas\",\n    summary: \"Comprehensive analytics across 3 active projects\"\n  },\n  projects: [\n    {\n      id: \"mars_project\",\n      title: \"MARS Analytics Platform\",\n      description: \"AI-driven production optimization and predictive maintenance system\",\n      status: \"Active\",\n      completion: 85\n    },\n    {\n      id: \"shell_optimization\", \n      title: \"Production Optimization Initiative\",\n      description: \"Advanced analytics for production efficiency improvements\",\n      status: \"Active\", \n      completion: 72\n    }\n  ],\n  kpis: [\n    {\n      name: \"Production Efficiency\",\n      value: \"94.2%\",\n      change: \"+2.8%\",\n      trend: \"up\",\n      icon: TrendingUp\n    },\n    {\n      name: \"Cost Reduction\",\n      value: \"$2.3M\",\n      change: \"+15%\", \n      trend: \"up\",\n      icon: DollarSign\n    },\n    {\n      name: \"Downtime Reduction\",\n      value: \"18%\",\n      change: \"-5.2%\",\n      trend: \"down\",\n      icon: Activity\n    },\n    {\n      name: \"Data Quality Score\",\n      value: \"96.7%\",\n      change: \"+1.1%\",\n      trend: \"up\", \n      icon: BarChart3\n    }\n  ],\n  valueOutcomes: [\n    {\n      type: \"Cost Savings\",\n      description: \"Reduced operational costs through predictive maintenance and optimization\",\n      amount: \"$2.3M\",\n      timeframe: \"Annual\"\n    },\n    {\n      type: \"Efficiency Gain\",\n      description: \"Improved production efficiency across all monitored facilities\",\n      amount: \"2.8%\",\n      timeframe: \"YTD\"\n    },\n    {\n      type: \"Risk Reduction\", \n      description: \"Decreased unplanned downtime through predictive analytics\",\n      amount: \"18%\",\n      timeframe: \"Quarterly\"\n    }\n  ]\n}\n\nfunction App() {\n  const [dashboardData, setDashboardData] = useState(sampleData)\n  const [isLoading, setIsLoading] = useState(false)\n\n  useEffect(() => {\n    // In production, this would fetch data from the main API\n    // based on URL parameters or API calls\n    const urlParams = new URLSearchParams(window.location.search)\n    const dataParam = urlParams.get('data')\n\n    if (dataParam) {\n      try {\n        const parsedData = JSON.parse(decodeURIComponent(dataParam))\n        setDashboardData(parsedData)\n      } catch (error) {\n        console.error('Failed to parse dashboard data:', error)\n      }\n    }\n  }, [])\n\n  if (isLoading) {\n    return (\n      <div className=\"min-h-screen bg-gray-100 flex items-center justify-center\">\n        <div className=\"text-center\">\n          <div className=\"animate-spin rounded-full h-12 w-12 border-b-2 border-blue-600 mx-auto\"></div>\n          <p className=\"mt-4 text-gray-600\">Loading dashboard...</p>\n        </div>\n      </div>\n    )\n  }\n\n  return <Dashboard data={dashboardData} />\n}\n\nexport default App\n"
    },
    {
      "path": "microsite/src/main.tsx",
      "filename": "main.tsx",
      "content": "import React from 'react'\nimport ReactDOM from 'react-dom/client'\nimport App from './App.tsx'\n\nReactDOM.createRoot(document.getElementById('root')!).render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n)\n"
    },
    {
      "path": "microsite/src/components/Dashboard.tsx",
      "filename": "Dashboard.tsx",
      "content": "import React from 'react'\nimport { BarChart3, TrendingUp, TrendingDown, Building2, Calendar, CheckCircle } from 'lucide-react'\n\ninterface DashboardProps {\n  data: {\n    title: string\n    client: {\n      name: string\n      sector: string\n      summary: string\n    }\n    projects: Array<{\n      id: string\n      title: string\n      description: string\n      status: string\n      completion?: number\n    }>\n    kpis: Array<{\n      name: string\n      value: string\n      change: string\n      trend: string\n      icon: any\n    }>\n    valueOutcomes: Array<{\n      type: string\n      description: string\n      amount: string\n      timeframe: string\n    }>\n  }\n}\n\nexport default function Dashboard({ data }: DashboardProps) {\n  return (\n    <div className=\"min-h-screen bg-gray-50\">\n      {/* Header */}\n      <div className=\"bg-white shadow-sm\">\n        <div className=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n          <div className=\"flex items-center justify-between\">\n            <div>\n              <h1 className=\"text-2xl font-bold text-gray-900\">{data.title}</h1>\n              <div className=\"flex items-center mt-2 text-sm text-gray-600\">\n                <Building2 className=\"h-4 w-4 mr-2\" />\n                <span>{data.client.name}</span>\n                <span className=\"mx-2\">\u2022</span>\n                <span>{data.client.sector}</span>\n              </div>\n            </div>\n            <div className=\"flex items-center text-sm text-gray-500\">\n              <Calendar className=\"h-4 w-4 mr-2\" />\n              <span>Generated: {new Date().toLocaleDateString()}</span>\n            </div>\n          </div>\n          <p className=\"mt-2 text-gray-700\">{data.client.summary}</p>\n        </div>\n      </div>\n\n      <div className=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {/* KPIs Grid */}\n        <div className=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6 mb-8\">\n          {data.kpis.map((kpi, index) => {\n            const IconComponent = kpi.icon\n            const isPositive = kpi.trend === 'up'\n\n            return (\n              <div key={index} className=\"bg-white p-6 rounded-lg shadow\">\n                <div className=\"flex items-center justify-between\">\n                  <div className=\"flex items-center\">\n                    <div className={`p-2 rounded-lg ${\n                      isPositive ? 'bg-green-100 text-green-600' : 'bg-red-100 text-red-600'\n                    }`}>\n                      <IconComponent className=\"h-5 w-5\" />\n                    </div>\n                  </div>\n                  <div className={`flex items-center text-sm font-medium ${\n                    isPositive ? 'text-green-600' : 'text-red-600'\n                  }`}>\n                    {isPositive ? (\n                      <TrendingUp className=\"h-4 w-4 mr-1\" />\n                    ) : (\n                      <TrendingDown className=\"h-4 w-4 mr-1\" />\n                    )}\n                    {kpi.change}\n                  </div>\n                </div>\n                <div className=\"mt-4\">\n                  <h3 className=\"text-sm font-medium text-gray-700\">{kpi.name}</h3>\n                  <p className=\"text-2xl font-bold text-gray-900 mt-1\">{kpi.value}</p>\n                </div>\n              </div>\n            )\n          })}\n        </div>\n\n        {/* Projects and Value Outcomes */}\n        <div className=\"grid grid-cols-1 lg:grid-cols-2 gap-8\">\n          {/* Active Projects */}\n          <div className=\"bg-white p-6 rounded-lg shadow\">\n            <h2 className=\"text-lg font-semibold text-gray-900 mb-4 flex items-center\">\n              <BarChart3 className=\"h-5 w-5 mr-2 text-blue-600\" />\n              Active Projects\n            </h2>\n            <div className=\"space-y-4\">\n              {data.projects.map((project, index) => (\n                <div key={index} className=\"border border-gray-200 rounded-lg p-4\">\n                  <div className=\"flex items-start justify-between\">\n                    <div className=\"flex-1\">\n                      <h3 className=\"font-medium text-gray-900\">{project.title}</h3>\n                      <p className=\"text-sm text-gray-600 mt-1\">{project.description}</p>\n                      <div className=\"flex items-center mt-2\">\n                        <CheckCircle className=\"h-4 w-4 text-green-500 mr-2\" />\n                        <span className=\"text-sm text-gray-700\">Status: {project.status}</span>\n                      </div>\n                    </div>\n                  </div>\n                  {project.completion && (\n                    <div className=\"mt-3\">\n                      <div className=\"flex justify-between text-sm text-gray-600 mb-1\">\n                        <span>Progress</span>\n                        <span>{project.completion}%</span>\n                      </div>\n                      <div className=\"w-full bg-gray-200 rounded-full h-2\">\n                        <div \n                          className=\"bg-blue-600 h-2 rounded-full\" \n                          style={{ width: `${project.completion}%` }}\n                        ></div>\n                      </div>\n                    </div>\n                  )}\n                </div>\n              ))}\n            </div>\n          </div>\n\n          {/* Value Outcomes */}\n          <div className=\"bg-white p-6 rounded-lg shadow\">\n            <h2 className=\"text-lg font-semibold text-gray-900 mb-4 flex items-center\">\n              <TrendingUp className=\"h-5 w-5 mr-2 text-green-600\" />\n              Value Delivered\n            </h2>\n            <div className=\"space-y-4\">\n              {data.valueOutcomes.map((outcome, index) => (\n                <div key={index} className=\"border-l-4 border-green-500 pl-4 py-2\">\n                  <div className=\"flex items-center justify-between\">\n                    <h3 className=\"font-medium text-gray-900\">{outcome.type}</h3>\n                    <div className=\"text-right\">\n                      <div className=\"text-lg font-bold text-green-600\">{outcome.amount}</div>\n                      <div className=\"text-xs text-gray-500\">{outcome.timeframe}</div>\n                    </div>\n                  </div>\n                  <p className=\"text-sm text-gray-600 mt-1\">{outcome.description}</p>\n                </div>\n              ))}\n            </div>\n          </div>\n        </div>\n\n        {/* Summary Stats */}\n        <div className=\"mt-8 bg-white p-6 rounded-lg shadow\">\n          <h2 className=\"text-lg font-semibold text-gray-900 mb-4\">Executive Summary</h2>\n          <div className=\"grid grid-cols-1 md:grid-cols-3 gap-6 text-center\">\n            <div className=\"p-4 bg-blue-50 rounded-lg\">\n              <div className=\"text-2xl font-bold text-blue-600\">{data.projects.length}</div>\n              <div className=\"text-sm text-gray-600\">Active Projects</div>\n            </div>\n            <div className=\"p-4 bg-green-50 rounded-lg\">\n              <div className=\"text-2xl font-bold text-green-600\">$2.3M+</div>\n              <div className=\"text-sm text-gray-600\">Value Delivered</div>\n            </div>\n            <div className=\"p-4 bg-purple-50 rounded-lg\">\n              <div className=\"text-2xl font-bold text-purple-600\">94.2%</div>\n              <div className=\"text-sm text-gray-600\">Avg Efficiency</div>\n            </div>\n          </div>\n        </div>\n      </div>\n    </div>\n  )\n}\n"
    },
    {
      "path": "scripts/=2.0.0",
      "filename": "=2.0.0",
      "content": "Collecting pydantic==2.11.0\n  Using cached pydantic-2.11.0-py3-none-any.whl.metadata (63 kB)\nRequirement already satisfied: pydantic-settings in /home/ameyaumesh/analytics-rag-platform-mvp/analytics-rag-platform/backend/venv/lib/python3.12/site-packages (2.1.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /home/ameyaumesh/analytics-rag-platform-mvp/analytics-rag-platform/backend/venv/lib/python3.12/site-packages (from pydantic==2.11.0) (0.7.0)\nCollecting pydantic-core==2.33.0 (from pydantic==2.11.0)\n  Downloading pydantic_core-2.33.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: typing-extensions>=4.12.2 in /home/ameyaumesh/analytics-rag-platform-mvp/analytics-rag-platform/backend/venv/lib/python3.12/site-packages (from pydantic==2.11.0) (4.15.0)\nRequirement already satisfied: typing-inspection>=0.4.0 in /home/ameyaumesh/analytics-rag-platform-mvp/analytics-rag-platform/backend/venv/lib/python3.12/site-packages (from pydantic==2.11.0) (0.4.2)\nRequirement already satisfied: python-dotenv>=0.21.0 in /home/ameyaumesh/analytics-rag-platform-mvp/analytics-rag-platform/backend/venv/lib/python3.12/site-packages (from pydantic-settings) (1.0.0)\nDownloading pydantic-2.11.0-py3-none-any.whl (442 kB)\nDownloading pydantic_core-2.33.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 9.4 MB/s  0:00:00\nInstalling collected packages: pydantic-core, pydantic\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.33.2\n    Uninstalling pydantic_core-2.33.2:\n      Successfully uninstalled pydantic_core-2.33.2\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.11.9\n    Uninstalling pydantic-2.11.9:\n      Successfully uninstalled pydantic-2.11.9\n\nSuccessfully installed pydantic-2.11.0 pydantic-core-2.33.0\n"
    },
    {
      "path": "scripts/get_query_vector.py",
      "filename": "get_query_vector.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nUtility script to generate the embedding vector for a given query text.\nThe output vector is a raw JSON array of 384 floats, ready to be copied\nand used directly in a Qdrant cURL search request.\n\"\"\"\nimport asyncio\nimport os\nimport sys\nimport json\nfrom sentence_transformers import SentenceTransformer\n\n# --- Path and Environment Setup (CRITICAL FIX FOR STANDALONE SCRIPTS) ---\n# 1. Determine the project root directory\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\nPROJECT_ROOT = os.path.join(CURRENT_DIR, '..')\n\n# 2. Add the project root to the Python path to ensure package imports work\nsys.path.insert(0, PROJECT_ROOT)\n\n# 3. Load environment variables (needed for other scripts, good practice here)\nfrom dotenv import load_dotenv\nload_dotenv(os.path.join(PROJECT_ROOT, '.env'))\n\n# --- Configuration ---\n# The model used for indexing (must match the model used in RAGService)\nEMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n\ndef get_query_vector(query: str):\n    \"\"\"Generates and prints the embedding vector for the query.\"\"\"\n    print(f\"--- Generating Vector for Query: '{query}' ---\")\n    \n    try:\n        # Load the embedding model\n        # This will automatically download weights if they aren't present\n        model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n        \n        # Generate embedding vector\n        vector = model.encode(query, convert_to_numpy=True).tolist()\n        \n        print(f\"\u2705 Vector Generated (Dimension: {len(vector)})\")\n        print(\"\\n--- RAW VECTOR OUTPUT (Copy this entire array for cURL) ---\")\n        \n        # Print the vector as a dense JSON array\n        print(json.dumps(vector))\n        \n        print(\"----------------------------------------------------------\")\n\n    except Exception as e:\n        print(f\"\\n\u274c FATAL ERROR: Failed to generate vector.\")\n        print(f\"Error: {e!r}\")\n        print(\"Check if 'sentence-transformers' is installed in your environment.\")\n\n\nif __name__ == \"__main__\":\n    # Use the problematic query for the test\n    query_text = \"What technologies does the Technical Architecture document mention?\"\n    \n    # Run the synchronous function directly\n    get_query_vector(query_text)\n"
    },
    {
      "path": "scripts/init.sql",
      "filename": "init.sql",
      "content": "-- Initial database setup for Analytics RAG Platform\n\n-- Enable extensions\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n\n-- Create tenants table\nCREATE TABLE IF NOT EXISTS tenants (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    slug VARCHAR(100) UNIQUE NOT NULL,\n    settings JSONB DEFAULT '{}',\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Create users table\nCREATE TABLE IF NOT EXISTS users (\n    id SERIAL PRIMARY KEY,\n    username VARCHAR(100) UNIQUE NOT NULL,\n    email VARCHAR(255) UNIQUE NOT NULL,\n    hashed_password VARCHAR(255) NOT NULL,\n    tenant_id INTEGER REFERENCES tenants(id),\n    is_active BOOLEAN DEFAULT TRUE,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    -- NEW: Fields for Google Drive OAuth token\n    gdrive_refresh_token TEXT, \n    gdrive_linked_at TIMESTAMP WITH TIME ZONE\n);\n\n-- Create projects table\nCREATE TABLE IF NOT EXISTS projects (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    description TEXT,\n    tenant_id INTEGER REFERENCES tenants(id),\n    folder_id VARCHAR(255),\n    status VARCHAR(50) DEFAULT 'active',\n    settings JSONB DEFAULT '{}',\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Create documents table\nCREATE TABLE IF NOT EXISTS documents (\n    id SERIAL PRIMARY KEY,\n    file_id VARCHAR(255) UNIQUE NOT NULL,\n    filename VARCHAR(255) NOT NULL,\n    mime_type VARCHAR(100),\n    size_bytes BIGINT,\n    project_id INTEGER REFERENCES projects(id),\n    status VARCHAR(50) DEFAULT 'pending',\n    metadata JSONB DEFAULT '{}',\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Insert demo data\nINSERT INTO tenants (name, slug, settings) \nVALUES ('Demo Company', 'demo', '{\"max_projects\": 10}')\nON CONFLICT (slug) DO NOTHING;\n\nINSERT INTO users (username, email, hashed_password, tenant_id)\nVALUES ('demo', 'demo@example.com', '$2b$12$LQv3c1yqBWVHxkd0LHAkCOYz6TtxMQJqhN8/LewFaY9T9C8xK.K0K', 1)\nON CONFLICT (username) DO NOTHING;\n\nINSERT INTO projects (name, description, tenant_id, folder_id, status)\nVALUES \n    ('MARS Analytics Platform', 'Comprehensive analytics solution for MARS operations', 1, 'demo_folder_123', 'active'),\n    ('Shell Production Optimization', 'AI-driven optimization for Shell production facilities', 1, 'demo_folder_456', 'active')\nON CONFLICT DO NOTHING;\n"
    },
    {
      "path": "scripts/init_db.py",
      "filename": "init_db.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nDatabase initialization script for Analytics RAG Platform\n\"\"\"\nimport asyncio\nimport sys\nimport os\n\n# Add the backend directory to the Python path\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', 'backend'))\nprint(sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'backend')))\n\nfrom database import init_db, get_db_session\nfrom config import get_settings\n\n# NEW: Import text for raw SQL execution\nfrom sqlalchemy import text\n\nasync def main():\n    \"\"\"Initialize the database with tables and sample data\"\"\"\n    print(\"Initializing Analytics RAG Platform database...\")\n\n    try:\n        # Initialize database tables\n        await init_db()\n        print(\"\u2705 Database tables created successfully\")\n        \n        # Load and execute init.sql contents\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        with open(os.path.join(script_dir, 'init.sql'), 'r') as f:\n            sql_script = f.read()\n\n        # Add sample data\n        async with get_db_session() as db:\n            # Execute the full script, which handles table creation and demo data insertion\n            await db.execute(text(sql_script))\n            await db.commit()\n            print(\"\u2705 Sample data created successfully\")\n\n        print(\"Database initialization complete!\")\n\n    except Exception as e:\n        print(f\"\u274c Database initialization failed: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"
    },
    {
      "path": "scripts/qdrant_status.py",
      "filename": "qdrant_status.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nUtility script to check the status of the Qdrant vector database and perform\na manual test search against the rag_documents collection.\n\"\"\"\nimport asyncio\nimport os\nimport sys\n\n# --- FINAL PATH FIX: Insert project root to ensure backend imports work ---\n# Get the absolute path to the project root directory (one level up from 'scripts')\nPROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n# Insert this path at the beginning of the system path\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n# --------------------------------------------------------------------------\n\nimport structlog\nimport json\nimport logging\nfrom typing import Any, Dict, List\nfrom dotenv import load_dotenv\n\n# Load environment variables from the project root .env file\nload_dotenv(os.path.join(PROJECT_ROOT, '.env'))\n\n# Import services (should now work because PROJECT_ROOT is on sys.path)\nfrom backend.services.qdrant_service import QdrantService\nfrom backend.services.rag_service import RAGService # Needed for search test\n\n# --- Configuration ---\n# Set the log level for this script to prevent verbose logging from dependencies\ndef configure_structlog_for_script():\n    # Only show critical logs to prevent Qdrant and other libs from spamming console\n    logging.basicConfig(level=logging.CRITICAL)\n    structlog.configure(\n        processors=[\n            structlog.stdlib.filter_by_level,\n            structlog.stdlib.add_logger_name,\n            structlog.stdlib.add_log_level,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.StackInfoRenderer(),\n            structlog.processors.format_exc_info,\n            structlog.processors.JSONRenderer(),\n        ],\n        wrapper_class=structlog.make_filtering_bound_logger(min_level=logging.CRITICAL),\n        cache_logger_on_first_use=False,\n    )\n\ndef _get_nested_value(data: Dict, path: str, default: Any = \"N/A\"):\n    \"\"\"Safely retrieves a value from a nested dictionary path.\"\"\"\n    keys = path.split('.')\n    current = data\n    for key in keys:\n        if isinstance(current, dict):\n            current = current.get(key)\n        else:\n            return default\n    return current if current is not None else default\n\nasync def check_qdrant_status(test_query: str):\n    \"\"\"Initializes QdrantService and prints collection status and performs test search.\"\"\"\n    print(\"Initializing Qdrant Service...\")\n\n    qdrant_service: QdrantService = None\n    rag_service: RAGService = None\n    collection_name = \"rag_documents\"\n\n    try:\n        # 1. Initialize services\n        qdrant_service = QdrantService(collection_name=collection_name)\n        rag_service = RAGService()\n        \n        # 2. Get collection info\n        collection_info = await asyncio.to_thread(\n            qdrant_service.client.get_collection,\n            collection_name=collection_name\n        )\n        \n        # Convert to dictionary for safe traversal (using model_dump(mode='json') is safer if available)\n        try:\n            info_dict = collection_info.dict()\n        except AttributeError:\n            # Fallback for older Pydantic models\n            info_dict = collection_info.__dict__\n        \n        # 3. Extract status details\n        status = info_dict.get('status', 'UNKNOWN')\n        vector_count = _get_nested_value(info_dict, 'points_count', 0)\n        \n        # Access vector dimension safely\n        vector_size = \"N/A\"\n        if info_dict.get('config') and info_dict['config'].get('params') and info_dict['config']['params'].get('vectors_config'):\n            vector_config = info_dict['config']['params']['vectors_config']\n            # Handles both single vector and multi-vector config structures\n            if isinstance(vector_config, dict) and vector_config.get('size'):\n                 vector_size = vector_config['size']\n            elif isinstance(vector_config, dict) and 'default' in vector_config:\n                 vector_size = _get_nested_value(vector_config['default'], 'size')\n        \n        shard_count = _get_nested_value(info_dict, 'shards_count', 1) # Assumes 'shards_count' exists\n        indexing_progress = _get_nested_value(info_dict, 'indexed_vectors_count', 0) / (vector_count or 1) * 100\n        disk_used = _get_nested_value(info_dict, 'disk_usage_bytes', 'N/A')\n        \n        print(f\"\\n=======================================================\")\n        print(f\"\u2705 Qdrant Collection: {collection_name}\")\n        print(f\"=======================================================\")\n        print(f\"\ud83d\udd39 Status: {status}\")\n        print(f\"\ud83d\udd39 Total Vectors Indexed: {vector_count}\")\n        print(f\"\ud83d\udd39 Vector Size: {vector_size}\")\n        print(f\"\ud83d\udd39 Shard Count: {shard_count}\")\n        print(f\"\ud83d\udd39 Indexing Progress: {indexing_progress:.1f}%\")\n        # Convert bytes to MB for display\n        disk_display = f\"{disk_used / (1024 * 1024):.2f} MB\" if isinstance(disk_used, (int, float)) else disk_used\n        print(f\"\ud83d\udd39 Disk Used: {disk_display}\")\n        print(f\"=======================================================\")\n        \n        # --- RAG Retrieval Test ---\n        print(f\"\\n--- RAG Retrieval Test: Querying '{test_query}' ---\")\n        \n        if rag_service:\n            context = await rag_service.retrieve_context(\n                query=test_query,\n                tenant_id=\"demo\",\n                project_ids=[\"mars_project\"],\n                limit=3\n            )\n            \n            if context:\n                print(f\"\u2705 Retrieved {len(context)} context item(s) from Qdrant.\")\n                for i, ctx in enumerate(context):\n                    print(f\"--- Chunk {i+1} (Score: {ctx['score']:.4f}) ---\")\n                    print(f\"Source: {ctx['source_file']} ({ctx['project_id']})\")\n                    print(f\"Content: {ctx['content'][:200]}...\")\n            else:\n                print(\"\u274c Retrieval failed or returned no context.\")\n        \n        print(\"-------------------------------------------------------\")\n\n\n    except Exception as e:\n        print(f\"\\n=======================================================\")\n        print(f\"\u274c FAILED TO CONNECT OR RETRIEVE QDRANT STATUS\")\n        print(f\"=======================================================\")\n        print(f\"Error: {e!r}\")\n        print(f\"\")\n        print(\"Check the following:\")\n        print(\"1. Is the Qdrant server running and accessible at the URL in your .env file?\")\n        print(\"2. Are all required environment variables set in .env?\")\n        print(\"3. Are you running this script from the project root or the 'scripts' directory?\")\n        print(\"=======================================================\")\n\n\nif __name__ == \"__main__\":\n    configure_structlog_for_script()\n    # Define the test query here\n    query = \"What technologies does the Technical Architecture document mention?\"\n    \n    try:\n        asyncio.run(check_qdrant_status(query))\n    except RuntimeError as e:\n        if \"cannot be called from a running event loop\" in str(e):\n             print(\"\\nRuntime Error: Script cannot be run while the Celery worker or FastAPI server is running in the same terminal session.\")\n             print(\"Please run this script after stopping all other local services.\")\n        else:\n             raise\n"
    },
    {
      "path": "scripts/setup.sh",
      "filename": "setup.sh",
      "content": "#!/bin/bash\nset -e\n\necho \"Setting up Analytics RAG Platform...\"\n\n# Check if Docker is installed\nif ! command -v docker &> /dev/null; then\n    echo \"Docker is not installed. Please install Docker first.\"\n    exit 1\nfi\n\n# Check if Docker Compose is installed\nif ! command -v docker-compose &> /dev/null; then\n    echo \"Docker Compose is not installed. Please install Docker Compose first.\"\n    exit 1\nfi\n\n# Create .env file if it doesn't exist\nif [ ! -f .env ]; then\n    echo \"Creating .env file from template...\"\n    cp .env.example .env\n    echo \"Please edit .env file with your API keys and credentials.\"\n    echo \"Required:\"\n    echo \"  - GEMINI_API_KEY: Your Google Gemini API key\"\n    echo \"  - GOOGLE_OAUTH_CLIENT_ID: Your Google OAuth Client ID\"\n    echo \"  - GOOGLE_OAUTH_CLIENT_SECRET: Your Google OAuth Client Secret\"\n    echo \"\"\n    read -p \"Press enter to continue after updating .env file...\"\nfi\n\n# Start services\necho \"Starting services with Docker Compose...\"\ndocker-compose up -d\n\n# Wait for services to be ready\necho \"Waiting for services to start...\"\nsleep 30\n\n# Check service health\necho \"Checking service health...\"\ncurl -f http://localhost:8000/health || echo \"Backend not ready yet\"\ncurl -f http://localhost:3000 || echo \"Frontend not ready yet\"\ncurl -f http://localhost:5173 || echo \"Microsite not ready yet\"\n\necho \"\"\necho \"Setup complete! Services are starting up.\"\necho \"\"\necho \"Access the application:\"\necho \"  - Chat Interface: http://localhost:3000\"\necho \"  - API Documentation: http://localhost:8000/docs\" \necho \"  - Microsite Preview: http://localhost:5173\"\necho \"\"\necho \"Login with demo credentials: username 'demo', password 'demo'\"\necho \"\"\necho \"To view logs: docker-compose logs -f\"\necho \"To stop: docker-compose down\"\n"
    },
    {
      "path": "scripts/test_e2e_rag.py",
      "filename": "test_e2e_rag.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nUtility script to run an End-to-End RAG query (Embed -> Retrieve -> Generate) \noutside of the FastAPI server to diagnose retrieval and grounding issues.\n\"\"\"\nimport asyncio\nimport os\nimport sys\nimport structlog\nimport logging\nfrom typing import Any, Dict, List\n\n# --- Path and Environment Setup (CRITICAL FIX FOR ModuleNotFoundError) ---\n# 1. Determine the project root directory\nCURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\nPROJECT_ROOT = os.path.join(CURRENT_DIR, '..')\n\n# 2. Add the project root to the Python path to resolve 'backend' imports\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\n# 3. Load environment variables from the project root .env file\nfrom dotenv import load_dotenv\nload_dotenv(os.path.join(PROJECT_ROOT, 'backend/.env')) \n\n# Import services using the absolute package path (guaranteed to work)\nfrom backend.services.rag_service import RAGService \nfrom backend.services.qdrant_service import QdrantService \nfrom backend.config import get_settings \n\n\n# --- Configuration ---\ndef configure_structlog_for_script():\n    \"\"\"Configures structlog to be non-verbose for simple script output.\"\"\"\n    logging.basicConfig(level=logging.CRITICAL)\n    structlog.configure(\n        processors=[\n            structlog.stdlib.filter_by_level,\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.dict_tracebacks, \n            structlog.processors.JSONRenderer(),\n        ],\n        wrapper_class=structlog.make_filtering_bound_logger(min_level=logging.CRITICAL),\n        cache_logger_on_first_use=False,\n    )\n\nasync def test_rag_e2e(query: str):\n    \"\"\"Runs a full RAG cycle (retrieve and generate).\"\"\"\n    print(f\"\\n=======================================================\")\n    print(f\"       END-TO-END RAG DIAGNOSTIC TEST\")\n    print(f\"=======================================================\")\n    print(f\"Query: {query}\")\n    \n    # --- CONFIGURATION MATCHING LAST SYNC ---\n    # Inside scripts/test_e2e_rag.py (Update the TEST_PROJECT_IDS variable)\n    TEST_PROJECT_IDS = [\"6\"]  # FINAL TEST: Use the correct indexed project ID\n    TEST_TENANT_ID = \"1\"\n    # ... \n\n    try:\n        # 1. Initialize RAG Service (This initializes Qdrant and LLM services too)\n        rag_service = RAGService()\n        \n        # 2. Retrieve Context from Qdrant\n        print(\"\\n--- Step 1: Context Retrieval (Qdrant) ---\")\n        context = await rag_service.retrieve_context(\n                query=query,\n                tenant_id=TEST_TENANT_ID,\n                project_ids=TEST_PROJECT_IDS, # This is now []\n                limit=5\n        )\n\n        if not context:\n            print(f\"\\u274c Retrieval failed: No relevant context chunks were returned by Qdrant for project {TEST_PROJECT_IDS} (Tenant: {TEST_TENANT_ID}).\")\n            print(\"   (Reason: Poor semantic match or data not fully indexed.)\")\n            return\n        \n        # 3. Print Retrieved Context (The diagnostic data we need!)\n        print(f\"\\u2705 Retrieved {len(context)} context item(s).\")\n        print(\"\\n--- Step 2: Retrieved Context (Raw Data) ---\")\n        for i, ctx in enumerate(context):\n            print(f\"--- Chunk {i+1} (Score: {ctx['score']:.4f}) ---\")\n            print(f\"Source: {ctx['source_file']} (Project ID: {ctx['project_id']})\")\n            # Print the raw content retrieved, limiting length for readability\n            print(f\"Content: {ctx['content'].strip()}\")\n            print(\"-\" * 20)\n\n        # 4. Generate Final Response using LLM\n        print(\"\\n--- Step 3: LLM Grounded Generation ---\")\n        response_data = await rag_service.generate_response(\n            query=query,\n            context=context\n        )\n        \n        print(f\"\\u2705 Generated Answer: {response_data['response'].strip()}\")\n        print(f\"Sources Used: {[s['file'] for s in response_data['sources']]}\")\n        print(\"\\n=======================================================\")\n\n    except Exception as e:\n        print(f\"\\n\\u274c FATAL RAG ERROR: {e!r}\")\n        import traceback\n        traceback.print_exc()\n\n\nif __name__ == \"__main__\":\n    configure_structlog_for_script()\n    \n    # Use a query that should match your 'Diatomite Data Overview.txt' file\n    query = \"Summarize the objectives for the Diatomite project based on the overview document.\" \n    \n    try:\n        asyncio.run(test_rag_e2e(query))\n    except RuntimeError as e:\n        if \"cannot be called from a running event loop\" in str(e):\n             print(\"\\nRuntime Error: Script cannot be run while the Celery worker or FastAPI server is running in the same terminal session.\")\n             print(\"Please stop all services and rerun this script.\")\n        else:\n             raise\n"
    },
    {
      "path": "scripts/test_qdrant.sh",
      "filename": "test_qdrant.sh",
      "content": "#!/bin/bash\n\n# --- RAG End-to-End Test Runner ---\n# This script executes the Python RAG diagnostic test (test_rag_e2e.py)\n# by first navigating to the 'backend' directory. This fixes the \n# 'ModuleNotFoundError: No module named 'config'' by allowing relative imports \n# to resolve correctly within the backend package structure.\n\n# Ensure the script stops on the first error\nset -e\n\n# Get the directory where this script is located (scripts/)\nSCRIPT_DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" &> /dev/null && pwd )\"\nPROJECT_ROOT=$(dirname \"$SCRIPT_DIR\")\nBACKEND_DIR=\"$PROJECT_ROOT/backend\"\nPYTHON_SCRIPT=\"$SCRIPT_DIR/test_e2e_rag.py\"\n\necho \"Navigating to: $BACKEND_DIR to resolve module imports...\"\n\n# Change the current working directory to the backend/ folder\ncd \"$BACKEND_DIR\"\n\necho \"Executing Python RAG E2E Test...\"\n\n# Execute the test script. Note: We still pass the full path to the Python script\n# as running the script directly from the backend folder simplifies the imports \n# *within* the Python code.\npython \"$PYTHON_SCRIPT\"\n\n# The return code of the Python script determines the exit status of the bash script\nEXIT_CODE=$?\n\n# Navigate back to the original directory (optional, but good practice)\ncd \"$OLDPWD\"\n\nif [ $EXIT_CODE -eq 0 ]; then\n    echo \"=======================================================\"\n    echo \"\u2705 RAG E2E Test completed successfully!\"\nelse\n    echo \"=======================================================\"\n    echo \"\u274c RAG E2E Test failed during execution.\"\nfi\n\nexit $EXIT_CODE\n"
    },
    {
      "path": "scripts/test_rag.py",
      "filename": "test_rag.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nTest script for RAG functionality\n\"\"\"\nimport asyncio\nimport sys\nimport os\n\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', 'backend'))\n\nfrom services.rag_service import RAGService\nfrom services.llm_service import LLMService\n\nasync def test_rag_system():\n    \"\"\"Test the RAG system with sample data\"\"\"\n    print(\"Testing RAG system...\")\n\n    try:\n        # Initialize services\n        rag_service = RAGService()\n        llm_service = LLMService()\n\n        # Test LLM service\n        print(\"\\n1. Testing LLM service...\")\n        response = await llm_service.generate_response(\"Hello, how are you?\")\n        print(f\"LLM Response: {response[:100]}...\")\n\n        # Test query classification\n        print(\"\\n2. Testing query classification...\")\n        intent = await llm_service.classify_query_intent(\"What are the KPIs for our projects?\")\n        print(f\"Intent: {intent}\")\n\n        # Test RAG retrieval (with empty store)\n        print(\"\\n3. Testing RAG retrieval...\")\n        context = await rag_service.retrieve_context(\n            query=\"production efficiency\",\n            tenant_id=\"demo\",\n            project_ids=[\"demo_project\"],\n            limit=5\n        )\n        print(f\"Retrieved {len(context)} context items\")\n\n        # Test response generation\n        print(\"\\n4. Testing response generation...\")\n        response_data = await rag_service.generate_response(\n            query=\"What are our key performance indicators?\",\n            context=context\n        )\n        print(f\"Generated response: {response_data['response'][:200]}...\")\n\n        print(\"\\n\u2705 RAG system test completed successfully!\")\n\n    except Exception as e:\n        print(f\"\u274c RAG system test failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    asyncio.run(test_rag_system())\n"
    }
  ]
}