import json
import os
from typing import Dict, Any

# --- Simulated Reading of the JSON File ---
# In a real scenario, this block would be: 
# with open('rag_service_update.json', 'r') as f: data = json.load(f)

# The content provided by the user is structured as a Python dictionary.
# We simulate loading it from the JSON file by assigning it directly.
RAG_SERVICE_UPDATE_JSON_CONTENT = {
  "files": [
    {
      "path": "backend/services/history_service.py",
      "content": "#!/usr/bin/env python3\nimport redis.asyncio as redis\nimport structlog\nimport json\nfrom typing import List, Dict, Any, Optional\n\n# Use multi-level parent relative import for config\nfrom ..config import get_settings\n\nlogger = structlog.get_logger()\nsettings = get_settings()\n\nclass HistoryService:\n    \"\"\"\n    Manages chat history persistence in Redis for conversational context.\n    The history is stored as a list of JSON objects.\n    \"\"\"\n    def __init__(self, redis_client: redis.Redis):\n        self.redis = redis_client\n        # Key format: 'chat_history:{session_id}'\n        self.key_prefix = \"chat_history:\"\n        self.max_history_length = 10  # Store last 10 messages (5 user, 5 assistant)\n        self.history_ttl = 3600  # History expires after 1 hour (3600 seconds)\n\n    def _get_key(self, session_id: str) -> str:\n        \"\"\"Generates the Redis key for a session.\"\"\"\n        return f\"{self.key_prefix}{session_id}\"\n\n    async def get_history(self, session_id: str) -> List[Dict[str, str]]:\n        \"\"\"Retrieves the full conversation history for a session.\"\"\"\n        key = self._get_key(session_id)\n        \n        # Redis returns a list of JSON strings\n        history_json_list = await self.redis.lrange(key, 0, -1)\n        \n        history = []\n        for json_str in history_json_list:\n            try:\n                # Decode the JSON string back into a Python dictionary\n                history.append(json.loads(json_str))\n            except json.JSONDecodeError:\n                logger.error(\"Failed to decode history item.\", json_str=json_str)\n                continue\n                \n        # Return history in chronological order (oldest first)\n        return history\n\n    async def add_message(self, session_id: str, role: str, content: str):\n        \"\"\"Adds a new message to the session history.\"\"\"\n        key = self._get_key(session_id)\n        \n        message = {\n            \"role\": role,\n            \"content\": content\n        }\n        message_json = json.dumps(message)\n        \n        # 1. Add the new message to the right (end of list)\n        await self.redis.rpush(key, message_json)\n        \n        # 2. Trim the list to the maximum length (removes oldest messages from the left)\n        await self.redis.ltrim(key, -self.max_history_length, -1)\n        \n        # 3. Reset the expiry time\n        await self.redis.expire(key, self.history_ttl)\n        \n        logger.debug(\"Message added to history.\", session_id=session_id, role=role, content=content[:50])\n\n    async def clear_history(self, session_id: str):\n        \"\"\"Clears the history for a session.\"\"\"\n        key = self._get_key(session_id)\n        await self.redis.delete(key)\n        logger.info(\"Chat history cleared.\", session_id=session_id)\n\n    async def format_history_for_prompt(self, session_id: str) -> List[Dict[str, str]]:\n        \"\"\"Retrieves history and formats it for LLM chat messages (Gemini format).\"\"\"\n        raw_history = await self.get_history(session_id)\n        \n        # Gemini API chat format expects role: 'user' or 'model'\n        formatted_history = []\n        for msg in raw_history:\n            # Map internal 'assistant' role to Gemini's 'model' role\n            role = 'model' if msg.get('role') == 'assistant' else msg.get('role', 'user')\n            \n            # Ensure only 'user' and 'model' roles are used\n            if role in ['user', 'model']:\n                formatted_history.append({\n                    \"role\": role,\n                    \"parts\": [{\"text\": msg.get(\"content\", \"\")}]\n                })\n        \n        return formatted_history\n"
    },
    {
      "path": "backend/services/artifact_service.py",
      "content": "#!/usr/bin/env python3\nimport io\nimport structlog\nfrom typing import Dict, Any, List, Optional\nimport uuid\n# Imported libraries are assumed to be in requirements.txt (docx, openpyxl, pptx)\nfrom docx import Document\nfrom openpyxl import Workbook\nfrom pptx import Presentation\n# Importing utilities to allow for robust PPTX creation (even with mock data)\nfrom pptx.util import Inches\n\nfrom ..config import get_settings\n\nlogger = structlog.get_logger()\nsettings = get_settings()\n\nclass ArtifactService:\n    \"\"\"\n    Handles the generation of Project Management artifacts (Excel, Word, PPTX).\n    Methods return file bytes.\n    \"\"\"\n    def __init__(self):\n        pass\n\n    def _generate_excel_risk_register(self, project_name: str, risks: List[Dict[str, Any]]) -> bytes:\n        \"\"\"Generates a simple Excel file (Risk Register) and returns its bytes.\"\"\"\n        wb = Workbook()\n        ws = wb.active\n        ws.title = \"Risk Register\"\n        \n        # Headers\n        headers = [\"Risk ID\", \"Description\", \"Impact\", \"Probability\", \"Status\", \"Mitigation Plan\", \"Owner\"]\n        ws.append(headers)\n\n        # Simple data based on mock or LLM summary\n        for i, risk in enumerate(risks, 2):\n            ws.append([\n                i - 1,\n                risk.get(\"description\", \"Geological uncertainty.\"),\n                risk.get(\"impact\", \"High\"),\n                risk.get(\"probability\", \"Medium\"),\n                \"Open\",\n                risk.get(\"mitigation\", \"Increased seismic survey.\"),\n                risk.get(\"owner\", \"Geology Team\")\n            ])\n\n        file_stream = io.BytesIO()\n        wb.save(file_stream)\n        file_stream.seek(0)\n        return file_stream.read()\n\n    def _generate_word_status_report(self, project_name: str, summary_content: str) -> bytes:\n        \"\"\"Generates a simple Word file (Status Report) and returns its bytes.\"\"\"\n        doc = Document()\n        \n        doc.add_heading(f\"Project Status Report: {project_name}\", 0)\n        doc.add_paragraph(f\"Report Generated: {structlog.get_logger().info('Current time').get('timestamp')}\")\n        \n        doc.add_heading(\"Executive Summary\", 1)\n        doc.add_paragraph(summary_content)\n\n        doc.add_heading(\"Spud Prediction Key Insights\", 1)\n        p = doc.add_paragraph(\"Based on the latest data for the target Q4 window, the spud predictive model remains highly accurate, maintaining an 85% confidence level for identified sites.\")\n        \n        file_stream = io.BytesIO()\n        doc.save(file_stream)\n        file_stream.seek(0)\n        return file_stream.read()\n\n    def _generate_pptx_executive_pitch(self, project_name: str, key_metrics: Dict[str, Any]) -> bytes:\n        \"\"\"Generates a simple PPTX file (Executive Pitch) and returns its bytes.\"\"\"\n        prs = Presentation()\n        \n        # Slide 1: Title Slide\n        title_slide_layout = prs.slide_layouts[0]\n        slide = prs.slides.add_slide(title_slide_layout)\n        slide.shapes.title.text = f\"Executive Pitch: {project_name}\"\n        slide.placeholders[1].text = \"Predicting Spud Activity Success | PMO Analytics\"\n\n        # Slide 2: Key Metrics Slide\n        bullet_slide_layout = prs.slide_layouts[1]\n        slide = prs.slides.add_slide(bullet_slide_layout)\n        slide.shapes.title.text = \"Key Success Factors\"\n\n        body = slide.shapes.placeholders[1]\n        tf = body.text_frame\n        tf.clear()  \n        \n        p = tf.add_paragraph()\n        p.text = f\"Predicted Spud Sites: {key_metrics.get('sites', 'Sites A & C')}\"\n        p.level = 0\n        \n        p = tf.add_paragraph()\n        p.text = f\"Confidence Level: {key_metrics.get('confidence', '85%')}\"\n        p.level = 1\n        \n        file_stream = io.BytesIO()\n        prs.save(file_stream)\n        file_stream.seek(0)\n        return file_stream.read()\n\n\n    async def generate_artifact(self, artifact_type: str, project_name: str, data: Dict[str, Any] = None) -> Optional[bytes]:\n        \"\"\"Routes the generation request to the correct internal method.\"\"\"\n        if data is None:\n            data = {}\n        \n        try:\n            if artifact_type == \"excel_risk_register\":\n                risks = data.get(\"risks\", [{\"description\": data.get(\"summary\", \"New risk identified.\"), \"impact\": \"Medium\", \"owner\": \"AI\"}])\n                return self._generate_excel_risk_register(project_name, risks)\n            \n            elif artifact_type == \"word_status_report\":\n                summary = data.get(\"summary\", \"Project status is Green.\")\n                return self._generate_word_status_report(project_name, summary)\n            \n            elif artifact_type == \"pptx_executive_pitch\":\n                key_metrics = data.get(\"key_metrics\", {\"confidence\": \"85%\", \"sites\": \"Sites A & C\"})\n                return self._generate_pptx_executive_pitch(project_name, key_metrics)\n            \n            else:\n                logger.error(\"Unknown artifact type requested.\", type=artifact_type)\n                return None\n        \n        except Exception as e:\n            logger.error(\"Failed to generate artifact.\", type=artifact_type, error=str(e))\n            raise"
    },
    {
      "path": "backend/api/routes/artifacts.py",
      "content": "#!/usr/bin/env python3\nfrom fastapi import APIRouter, HTTPException, status, Depends\nfrom fastapi.responses import Response, JSONResponse\nfrom typing import Dict, Any, Union\nimport structlog\nimport uuid\n\n# Use multi-level parent relative imports\nfrom ...services.artifact_service import ArtifactService\nfrom ...main import get_artifact_service, manager # Import manager for WebSocket access\n\nrouter = APIRouter()\nlogger = structlog.get_logger()\n\n# --- Mock Data Store (In a real system, this would be S3 or GCS) ---\n# Stores temporary artifact bytes with a UUID key and metadata\nARTIFACT_CACHE: Dict[str, Dict[str, Union[bytes, str]]] = {} \n# ------------------------------------------------------------------\n\ndef add_artifact_to_cache(file_bytes: bytes, filename: str, mime_type: str) -> Dict[str, str]:\n    \"\"\"Stores the generated file in a temporary cache and returns metadata.\"\"\"\n    file_id = str(uuid.uuid4())\n    ARTIFACT_CACHE[file_id] = {\n        \"bytes\": file_bytes,\n        \"filename\": filename,\n        \"mime_type\": mime_type\n    }\n    \n    # Return metadata for the chat response\n    return {\n        \"id\": file_id,\n        \"filename\": filename,\n        \"mime_type\": mime_type,\n        \"size_bytes\": str(len(file_bytes))\n    }\n\n@router.post(\"/generate\")\nasync def generate_artifact_endpoint(\n    request_data: Dict[str, Union[str, Dict[str, Any]]],\n    artifact_service: ArtifactService = Depends(get_artifact_service)\n):\n    \"\"\"\n    Called internally by the AgentOrchestrator to create a file \n    and send a temporary download link via WebSocket.\n    \"\"\"\n    artifact_type = request_data.get(\"artifact_type\")\n    project_name = request_data.get(\"project_name\")\n    session_id = request_data.get(\"session_id\")\n    data = request_data.get(\"data\", {})\n\n    if not all([artifact_type, project_name, session_id]):\n        raise HTTPException(status_code=400, detail=\"Missing artifact_type, project_name, or session_id.\")\n\n    try:\n        file_bytes = await artifact_service.generate_artifact(artifact_type, project_name, data)\n        \n        if file_bytes is None:\n            raise HTTPException(status_code=500, detail=\"Artifact generation failed internally.\")\n\n        # Determine file metadata based on type\n        mime_type = \"application/octet-stream\"\n        if artifact_type == \"excel_risk_register\":\n            filename = f\"{project_name}_Risk_Register.xlsx\"\n            mime_type = \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n        elif artifact_type == \"word_status_report\":\n            filename = f\"{project_name}_Status_Report.docx\"\n            mime_type = \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n        elif artifact_type == \"pptx_executive_pitch\":\n            filename = f\"{project_name}_Executive_Pitch.pptx\"\n            mime_type = \"application/vnd.openxmlformats-officedocument.presentationml.presentation\"\n        else:\n            filename = f\"{project_name}_Artifact.bin\"\n\n        metadata = add_artifact_to_cache(file_bytes, filename, mime_type)\n        \n        # Send a direct WebSocket message to the user about the generated artifact\n        await manager.send_message(session_id, {\n            \"type\": \"artifact_generated\",\n            \"session_id\": session_id,\n            \"artifact\": {\n                \"id\": metadata[\"id\"],\n                \"filename\": metadata[\"filename\"],\n                \"mime_type\": metadata[\"mime_type\"],\n                \"download_url\": f\"/api/v1/artifacts/download/{metadata['id']}\" # Relative path\n            }\n        })\n\n        # Return a simple confirmation to the orchestrator\n        return {\"status\": \"success\", \"artifact_id\": metadata[\"id\"]}\n\n    except Exception as e:\n        logger.error(\"Artifact generation endpoint failed.\", error=str(e))\n        raise HTTPException(status_code=500, detail=f\"Failed to generate artifact: {e}\")\n\n@router.get(\"/download/{file_id}\")\nasync def download_artifact_endpoint(file_id: str):\n    \"\"\"Retrieves and serves the temporary artifact file.\"\"\"\n    if file_id not in ARTIFACT_CACHE:\n        raise HTTPException(status_code=404, detail=\"File not found or expired.\")\n\n    # Retrieve data from cache\n    artifact_data = ARTIFACT_CACHE[file_id]\n    file_bytes = artifact_data[\"bytes\"]\n    filename = artifact_data[\"filename\"]\n    mime_type = artifact_data[\"mime_type\"]\n    \n    # Optional: Delete from cache after download to clean up mock storage\n    del ARTIFACT_CACHE[file_id] \n\n    return Response(\n        content=file_bytes,\n        media_type=mime_type,\n        headers={\n            \"Content-Disposition\": f\"attachment; filename=\\\"{filename}\\\"",\n            \"Content-Length\": str(len(file_bytes))\n        }\n    )\n"
    },
    {
      "path": "backend/services/llm_service.py",
      "content": "#!/usr/bin/env python3\nimport asyncio\nfrom typing import Dict, Any, Optional, List\nimport google.generativeai as genai\nimport structlog\nimport json\nimport re \n\n# CORRECTED: Use parent-level import (..) to find config.py\nfrom ..config import get_settings \n\nlogger = structlog.get_logger()\nsettings = get_settings()\n\n\n# --- Function Definition for Gemini ---\nARTIFACT_FUNCTION_SCHEMA = {\n    \"name\": \"generate_project_artifact\",\n    \"description\": \"Generates a structured Project Management artifact (Excel Risk Register, Word Status Report, or PowerPoint Executive Pitch) based on the user's request and project context.\",\n    \"parameters\": {\n        \"type\": \"OBJECT\",\n        \"properties\": {\n            \"artifact_type\": {\n                \"type\": \"STRING\",\n                \"description\": \"The type of artifact to generate. Must be one of: 'excel_risk_register', 'word_status_report', 'pptx_executive_pitch'.\"\n            },\n            \"project_name\": {\n                \"type\": \"STRING\",\n                \"description\": \"The name of the currently selected project (e.g., 'Stone Hill Spud Prediction').\"\n            },\n            \"summary_content\": {\n                \"type\": \"STRING\",\n                \"description\": \"A brief, 2-3 sentence summary of the artifact's core content, derived from the user's query and RAG context.\"\n            }\n        },\n        \"required\": [\"artifact_type\", \"project_name\", \"summary_content\"]\n    }\n}\n# --------------------------------------\n\n\nclass LLMService:\n    \"\"\"\n    Handles interactions with the Gemini LLM, including response generation\n    and query classification.\n    \"\"\"\n    def __init__(self):\n        genai.configure(api_key=settings.gemini_api_key)\n        self.model = genai.GenerativeModel('gemini-2.5-flash')\n        \n        self.generation_config = {\n            'temperature': 0.1,\n            'top_p': 0.9,\n            'top_k': 40,\n            'max_output_tokens': 2048,\n        }\n        \n        self.tools = [ARTIFACT_FUNCTION_SCHEMA]\n\n\n    async def generate_response(self, prompt: str, history: List[Dict[str, Any]] = None) -> Dict[str, Any]:\n        \"\"\"\n        Generates a text response from the LLM, potentially involving function calls.\n        \n        Returns:\n            Dict: {'text': str, 'function_call': Optional[Dict]}\n        \"\"\"\n        \n        full_contents = history if history is not None else []\n        full_contents.append({\"role\": \"user\", \"parts\": [{\"text\": prompt}]})\n        \n        try:\n            # Placeholder Logic: Detect keywords that imply artifact generation\n            if re.search(r'generate|create|draft|export|excel|word|pptx|report|log|pitch', prompt.lower()):\n                artifact_type = None\n                if 'excel' in prompt.lower() or 'risk log' in prompt.lower():\n                    artifact_type = 'excel_risk_register'\n                elif 'word' in prompt.lower() or 'status report' in prompt.lower():\n                    artifact_type = 'word_status_report'\n                elif 'pptx' in prompt.lower() or 'executive pitch' in prompt.lower():\n                    artifact_type = 'pptx_executive_pitch'\n                \n                if artifact_type:\n                    # Mock the function call result\n                    function_call_mock = {\n                        \"name\": \"generate_project_artifact\",\n                        \"args\": {\n                            \"artifact_type\": artifact_type,\n                            \"project_name\": \"Stone Hill Spud Prediction\", \n                            \"summary_content\": f\"Drafting the artifact based on the current context and the request: '{prompt[:50]}...'\"\n                        }\n                    }\n                    return {\"text\": None, \"function_call\": function_call_mock}\n\n\n            # Normal generation call (wrapped in a thread)\n            response = await asyncio.to_thread(\n                self.model.generate_content,\n                contents=full_contents,\n                generation_config=self.generation_config,\n                tools=self.tools # Passed to enable function calling\n            )\n            \n            return {\"text\": response.text, \"function_call\": None}\n\n        except Exception as e:\n            logger.error(\"Failed to generate response/handle function call.\", error=str(e))\n            return {\"text\": \"I apologize, but I encountered a service error while trying to process your request.\", \"function_call\": None}\n\n\n    async def classify_query_intent(self, query: str) -> Dict[str, Any]:\n        \"\"\"Classifies the user query's intent using the LLM.\"\"\"\n        # Added 'requires_artifact' field to the classification output\n        classification_prompt = f\"\"\"\n        Classify the following query into categories and determine if it requires a microsite/dashboard or an explicit structured artifact (Excel/Word/PPTX) response.\n\n        Query: \"{query}\"\n\n        Categories:\n        - project_overview: Questions about project summaries, objectives\n        - kpi_metrics: Questions about KPIs, metrics, performance data\n        - value_outcomes: Questions about business value, ROI\n        - microsite_request: Explicit request to generate a dashboard/microsite.\n        - artifact_request: Explicit request to generate a structured document (e.g., 'risk log', 'status report', 'pitch deck').\n\n        Respond *only* with a single JSON object in this exact format: {{\"primary_intent\": \"category\", \"requires_microsite\": true/false, \"requires_artifact\": true/false}}\n        \"\"\"\n\n        try:\n            response_data = await self.generate_response(prompt=classification_prompt)\n            response_json_string = response_data['text']\n\n            # Safely attempt to parse the JSON string response\n            try:\n                start = response_json_string.find('{')\n                end = response_json_string.rfind('}')\n                if start != -1 and end != -1:\n                    clean_json_string = response_json_string[start:end+1]\n                else:\n                    clean_json_string = response_json_string\n                    \n                result = json.loads(clean_json_string)\n                # Ensure boolean types\n                if 'requires_microsite' in result and isinstance(result['requires_microsite'], str):\n                    result['requires_microsite'] = result['requires_microsite'].lower() == 'true'\n                if 'requires_artifact' in result and isinstance(result['requires_artifact'], str):\n                    result['requires_artifact'] = result['requires_artifact'].lower() == 'true'\n                \n                return result\n            except json.JSONDecodeError:\n                logger.warning(\"Failed to decode intent classification JSON. Defaulting intent.\", response=response_json_string)\n                return {\"primary_intent\": \"general_inquiry\", \"requires_microsite\": False, \"requires_artifact\": False}\n        except Exception as e:\n            logger.error(\"Failed to classify query intent.\", error=str(e))\n            return {\"primary_intent\": \"general_inquiry\", \"requires_microsite\": False, \"requires_artifact\": False}\n"
    },
    {
      "path": "backend/services/rag_service.py",
      "content": "#!/usr/bin/env python3\nimport asyncio\nfrom typing import List, Dict, Any, Optional\nfrom sentence_transformers import SentenceTransformer\nimport structlog\nfrom qdrant_client.http.exceptions import UnexpectedResponse\n\n# CORRECTED: Use parent-level import (..) to find config.py\nfrom ..config import get_settings\n# CORRECTED: Use relative imports for sibling services\nfrom .llm_service import LLMService\nfrom .qdrant_service import QdrantService \n\nlogger = structlog.get_logger()\nsettings = get_settings()\n\n\nclass RAGService:\n    def __init__(self):\n        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n        self.llm_service = LLMService()\n        self.qdrant_service = QdrantService()\n\n    def create_embeddings(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Generates embeddings for a list of text strings.\"\"\"\n        embeddings = self.embedding_model.encode(texts, convert_to_numpy=True).tolist()\n        return embeddings\n\n    async def index_document_chunks(self, augmented_chunks: List[Dict[str, Any]]):\n        if not augmented_chunks:\n            logger.warning(\"No chunks provided for indexing.\")\n            return\n\n        texts = [chunk['content'] for chunk in augmented_chunks]\n        embeddings = await asyncio.to_thread(self.create_embeddings, texts)\n        \n        if len(embeddings) != len(augmented_chunks):\n            logger.error(\"Embedding count mismatch with chunk count. Aborting index.\")\n            return\n\n        points_data = []\n        logger.info(\"Preparing points for Qdrant indexing.\", count=len(augmented_chunks))\n        \n        for i, chunk in enumerate(augmented_chunks):\n            payload = chunk.copy() \n            \n            points_data.append({\n                \"vector\": embeddings[i],\n                \"payload\": payload \n            })\n\n        try:\n            await self.qdrant_service.index_points(points_data)\n            logger.info(\"Batch indexing complete.\", count=len(points_data))\n        except UnexpectedResponse as e:\n            logger.error(\"Qdrant indexing failed due to unexpected response.\", error=str(e))\n            raise \n        except Exception as e:\n            logger.error(\"General error during Qdrant indexing.\", error=str(e))\n            raise \n\n\n    async def retrieve_context(self, query: str, tenant_id: str, project_ids: List[str], limit: int = 10) -> List[Dict[str, Any]]:\n        \n        standardized_tenant_id = \"1\" if tenant_id == \"demo\" else tenant_id\n        \n        query_embedding_list = await asyncio.to_thread(self.create_embeddings, [query])\n        query_embedding = query_embedding_list[0]\n\n        retrieved_results = await self.qdrant_service.search_vectors(\n            query_embedding=query_embedding,\n            project_ids=project_ids,\n            tenant_id=standardized_tenant_id,\n            limit=limit\n        )\n\n        context = []\n        for result in retrieved_results:\n            payload = result['payload']\n            context.append({\n                \"content\": payload.get('content', 'Content not available.').strip(),\n                \"score\": result.get('score', 0.0),\n                \"source_file\": payload.get('source_file', 'Unknown File'),\n                \"project_id\": payload.get('project_id', 'Unknown Project'),\n                \"document_id\": payload.get('document_id', 'Unknown Doc'),\n                \"context_type\": \"document_chunk\"\n            })\n            \n        logger.info(\"Context retrieved.\", query=query, retrieved_count=len(context))\n        return context\n\n\n    async def generate_response(self, query: str, context: List[Dict[str, Any]], history: List[Dict[str, Any]], project_context: Dict[str, Any] = None) -> Dict[str, Any]:\n        \"\"\"\n        Generates a final answer using the LLM based on the query, retrieved context, and chat history.\n        \"\"\"\n        context_text = \"\\n\\n\".join([f\"[{ctx['source_file']}/{ctx['project_id']}] {ctx['content']}\" for ctx in context])\n        \n        has_context = bool(context_text.strip())\n        \n        # Format RAG context for the prompt\n        rag_context_section = f\"CONTEXT: {context_text if has_context else 'NO_RAG_CONTEXT_AVAILABLE'}\"\n        \n        # Format chat history for the prompt (as a summary for grounding)\n        history_summary_parts = []\n        for msg in history:\n            role = msg.get('role', 'user')\n            # Extract content from Gemini API format parts\n            content = msg.get('parts', [{}])[0].get('text', '')\n            history_summary_parts.append(f\"{role.upper()}: {content}\")\n        history_summary = \"\\n\".join(history_summary_parts)\n        \n        prompt = f\"\"\"\n        You are an expert financial and technical analyst for the \"Analytics RAG Platform\" specializing in the oil and gas sector.\n        \n        Your primary goal is to provide a helpful, structured response to the user's question, **while respecting the conversation history**.\n\n        ---\n        CONVERSATION_HISTORY: {history_summary if history_summary else \"No previous conversation.\"}\n        \n        {rag_context_section}\n        \n        Question: {query}\n        ---\n\n        INSTRUCTION:\n        1. **Contextualize**: Use the **CONVERSATION_HISTORY** to understand the user's intent. \n        2. **Citation & Grounding**: If CONTEXT is available, you **MUST** use the facts and terminology found in it. Any direct reference to project-specific data (e.g., spud activity predictions) from the CONTEXT **MUST** be immediately followed by a citation in the format [file/project].\n        3. **Constraint**: Do not mention that you have or lack context. Do not include the CONTEXT text in your final answer.\n        4. **Function Call**: Do NOT try to output a function call result here. If a function call is needed, the `LLMService` will handle it before this prompt is executed.\n        \n        Your Answer (Format the response professionally, e.g., using markdown lists/headings):\n        \"\"\"\n        \n        # Pass the full history (in Gemini format) for the actual API call\n        response_data = await self.llm_service.generate_response(prompt, history=history)\n        response_text = response_data['text']\n        function_call = response_data.get('function_call')\n\n        sources_list = []\n        unique_sources = set()\n        for ctx in context:\n            source_tuple = (ctx[\"source_file\"], ctx[\"project_id\"])\n            if source_tuple not in unique_sources:\n                unique_sources.add(source_tuple)\n                sources_list.append({\n                    \"file\": ctx[\"source_file\"], \n                    \"project\": ctx[\"project_id\"], \n                    \"type\": ctx[\"context_type\"],\n                    \"score\": ctx[\"score\"]\n                })\n\n        return {\n            \"response\": response_text,\n            \"sources\": sources_list,\n            \"context_used\": len(context),\n            \"function_call\": function_call # Pass function call back to orchestrator\n        }"
    },
    {
      "path": "backend/services/agent_orchestrator.py",
      "content": "#!/usr/bin/env python3\nimport asyncio\nfrom typing import Dict, Any, List\nimport structlog\nimport httpx \nfrom fastapi import Depends\n\n# CORRECTED: Use relative imports for modules within the 'backend' package\nfrom .rag_service import RAGService\nfrom .llm_service import LLMService\nfrom .gdrive_service import GoogleDriveService \nfrom .history_service import HistoryService\nfrom ..main import get_history_service \n\nlogger = structlog.get_logger()\n\n# Constants for project name mapping\nPROJECT_ID_TO_NAME = {\n    \"1\": \"MARS Analytics Platform\", \n    \"6\": \"Stone Hill Spud Prediction\" # Mocking the project ID for the user's use case\n}\nDEFAULT_PROJECT_NAME = \"Analytics Project\"\n\n\nclass AgentOrchestrator:\n    def __init__(self, history_service: HistoryService):\n        self.rag_service = RAGService()\n        self.llm_service = LLMService()\n        self.history_service = history_service\n        # Use httpx for internal service calls (e.g., to the local artifacts endpoint)\n        self.http_client = httpx.AsyncClient(base_url=\"http://localhost:8000\") \n\n    async def process_message(self, user_id: str, session_id: str, message: str, project_context: Dict[str, Any], message_type: str = \"chat\") -> Dict[str, Any]:\n        \n        # 0. Add user message to history immediately\n        await self.history_service.add_message(session_id, \"user\", message)\n        \n        try:\n            # 1. Get current conversation history for LLM grounding\n            history = await self.history_service.format_history_for_prompt(session_id)\n            \n            # 2. Get project details\n            tenant_id = project_context.get(\"tenant_id\", \"demo\")\n            project_ids = project_context.get(\"project_ids\", [])\n            selected_project_id = project_ids[0] if project_ids else None\n            project_name = PROJECT_ID_TO_NAME.get(selected_project_id, DEFAULT_PROJECT_NAME)\n\n            # 3. Retrieve context using RAG\n            context = await self.rag_service.retrieve_context(\n                query=message,\n                tenant_id=tenant_id,\n                project_ids=project_ids,\n                limit=5\n            )\n\n            # 4. Generate initial response (triggers LLM to decide on text vs function call)\n            response_data = await self.rag_service.generate_response(\n                query=message,\n                context=context,\n                history=history,\n                project_context=project_context\n            )\n            \n            response_text = response_data[\"response\"]\n            function_call = response_data.get(\"function_call\")\n\n            # --- 5. Function Call Handling ---\n            if function_call and function_call.get(\"name\") == \"generate_project_artifact\":\n                \n                args = function_call.get(\"args\", {})\n                artifact_type = args.get(\"artifact_type\")\n                \n                logger.info(\"Handling function call to generate artifact.\", type=artifact_type)\n                \n                # Internal API call to the new artifact route\n                artifact_api_payload = {\n                    \"artifact_type\": artifact_type,\n                    \"project_name\": project_name,\n                    \"session_id\": session_id,\n                    # Pass the LLM's generated summary and mock data to fill the artifact\n                    \"data\": {\"summary\": args.get(\"summary_content\")}\n                }\n                \n                try:\n                    # Calls the local API endpoint which executes file generation and WS messaging\n                    api_response = await self.http_client.post(\"/api/v1/artifacts/generate\", json=artifact_api_payload)\n                    api_response.raise_for_status()\n                    \n                    # LLM's original text response is the final text message to the user\n                    final_response_text = response_text\n                    \n                except httpx.HTTPStatusError as e:\n                    logger.error(\"Internal artifact generation failed via API.\", error=str(e), status=e.response.status_code)\n                    final_response_text = f\"I failed to generate the requested artifact ({artifact_type.replace('_', ' ')}) due to an internal system error.\"\n                \n                except Exception as e:\n                    logger.error(\"Unexpected error during artifact generation call.\", error=str(e))\n                    final_response_text = \"I encountered an error while trying to generate the artifact.\"\n\n            else:\n                # Normal chat response\n                final_response_text = response_text\n            \n            # 6. Add assistant response to history\n            await self.history_service.add_message(session_id, \"assistant\", final_response_text)\n            \n            # 7. Return the final structured response (without artifact data, which is sent via separate WS message)\n            return {\n                \"type\": \"response\",\n                \"session_id\": session_id,\n                \"response\": final_response_text,\n                \"sources\": response_data[\"sources\"],\n                \"intent\": await self.llm_service.classify_query_intent(message),\n                \"context_used\": response_data[\"context_used\"],\n                \"timestamp\": asyncio.get_event_loop().time()\n            }\n\n        except Exception as e:\n            error_message = f\"I encountered an error processing your request: {type(e).__name__}.\"\n            await self.history_service.add_message(session_id, \"assistant\", error_message)\n            \n            logger.error(\"Failed to process message in orchestrator\", error=str(e))\n            return {\n                \"type\": \"error\",\n                \"session_id\": session_id,\n                \"error\": error_message,\n                \"timestamp\": asyncio.get_event_loop().time()\n            }"
    },
    {
      "path": "backend/main.py",
      "content": "#!/usr/bin/env python3\nimport logging\nfrom contextlib import asynccontextmanager\nfrom typing import Dict, Any\n\nimport structlog\nfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, Depends\nfrom fastapi.middleware.cors import CORSMiddleware\nimport redis.asyncio as redis\nfrom sqlalchemy import text\nfrom redis.asyncio import Redis \n\n# CRITICAL FIX: Changed absolute imports to relative imports\nfrom .config import get_settings\n# Import the new function to get the engine\nfrom .database import init_db, get_db_session, get_initialized_engine \nfrom .services.auth_service import AuthService\nfrom .services.agent_orchestrator import AgentOrchestrator\nfrom .services.history_service import HistoryService # NEW\nfrom .services.artifact_service import ArtifactService # NEW\n# NEW: artifacts route import\nfrom .api.routes import auth, chat, projects, microsite, admin, artifacts \nfrom .celery_app import celery_app \n\n# Configure logging\nstructlog.configure(\n    processors=[\n        structlog.stdlib.filter_by_level,\n        structlog.stdlib.add_logger_name,\n        structlog.stdlib.add_log_level,\n        structlog.processors.TimeStamper(fmt=\"iso\"),\n        structlog.processors.JSONRenderer(),\n    ],\n    logger_factory=structlog.stdlib.LoggerFactory(),\n    cache_logger_on_first_use=True,\n)\n\nlogger = structlog.get_logger()\nsettings = get_settings()\n\n# WebSocket connection manager\nclass ConnectionManager:\n    def __init__(self, history_service: HistoryService):\n        self.active_connections: Dict[str, WebSocket] = {}\n        self.user_sessions: Dict[str, Dict[str, Any]] = {}\n        self.history_service = history_service\n\n    async def connect(self, websocket: WebSocket, session_id: str, user_id: str):\n        await websocket.accept()\n        self.active_connections[session_id] = websocket\n        self.user_sessions[session_id] = {\"user_id\": user_id}\n\n    def disconnect(self, session_id: str):\n        self.active_connections.pop(session_id, None)\n        self.user_sessions.pop(session_id, None)\n\n    async def send_message(self, session_id: str, message: dict):\n        if session_id in self.active_connections:\n            websocket = self.active_connections[session_id]\n            await websocket.send_json(message)\n\nmanager: ConnectionManager \n\n# Dependency Injection Helpers (NEW)\ndef get_history_service() -> HistoryService:\n    return app.state.history_service\n\ndef get_artifact_service() -> ArtifactService:\n    return app.state.artifact_service\n\ndef get_agent_orchestrator() -> AgentOrchestrator:\n    return app.state.agent_orchestrator\n# ------------------------------------\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup\n    logger.info(\"Starting Analytics RAG Platform\")\n    await init_db()\n    \n    app.state.db_engine = get_initialized_engine()\n    app.state.redis = redis.from_url(settings.redis_url, decode_responses=True)\n    \n    # NEW: Initialize new services\n    app.state.history_service = HistoryService(app.state.redis)\n    app.state.artifact_service = ArtifactService()\n    \n    # Initialize connection manager and agent orchestrator with dependencies\n    global manager\n    manager = ConnectionManager(app.state.history_service)\n    app.state.agent_orchestrator = AgentOrchestrator(app.state.history_service)\n    \n    app.state.auth_service = AuthService()\n    logger.info(\"Application startup complete\")\n\n    yield\n\n    # Shutdown\n    logger.info(\"Shutting down\")\n    await app.state.db_engine.dispose()\n    await app.state.redis.close()\n\n\n# Create FastAPI application\napp = FastAPI(\n    title=\"Analytics RAG Platform\",\n    description=\"Enterprise RAG system for analytics companies\",\n    version=\"1.0.0\",\n    lifespan=lifespan\n)\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"http://localhost:3000\", \"http://localhost:5173\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Include API routes\napp.include_router(auth.router, prefix=\"/api/v1/auth\", tags=[\"Authentication\"])\napp.include_router(chat.router, prefix=\"/api/v1/chat\", tags=[\"Chat\"])\napp.include_router(projects.router, prefix=\"/api/v1/projects\", tags=[\"Projects\"])\napp.include_router(microsite.router, prefix=\"/api/v1/microsite\", tags=[\"Microsite\"])\napp.include_router(admin.router, prefix=\"/api/v1/admin\", tags=[\"Administration\"])\napp.include_router(artifacts.router, prefix=\"/api/v1/artifacts\", tags=[\"Artifact Generation\"]) # NEW\n\n\n@app.get(\"/\")\nasync def root():\n    return {\n        \"status\": \"healthy\",\n        \"service\": \"Analytics RAG Platform\",\n        \"version\": \"1.0.0\"\n    }\n\n\n@app.get(\"/health\")\nasync def health_check():\n    try:\n        async with app.state.db_engine.begin() as conn:\n            await conn.execute(text(\"SELECT 1\"))\n            \n        await app.state.redis.ping()\n\n        return {\n            \"status\": \"healthy\",\n            \"database\": \"connected\",\n            \"redis\": \"connected\"\n        }\n    except Exception as e:\n        logger.error(\"Health check failed\", error=str(e))\n        raise HTTPException(status_code=503, detail=\"Service unavailable\")\n\n\n@app.websocket(\"/ws/{session_id}\")\nasync def websocket_endpoint(websocket: WebSocket, session_id: str):\n    user_id = \"1\"\n\n    await manager.connect(websocket, session_id, user_id)\n\n    try:\n        while True:\n            data = await websocket.receive_json()\n\n            # Process message through agent orchestrator\n            orchestrator = get_agent_orchestrator() \n            response = await orchestrator.process_message(\n                user_id=user_id,\n                session_id=session_id,\n                message=data[\"message\"],\n                project_context=data.get(\"project_context\", {}),\n                message_type=data.get(\"type\", \"chat\")\n            )\n\n            # Only send the standard 'response'/'error' message back here\n            # 'artifact_generated' messages are sent directly from the artifacts endpoint via manager\n            if response.get(\"type\") in [\"response\", \"error\"]:\n                await manager.send_message(session_id, response)\n\n    except WebSocketDisconnect:\n        manager.disconnect(session_id)\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True)"
    },
    {
      "path": "frontend/src/types/index.ts",
      "content": "export interface User {\n  id: string // MODIFIED: In the demo, this now maps to the DB ID (e.g., '1')\n  username: string\n  name: string\n  email: string\n  tenant_id: string\n}\n\nexport interface Project {\n  id: string\n  name: string\n  description: string\n  status: string\n  folder_id: string\n  created_at: string\n  updated_at: string\n}\n\n// NEW: Artifact Metadata Interface\nexport interface ArtifactData {\n  id: string\n  filename: string\n  mime_type: string\n  download_url: string\n}\n\nexport interface Message {\n  id: string\n  // Added new types for conversational response and artifact message\n  type: 'user' | 'assistant' | 'artifact_generated' | 'error' \n  content: string // Used for 'user' and 'assistant' text responses\n  timestamp: string\n  sources?: Source[]\n  microsite?: MicrositeData\n  artifact?: ArtifactData // NEW: Field for structured artifact files\n}\n\nexport interface Source {\n  file: string\n  project: string\n  type: string\n  score?: number\n}\n\nexport interface MicrositeData {\n  title: string\n  url: string\n  data: any\n}"
    },
    {
      "path": "frontend/src/components/Chat/ChatInterface.tsx",
      "content": "import React, { useState, useEffect, useRef } from 'react'\nimport { Send, Loader2 } from 'lucide-react'\nimport MessageBubble from './MessageBubble'\nimport MicrositePreview from './MicrositePreview'\nimport { Message, User } from '../../types'\n\ninterface ChatInterfaceProps {\n  selectedProject: string // CRITICAL: This MUST be the numeric ID (e.g., \"6\"), not the name (\"Diatomite\")\n  user: User | null\n}\n\nexport default function ChatInterface({ selectedProject, user }: ChatInterfaceProps) {\n  const [messages, setMessages] = useState<Message[]>([])\n  const [inputMessage, setInputMessage] = useState('')\n  const [isLoading, setIsLoading] = useState(false)\n  const [websocket, setWebsocket] = useState<WebSocket | null>(null)\n  const messagesEndRef = useRef<HTMLDivElement>(null)\n\n  const scrollToBottom = () => {\n    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' })\n  }\n\n  useEffect(() => {\n    scrollToBottom()\n  }, [messages])\n\n  useEffect(() => {\n    // Initialize WebSocket connection\n    const wsUrl = import.meta.env.VITE_WS_URL || 'ws://localhost:8000'\n    const sessionId = `session_${Date.now()}`\n    const ws = new WebSocket(`${wsUrl}/ws/${sessionId}`)\n\n    ws.onopen = () => {\n      console.log('WebSocket connected')\n      setWebsocket(ws)\n      // Send initial welcome/system message to user\n      const initialMessage: Message = {\n        id: `sys_msg_${Date.now()}`,\n        type: 'assistant',\n        content: `Hello ${user?.name || 'User'}! I'm your PMO Assistant. Ask me about your projects or request an artifact (e.g., \"Generate an Excel risk log\").`,\n        timestamp: new Date().toISOString()\n      }\n      setMessages(prev => [...prev, initialMessage])\n    }\n\n    ws.onmessage = (event) => {\n      const data = JSON.parse(event.data)\n\n      if (data.type === 'response') {\n        // Standard chat response (could be the result of a normal query or a function call confirmation)\n        const assistantMessage: Message = {\n          id: `msg_${Date.now()}_resp`,\n          type: 'assistant',\n          content: data.response,\n          timestamp: new Date().toISOString(),\n          sources: data.sources,\n          microsite: data.microsite\n        }\n\n        setMessages(prev => [...prev, assistantMessage])\n        setIsLoading(false)\n      } else if (data.type === 'artifact_generated') {\n        // NEW: Artifact message sent directly from the artifacts endpoint (via manager.send_message)\n        const artifactMessage: Message = {\n          id: `msg_${Date.now()}_art`,\n          type: 'artifact_generated',\n          content: `Your requested artifact, \"${data.artifact.filename}\", is ready! Click the link below to download.`,\n          timestamp: new Date().toISOString(),\n          artifact: data.artifact\n        }\n        \n        setMessages(prev => [...prev, artifactMessage])\n\n      } else if (data.type === 'error') {\n        const errorMessage: Message = {\n          id: `msg_${Date.now()}_err`,\n          type: 'assistant',\n          content: data.error,\n          timestamp: new Date().toISOString()\n        }\n\n        setMessages(prev => [...prev, errorMessage])\n        setIsLoading(false)\n      }\n    }\n\n    ws.onclose = () => {\n      console.log('WebSocket disconnected')\n    }\n\n    ws.onerror = (error) => {\n      console.error('WebSocket error:', error)\n      setIsLoading(false)\n    }\n\n    return () => {\n      ws.close()\n    }\n  }, [user]) // Re-run effect if user changes (auth state)\n\n  const sendMessage = async () => {\n    if (!inputMessage.trim() || !websocket || isLoading) return\n\n    const userMessage: Message = {\n      id: `msg_${Date.now()}_user`,\n      type: 'user',\n      content: inputMessage,\n      timestamp: new Date().toISOString()\n    }\n\n    setMessages(prev => [...prev, userMessage])\n    setIsLoading(true)\n    \n    // CRITICAL FIX: Ensure the project ID is passed as a string and ONLY if it exists.\n    const projectIds = selectedProject ? [String(selectedProject)] : [];\n\n    // Send message via WebSocket\n    websocket.send(JSON.stringify({\n      message: inputMessage,\n      project_context: {\n        tenant_id: user?.tenant_id || 'demo',\n        project_ids: projectIds, // Using the ensured array of string IDs\n        selected_project: selectedProject\n      },\n      type: 'chat'\n    }))\n\n    setInputMessage('')\n  }\n\n  const handleKeyPress = (e: React.KeyboardEvent) => {\n    if (e.key === 'Enter' && !e.shiftKey) {\n      e.preventDefault()\n      sendMessage()\n    }\n  }\n\n  return (\n    <div className=\"flex flex-col h-[600px]\">\n      {/* Messages Area */}\n      <div className=\"flex-1 overflow-y-auto p-4 space-y-4\">\n        {messages.length === 0 && (\n          <div className=\"text-center text-gray-500 mt-8\">\n            <p className=\"text-lg font-medium\">Welcome to Analytics RAG Platform</p>\n            <p className=\"text-sm mt-2\">\n              Ask questions about your projects, KPIs, or request dashboard generation\n            </p>\n            <div className=\"mt-4 text-sm text-gray-400\">\n              Example queries:\n              <ul className=\"mt-2 space-y-1\">\n                <li>• \"Generate an Excel risk log for the Stone Hill project\"</li>\n                <li>• \"Draft a Word status report based on the latest documents\"</li>\n                <li>• \"What is the predicted spud success rate?\"</li>\n              </ul>\n            </div>\n          </div>\n        )}\n\n        {messages.map((message) => (\n          <div key={message.id}>\n            <MessageBubble message={message} />\n            {/* Microsite is rendered below the bubble if present */}\n            {message.microsite && (\n              <MicrositePreview micrositeData={message.microsite} />\n            )}\n          </div>\n        ))}\n\n        {isLoading && (\n          <div className=\"flex items-center space-x-2 text-gray-500\">\n            <Loader2 className=\"h-4 w-4 animate-spin\" />\n            <span>Processing your request...</span>\n          </div>\n        )}\n\n        <div ref={messagesEndRef} />\n      </div>\n\n      {/* Input Area */}\n      <div className=\"border-t border-gray-200 p-4\">\n        <div className=\"flex space-x-2\">\n          <textarea\n            value={inputMessage}\n            onChange={(e) => setInputMessage(e.target.value)}\n            onKeyPress={handleKeyPress}\n            placeholder={selectedProject \n              ? `Ask about ${selectedProject} or request insights (e.g., 'Generate risk log')...`\n              : \"Ask about your projects or request insights...\"\n            }\n            className=\"flex-1 resize-none border border-gray-300 rounded-lg px-3 py-2 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent\"\n            rows={2}\n            disabled={isLoading}\n          />\n          <button\n            onClick={sendMessage}\n            disabled={!inputMessage.trim() || isLoading}\n            className=\"bg-blue-600 text-white px-4 py-2 rounded-lg hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2 disabled:opacity-50 disabled:cursor-not-allowed flex items-center\"\n          >\n            <Send className=\"h-4 w-4\" />\n          </button>\n        </div>\n\n        {selectedProject && (\n          <div className=\"mt-2 text-xs text-gray-500\">\n            Active project: <span className=\"font-medium\">{selectedProject}</span>\n          </div>\n        )}\n      </div>\n    </div>\n  )\n}"
    },
    {
      "path": "frontend/src/components/Chat/MessageBubble.tsx",
      "content": "import React from 'react'\nimport { User, Bot, ExternalLink, Download, FileText, FileSpreadsheet, Presentation } from 'lucide-react'\nimport { Message } from '../../types'\n\ninterface MessageBubbleProps {\n  message: Message\n}\n\nexport default function MessageBubble({ message }: MessageBubbleProps) {\n  const isUser = message.type === 'user'\n  const isArtifact = message.type === 'artifact_generated'\n  \n  const getFileIcon = (mimeType: string) => {\n    if (mimeType.includes('spreadsheet')) return <FileSpreadsheet className=\"h-4 w-4 text-green-600\" />\n    if (mimeType.includes('wordprocessing')) return <FileText className=\"h-4 w-4 text-blue-600\" />\n    if (mimeType.includes('presentation')) return <Presentation className=\"h-4 w-4 text-orange-600\" />\n    return <Download className=\"h-4 w-4\" />\n  }\n\n  return (\n    <div className={`chat-message ${isUser ? 'user-message' : isArtifact ? 'assistant-message' : 'assistant-message'}`}>\n      <div className=\"flex items-start space-x-3\">\n        <div className={`flex-shrink-0 w-8 h-8 rounded-full flex items-center justify-center ${\n          isUser ? 'bg-blue-600 text-white' : 'bg-gray-600 text-white'\n        }`}>\n          {isUser ? <User className=\"h-4 w-4\" /> : <Bot className=\"h-4 w-4\" />}\n        </div>\n\n        <div className=\"flex-1\">\n          <div className=\"flex items-center space-x-2 mb-1\">\n            <span className=\"text-sm font-medium text-gray-900\">\n              {isUser ? 'You' : 'Assistant'}\n            </span>\n            <span className=\"text-xs text-gray-500\">\n              {new Date(message.timestamp).toLocaleTimeString()}\n            </span>\n          </div>\n\n          {/* Render content text for standard messages */}\n          {message.content && (\n            <div className=\"text-gray-800 whitespace-pre-wrap\">\n              {message.content}\n            </div>\n          )}\n\n          {/* NEW: Render Artifact Download Link */}\n          {isArtifact && message.artifact && (\n            <a \n              // The download URL is a relative API path which the browser will resolve correctly\n              href={message.artifact.download_url} \n              target=\"_blank\" \n              download \n              className=\"mt-3 inline-flex items-center space-x-2 text-sm font-medium text-purple-600 hover:text-purple-800 transition-colors bg-purple-50 p-3 rounded-lg border border-purple-200 shadow-sm\"\n              rel=\"noopener noreferrer\"\n            >\n              {getFileIcon(message.artifact.mime_type)}\n              <span>Download: {message.artifact.filename}</span>\n            </a>\n          )}\n\n          {/* Render Sources */}\n          {message.sources && message.sources.length > 0 && (\n            <div className=\"mt-3 pt-3 border-t border-gray-200\">\n              <div className=\"text-xs font-medium text-gray-700 mb-2\">Sources:</div>\n              <div className=\"space-y-1\">\n                {message.sources.map((source, index) => (\n                  <div\n                    key={index}\n                    className=\"flex items-center space-x-2 text-xs text-gray-600\"\n                  >\n                    <ExternalLink className=\"h-3 w-3\" />\n                    <span className=\"font-medium\">{source.file}</span>\n                    <span className=\"text-gray-400\">\u2022</span>\n                    <span>{source.project}</span>\n                    {source.score && (\n                      <>\n                        <span className=\"text-gray-400\">\u2022</span>\n                        <span>Score: {(source.score * 100).toFixed(0)}%</span>\n                      </>\n                    )}\n                  </div>\n                ))}\n              </div>\n            </div>\n          )}\n        </div>\n      </div>\n    </div>\n  )\n}"
    }
  ]
}

# Load the JSON content to ensure it's processed as a Python dictionary
# This handles the internal escaped characters correctly.
with open('update.json', 'r') as f: data = json.load(f)
# data = RAG_SERVICE_UPDATE_JSON_CONTENT

# --- Execution Function ---
def write_updates_from_json(data: Dict[str, Any]):
    """
    Reads the file content from the provided data dictionary and writes it to 
    the specified paths, relying on the Python JSON parser to handle string escapes.
    """
    print("✨ Starting file update process based on JSON content... ✨\n")
    
    for file_data in data.get("files", []):
        file_path = file_data.get("path")
        content = file_data.get("content")
        
        if not file_path or content is None:
            print("Skipping file entry with missing path or content.")
            continue
            
        # 1. Ensure the directory exists (e.g., create backend/services)
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        try:
            # 2. Write the content to the file path.
            # The 'content' variable is a standard Python string after JSON parsing, 
            # so it contains the correct newline (\n) and quote characters.
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            # Print success message for confirmation
            print(f"✅ Successfully wrote and updated: {file_path}")
            
        except IOError as e:
            print(f"❌ Failed to write file {file_path}: {e}")

    print("\n\nAll files have been updated using the content directly read from the JSON structure.")
    print("The code now reflects the complete artifact generation and WebSocket feature.")


# Run the function with the provided JSON content
write_updates_from_json(data)
